{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mPT0ipYE28wL",
    "outputId": "4352663d-20eb-4080-a83e-bf6b97183bf4",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:19:54.060620Z",
     "end_time": "2023-04-17T19:19:54.114726Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## install finrl library\n",
    "# !pip install git+https://github.com/AI4Finance-LLC/FinRL-Library.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "EeMK7Uentj1V",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:19:54.066281Z",
     "end_time": "2023-04-17T19:19:54.189892Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lPqeTTwoh1hn",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:19:54.084562Z",
     "end_time": "2023-04-17T19:19:54.193307Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "w9A8CN5R5PuZ",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:19:54.092213Z",
     "end_time": "2023-04-17T19:19:54.212632Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "cd002c5d-2490-4947-9bd3-2b0696cb0f69",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:19:54.102355Z",
     "end_time": "2023-04-17T19:19:54.215645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "743f675b-6126-44ea-bf39-7b3333d15044",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:19:54.145126Z",
     "end_time": "2023-04-17T19:20:03.814059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (93627, 8)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = '2009-04-01'\n",
    "TRAIN_END_DATE = '2021-01-01'\n",
    "TEST_START_DATE = '2021-01-01'\n",
    "TEST_END_DATE = '2022-06-01'\n",
    "# df = pd.read_csv('../datasets/DOW30.csv')\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GiRuFOTOtj1Y",
    "outputId": "632ce1e6-ad27-4f50-fec7-0ca27c8e3c96",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:03.813465Z",
     "end_time": "2023-04-17T19:20:04.064451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         date       open       high        low      close     volume   tic  \\\n0  2009-04-01   3.717500   3.892857   3.710357   3.303859  589372000  AAPL   \n1  2009-04-01  48.779999  48.930000  47.099998  35.911690   10850100  AMGN   \n2  2009-04-01  34.520000  35.599998  34.209999  26.850748    9288800    BA   \n3  2009-04-01  27.500000  29.520000  27.440001  19.726320   15308300   CAT   \n4  2009-04-01   7.815000   8.225000   7.500000   8.172500   13760000   CRM   \n\n   day  \n0    2  \n1    2  \n2    2  \n3    2  \n4    2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-04-01</td>\n      <td>3.717500</td>\n      <td>3.892857</td>\n      <td>3.710357</td>\n      <td>3.303859</td>\n      <td>589372000</td>\n      <td>AAPL</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-04-01</td>\n      <td>48.779999</td>\n      <td>48.930000</td>\n      <td>47.099998</td>\n      <td>35.911690</td>\n      <td>10850100</td>\n      <td>AMGN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-04-01</td>\n      <td>34.520000</td>\n      <td>35.599998</td>\n      <td>34.209999</td>\n      <td>26.850748</td>\n      <td>9288800</td>\n      <td>BA</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-04-01</td>\n      <td>27.500000</td>\n      <td>29.520000</td>\n      <td>27.440001</td>\n      <td>19.726320</td>\n      <td>15308300</td>\n      <td>CAT</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-04-01</td>\n      <td>7.815000</td>\n      <td>8.225000</td>\n      <td>7.500000</td>\n      <td>8.172500</td>\n      <td>13760000</td>\n      <td>CRM</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(f'../datasets/DOW30_{TRAIN_START_DATE}-{TRAIN_END_DATE}.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DSw4ZEzVtj1Z",
    "outputId": "0015b377-84ec-4a6d-ac4e-e138c2c9bac8",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:04.065873Z",
     "end_time": "2023-04-17T19:20:04.067861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             date        open        high         low       close    volume  \\\n93622  2022-05-31  503.619995  504.109985  495.660004  490.203857   4003100   \n93623  2022-05-31  210.380005  214.350006  209.110001  210.909439   9586400   \n93624  2022-05-31   51.259998   51.560001   50.849998   48.243828  25016600   \n93625  2022-05-31   43.480000   44.270000   43.049999   42.252159   8192000   \n93626  2022-05-31  127.459999  129.899994  127.419998  127.070404  12304100   \n\n       tic  day  \n93622  UNH    1  \n93623    V    1  \n93624   VZ    1  \n93625  WBA    1  \n93626  WMT    1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>93622</th>\n      <td>2022-05-31</td>\n      <td>503.619995</td>\n      <td>504.109985</td>\n      <td>495.660004</td>\n      <td>490.203857</td>\n      <td>4003100</td>\n      <td>UNH</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>93623</th>\n      <td>2022-05-31</td>\n      <td>210.380005</td>\n      <td>214.350006</td>\n      <td>209.110001</td>\n      <td>210.909439</td>\n      <td>9586400</td>\n      <td>V</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>93624</th>\n      <td>2022-05-31</td>\n      <td>51.259998</td>\n      <td>51.560001</td>\n      <td>50.849998</td>\n      <td>48.243828</td>\n      <td>25016600</td>\n      <td>VZ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>93625</th>\n      <td>2022-05-31</td>\n      <td>43.480000</td>\n      <td>44.270000</td>\n      <td>43.049999</td>\n      <td>42.252159</td>\n      <td>8192000</td>\n      <td>WBA</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>93626</th>\n      <td>2022-05-31</td>\n      <td>127.459999</td>\n      <td>129.899994</td>\n      <td>127.419998</td>\n      <td>127.070404</td>\n      <td>12304100</td>\n      <td>WMT</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV3HrZHLh1hy",
    "outputId": "42781af6-4cee-4277-8a00-cf46052f991c",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:04.067407Z",
     "end_time": "2023-04-17T19:20:04.073078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(93627, 8)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4hYkeaPiICHS",
    "outputId": "59c51c93-f786-469b-c008-4e4416a041b4",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:04.073258Z",
     "end_time": "2023-04-17T19:20:04.151914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         date       open       high        low      close     volume   tic  \\\n0  2009-04-01   3.717500   3.892857   3.710357   3.303859  589372000  AAPL   \n1  2009-04-01  48.779999  48.930000  47.099998  35.911690   10850100  AMGN   \n2  2009-04-01  34.520000  35.599998  34.209999  26.850748    9288800    BA   \n3  2009-04-01  27.500000  29.520000  27.440001  19.726320   15308300   CAT   \n4  2009-04-01   7.815000   8.225000   7.500000   8.172500   13760000   CRM   \n\n   day  \n0    2  \n1    2  \n2    2  \n3    2  \n4    2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-04-01</td>\n      <td>3.717500</td>\n      <td>3.892857</td>\n      <td>3.710357</td>\n      <td>3.303859</td>\n      <td>589372000</td>\n      <td>AAPL</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-04-01</td>\n      <td>48.779999</td>\n      <td>48.930000</td>\n      <td>47.099998</td>\n      <td>35.911690</td>\n      <td>10850100</td>\n      <td>AMGN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-04-01</td>\n      <td>34.520000</td>\n      <td>35.599998</td>\n      <td>34.209999</td>\n      <td>26.850748</td>\n      <td>9288800</td>\n      <td>BA</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-04-01</td>\n      <td>27.500000</td>\n      <td>29.520000</td>\n      <td>27.440001</td>\n      <td>19.726320</td>\n      <td>15308300</td>\n      <td>CAT</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-04-01</td>\n      <td>7.815000</td>\n      <td>8.225000</td>\n      <td>7.500000</td>\n      <td>8.172500</td>\n      <td>13760000</td>\n      <td>CRM</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['date','tic']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2vryMsdNL9H",
    "outputId": "dff3babf-4aba-44dd-ad61-8845df60243e",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:04.097789Z",
     "end_time": "2023-04-17T19:20:04.182731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "29"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcNyXa7RNPrF",
    "outputId": "fd13ad85-36fd-4a55-9084-dbf807bbeb02",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:04.099711Z",
     "end_time": "2023-04-17T19:20:04.186118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "AAPL    3315\nKO      3315\nWMT     3315\nWBA     3315\nVZ      3315\nV       3315\nUNH     3315\nTRV     3315\nPG      3315\nNKE     3315\nMSFT    3315\nMRK     3315\nMMM     3315\nMCD     3315\nJPM     3315\nAMGN    3315\nJNJ     3315\nINTC    3315\nIBM     3315\nHON     3315\nHD      3315\nGS      3315\nDIS     3315\nCVX     3315\nCSCO    3315\nCRM     3315\nCAT     3315\nBA      3315\nDOW      807\nName: tic, dtype: int64"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kM5bH9uroCeg",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:04.141150Z",
     "end_time": "2023-04-17T19:20:04.186190Z"
    }
   },
   "outputs": [],
   "source": [
    "#  INDICATORS = ['macd',\n",
    "#                'rsi_30',\n",
    "#                'cci_30',\n",
    "#                'dx_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "aa687295-c857-4366-d9af-96ea233c6463",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:04.141204Z",
     "end_time": "2023-04-17T19:20:17.896914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "grvhGJJII3Xn",
    "outputId": "6dd919fa-032b-4180-adf4-1f732777cedc",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:17.900528Z",
     "end_time": "2023-04-17T19:20:17.908013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             date        open        high         low       close    volume  \\\n37322  2014-07-17   70.839996   71.620003   69.910004   54.174847   5152900   \n60969  2017-11-21  138.449997  139.259995  138.190002  119.609756   5660600   \n91534  2022-03-25  187.759995  191.699997  186.929993  188.949997   6365600   \n86468  2021-07-08  244.839996  246.580002  241.740005  245.800003   5159600   \n58049  2017-06-26   32.220001   32.500000   32.180000   26.801701  22175000   \n\n        tic  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n37322   WBA    3 -0.310616   58.500368   54.057193  47.901617 -160.466106   \n60969   JNJ    1  0.194054  123.029167  118.853350  53.553299  -57.342692   \n91534    BA    4 -3.048009  203.557696  167.672303  46.602395  -24.733990   \n86468   CRM    3  3.881783  249.589798  239.043201  57.046148   55.067141   \n58049  CSCO    0 -0.102618   26.771634   25.978829  49.278702   56.338151   \n\n           dx_30  close_30_sma  close_60_sma  turbulence  \n37322  21.967624     56.636575     55.002312   28.591389  \n60969   8.008496    120.895144    117.592488   12.850574  \n91534   8.498226    192.965999    201.340000   12.655687  \n86468  15.751151    240.990999    232.666000   16.447619  \n58049   6.074628     26.526254     27.142789   22.070596  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n      <th>macd</th>\n      <th>boll_ub</th>\n      <th>boll_lb</th>\n      <th>rsi_30</th>\n      <th>cci_30</th>\n      <th>dx_30</th>\n      <th>close_30_sma</th>\n      <th>close_60_sma</th>\n      <th>turbulence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37322</th>\n      <td>2014-07-17</td>\n      <td>70.839996</td>\n      <td>71.620003</td>\n      <td>69.910004</td>\n      <td>54.174847</td>\n      <td>5152900</td>\n      <td>WBA</td>\n      <td>3</td>\n      <td>-0.310616</td>\n      <td>58.500368</td>\n      <td>54.057193</td>\n      <td>47.901617</td>\n      <td>-160.466106</td>\n      <td>21.967624</td>\n      <td>56.636575</td>\n      <td>55.002312</td>\n      <td>28.591389</td>\n    </tr>\n    <tr>\n      <th>60969</th>\n      <td>2017-11-21</td>\n      <td>138.449997</td>\n      <td>139.259995</td>\n      <td>138.190002</td>\n      <td>119.609756</td>\n      <td>5660600</td>\n      <td>JNJ</td>\n      <td>1</td>\n      <td>0.194054</td>\n      <td>123.029167</td>\n      <td>118.853350</td>\n      <td>53.553299</td>\n      <td>-57.342692</td>\n      <td>8.008496</td>\n      <td>120.895144</td>\n      <td>117.592488</td>\n      <td>12.850574</td>\n    </tr>\n    <tr>\n      <th>91534</th>\n      <td>2022-03-25</td>\n      <td>187.759995</td>\n      <td>191.699997</td>\n      <td>186.929993</td>\n      <td>188.949997</td>\n      <td>6365600</td>\n      <td>BA</td>\n      <td>4</td>\n      <td>-3.048009</td>\n      <td>203.557696</td>\n      <td>167.672303</td>\n      <td>46.602395</td>\n      <td>-24.733990</td>\n      <td>8.498226</td>\n      <td>192.965999</td>\n      <td>201.340000</td>\n      <td>12.655687</td>\n    </tr>\n    <tr>\n      <th>86468</th>\n      <td>2021-07-08</td>\n      <td>244.839996</td>\n      <td>246.580002</td>\n      <td>241.740005</td>\n      <td>245.800003</td>\n      <td>5159600</td>\n      <td>CRM</td>\n      <td>3</td>\n      <td>3.881783</td>\n      <td>249.589798</td>\n      <td>239.043201</td>\n      <td>57.046148</td>\n      <td>55.067141</td>\n      <td>15.751151</td>\n      <td>240.990999</td>\n      <td>232.666000</td>\n      <td>16.447619</td>\n    </tr>\n    <tr>\n      <th>58049</th>\n      <td>2017-06-26</td>\n      <td>32.220001</td>\n      <td>32.500000</td>\n      <td>32.180000</td>\n      <td>26.801701</td>\n      <td>22175000</td>\n      <td>CSCO</td>\n      <td>0</td>\n      <td>-0.102618</td>\n      <td>26.771634</td>\n      <td>25.978829</td>\n      <td>49.278702</td>\n      <td>56.338151</td>\n      <td>6.074628</td>\n      <td>26.526254</td>\n      <td>27.142789</td>\n      <td>22.070596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "0194749d-62ec-420f-9b54-492873c266a9",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:17.902505Z",
     "end_time": "2023-04-17T19:20:17.931609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 28, State Space: 281\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "AWyp84Ltto19",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:17.918693Z",
     "end_time": "2023-04-17T19:20:17.931733Z"
    }
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 10_000,\n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "v-gthCxMtj1d",
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:17.922479Z",
     "end_time": "2023-04-17T19:20:18.076709Z"
    }
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T19:20:18.034030Z",
     "end_time": "2023-04-17T19:20:18.100293Z"
    }
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 512\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 512\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 75_000,\n",
    "                 'ppo' : 75_000,\n",
    "                 'ddpg' : 75_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "b2a1cfbc-ced9-4d06-dd9a-4300845e1113",
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-04-17T18:15:39.620682Z",
     "end_time": "2023-04-17T18:25:15.320016Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  195.05389298558748\n",
      "======Model training from:  2009-04-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_4\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 203           |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -39.8         |\n",
      "|    explained_variance | -53.4         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -4.41         |\n",
      "|    reward             | -0.0054410887 |\n",
      "|    std                | 1             |\n",
      "|    value_loss         | 0.0584        |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -40        |\n",
      "|    explained_variance | -3.79      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -6.38      |\n",
      "|    reward             | 0.03546387 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.0327     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 195         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -40.3       |\n",
      "|    explained_variance | -1.56       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -2.08       |\n",
      "|    reward             | 0.033528723 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.00426     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 198          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 10           |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -40.7        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -0.312       |\n",
      "|    reward             | 0.0014911968 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 0.000175     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 199          |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 12           |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.2        |\n",
      "|    explained_variance | -6.75        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | 1.71         |\n",
      "|    reward             | 0.0016424379 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.00396      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 199           |\n",
      "|    iterations         | 600           |\n",
      "|    time_elapsed       | 15            |\n",
      "|    total_timesteps    | 3000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.5         |\n",
      "|    explained_variance | -39           |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 599           |\n",
      "|    policy_loss        | -2.01         |\n",
      "|    reward             | 0.00087006117 |\n",
      "|    std                | 1.07          |\n",
      "|    value_loss         | 0.00311       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 198           |\n",
      "|    iterations         | 700           |\n",
      "|    time_elapsed       | 17            |\n",
      "|    total_timesteps    | 3500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.8         |\n",
      "|    explained_variance | -341          |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 699           |\n",
      "|    policy_loss        | -4.09         |\n",
      "|    reward             | -0.0014341177 |\n",
      "|    std                | 1.08          |\n",
      "|    value_loss         | 0.0151        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 199          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 20           |\n",
      "|    total_timesteps    | 4000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.2        |\n",
      "|    explained_variance | -4.85        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | 0.219        |\n",
      "|    reward             | -0.004117039 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 9.47e-05     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | -46.7      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 3.48       |\n",
      "|    reward             | 0.00945251 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.00711    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 199         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.5       |\n",
      "|    explained_variance | 0.744       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -0.00131    |\n",
      "|    reward             | 0.005322346 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 1.17e-06    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 199          |\n",
      "|    iterations         | 1100         |\n",
      "|    time_elapsed       | 27           |\n",
      "|    total_timesteps    | 5500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -44.3        |\n",
      "|    explained_variance | 0.45         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1099         |\n",
      "|    policy_loss        | -0.104       |\n",
      "|    reward             | -0.016752014 |\n",
      "|    std                | 1.18         |\n",
      "|    value_loss         | 0.00022      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 199           |\n",
      "|    iterations         | 1200          |\n",
      "|    time_elapsed       | 30            |\n",
      "|    total_timesteps    | 6000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -45           |\n",
      "|    explained_variance | -1.87e+03     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1199          |\n",
      "|    policy_loss        | -2.45         |\n",
      "|    reward             | 0.00023546205 |\n",
      "|    std                | 1.21          |\n",
      "|    value_loss         | 0.0048        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 199          |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 32           |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -45.7        |\n",
      "|    explained_variance | -0.114       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | -0.511       |\n",
      "|    reward             | 0.0015446364 |\n",
      "|    std                | 1.24         |\n",
      "|    value_loss         | 0.000227     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 199           |\n",
      "|    iterations         | 1400          |\n",
      "|    time_elapsed       | 35            |\n",
      "|    total_timesteps    | 7000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -46.6         |\n",
      "|    explained_variance | 0.728         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1399          |\n",
      "|    policy_loss        | 0.461         |\n",
      "|    reward             | -0.0040725004 |\n",
      "|    std                | 1.28          |\n",
      "|    value_loss         | 0.000128      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 198          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 37           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -47.6        |\n",
      "|    explained_variance | -0.506       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | 0.188        |\n",
      "|    reward             | 0.0043145875 |\n",
      "|    std                | 1.33         |\n",
      "|    value_loss         | 2.86e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 198          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 40           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -48.7        |\n",
      "|    explained_variance | -0.0014      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | -0.526       |\n",
      "|    reward             | 0.0011338299 |\n",
      "|    std                | 1.38         |\n",
      "|    value_loss         | 0.000127     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 198          |\n",
      "|    iterations         | 1700         |\n",
      "|    time_elapsed       | 42           |\n",
      "|    total_timesteps    | 8500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -49.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1699         |\n",
      "|    policy_loss        | -0.123       |\n",
      "|    reward             | 0.0070043365 |\n",
      "|    std                | 1.44         |\n",
      "|    value_loss         | 1.44e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 198         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -50.8       |\n",
      "|    explained_variance | 0.151       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 0.119       |\n",
      "|    reward             | 0.007093701 |\n",
      "|    std                | 1.48        |\n",
      "|    value_loss         | 8.78e-06    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 198          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 47           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -51.7        |\n",
      "|    explained_variance | 0.857        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | -0.0797      |\n",
      "|    reward             | 0.0025771838 |\n",
      "|    std                | 1.53         |\n",
      "|    value_loss         | 4.54e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 198           |\n",
      "|    iterations         | 2000          |\n",
      "|    time_elapsed       | 50            |\n",
      "|    total_timesteps    | 10000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -52.7         |\n",
      "|    explained_variance | -2.87         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1999          |\n",
      "|    policy_loss        | 0.231         |\n",
      "|    reward             | -0.0011030465 |\n",
      "|    std                | 1.59          |\n",
      "|    value_loss         | 6.32e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 197         |\n",
      "|    iterations         | 2100        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 10500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -53.8       |\n",
      "|    explained_variance | 0.796       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2099        |\n",
      "|    policy_loss        | -0.395      |\n",
      "|    reward             | 0.017981144 |\n",
      "|    std                | 1.66        |\n",
      "|    value_loss         | 8.24e-05    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 199           |\n",
      "|    iterations         | 2200          |\n",
      "|    time_elapsed       | 55            |\n",
      "|    total_timesteps    | 11000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -55           |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2199          |\n",
      "|    policy_loss        | -0.696        |\n",
      "|    reward             | -0.0012532511 |\n",
      "|    std                | 1.73          |\n",
      "|    value_loss         | 0.000157      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 199          |\n",
      "|    iterations         | 2300         |\n",
      "|    time_elapsed       | 57           |\n",
      "|    total_timesteps    | 11500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -56          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2299         |\n",
      "|    policy_loss        | -0.091       |\n",
      "|    reward             | 0.0016911332 |\n",
      "|    std                | 1.79         |\n",
      "|    value_loss         | 2.25e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 199           |\n",
      "|    iterations         | 2400          |\n",
      "|    time_elapsed       | 60            |\n",
      "|    total_timesteps    | 12000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -56.8         |\n",
      "|    explained_variance | -1.2          |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2399          |\n",
      "|    policy_loss        | 0.699         |\n",
      "|    reward             | -0.0014576417 |\n",
      "|    std                | 1.84          |\n",
      "|    value_loss         | 0.000199      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 199           |\n",
      "|    iterations         | 2500          |\n",
      "|    time_elapsed       | 62            |\n",
      "|    total_timesteps    | 12500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -57.8         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2499          |\n",
      "|    policy_loss        | 0.0744        |\n",
      "|    reward             | -0.0026254086 |\n",
      "|    std                | 1.91          |\n",
      "|    value_loss         | 7.45e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 199           |\n",
      "|    iterations         | 2600          |\n",
      "|    time_elapsed       | 65            |\n",
      "|    total_timesteps    | 13000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -59           |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2599          |\n",
      "|    policy_loss        | -0.092        |\n",
      "|    reward             | -0.0020545444 |\n",
      "|    std                | 1.99          |\n",
      "|    value_loss         | 2.61e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 199          |\n",
      "|    iterations         | 2700         |\n",
      "|    time_elapsed       | 67           |\n",
      "|    total_timesteps    | 13500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -60.6        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2699         |\n",
      "|    policy_loss        | -0.144       |\n",
      "|    reward             | 0.0016650249 |\n",
      "|    std                | 2.11         |\n",
      "|    value_loss         | 7.07e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 197           |\n",
      "|    iterations         | 2800          |\n",
      "|    time_elapsed       | 70            |\n",
      "|    total_timesteps    | 14000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -62.2         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2799          |\n",
      "|    policy_loss        | 0.0713        |\n",
      "|    reward             | -0.0010880337 |\n",
      "|    std                | 2.23          |\n",
      "|    value_loss         | 4.95e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 198           |\n",
      "|    iterations         | 2900          |\n",
      "|    time_elapsed       | 72            |\n",
      "|    total_timesteps    | 14500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -63.8         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2899          |\n",
      "|    policy_loss        | 0.0276        |\n",
      "|    reward             | 0.00046060752 |\n",
      "|    std                | 2.36          |\n",
      "|    value_loss         | 1.47e-06      |\n",
      "-----------------------------------------\n",
      "day: 2959, episode: 5\n",
      "begin_total_asset: 10000.00\n",
      "end_total_asset: 5094.45\n",
      "total_reward: -4905.55\n",
      "total_cost: 4277.77\n",
      "total_trades: 42036\n",
      "Sharpe: 0.045\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 200           |\n",
      "|    iterations         | 3000          |\n",
      "|    time_elapsed       | 74            |\n",
      "|    total_timesteps    | 15000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -65           |\n",
      "|    explained_variance | -0.916        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2999          |\n",
      "|    policy_loss        | 0.611         |\n",
      "|    reward             | -0.0052058725 |\n",
      "|    std                | 2.47          |\n",
      "|    value_loss         | 0.000135      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 201          |\n",
      "|    iterations         | 3100         |\n",
      "|    time_elapsed       | 77           |\n",
      "|    total_timesteps    | 15500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -66.1        |\n",
      "|    explained_variance | 0.624        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3099         |\n",
      "|    policy_loss        | 0.0942       |\n",
      "|    reward             | -0.000943388 |\n",
      "|    std                | 2.57         |\n",
      "|    value_loss         | 2.6e-06      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 202          |\n",
      "|    iterations         | 3200         |\n",
      "|    time_elapsed       | 78           |\n",
      "|    total_timesteps    | 16000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -67.4        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3199         |\n",
      "|    policy_loss        | 0.0372       |\n",
      "|    reward             | 0.0016364944 |\n",
      "|    std                | 2.69         |\n",
      "|    value_loss         | 9.23e-07     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 204           |\n",
      "|    iterations         | 3300          |\n",
      "|    time_elapsed       | 80            |\n",
      "|    total_timesteps    | 16500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -68.7         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3299          |\n",
      "|    policy_loss        | 0.637         |\n",
      "|    reward             | -0.0035883172 |\n",
      "|    std                | 2.82          |\n",
      "|    value_loss         | 0.000112      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 205          |\n",
      "|    iterations         | 3400         |\n",
      "|    time_elapsed       | 82           |\n",
      "|    total_timesteps    | 17000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -70.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3399         |\n",
      "|    policy_loss        | -0.0535      |\n",
      "|    reward             | 0.0052482057 |\n",
      "|    std                | 2.96         |\n",
      "|    value_loss         | 1.12e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 206          |\n",
      "|    iterations         | 3500         |\n",
      "|    time_elapsed       | 84           |\n",
      "|    total_timesteps    | 17500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -71.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3499         |\n",
      "|    policy_loss        | -0.507       |\n",
      "|    reward             | 0.0025585163 |\n",
      "|    std                | 3.08         |\n",
      "|    value_loss         | 6.26e-05     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 207            |\n",
      "|    iterations         | 3600           |\n",
      "|    time_elapsed       | 86             |\n",
      "|    total_timesteps    | 18000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -72            |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 3599           |\n",
      "|    policy_loss        | 0.169          |\n",
      "|    reward             | -0.00016856776 |\n",
      "|    std                | 3.17           |\n",
      "|    value_loss         | 7.74e-06       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 209          |\n",
      "|    iterations         | 3700         |\n",
      "|    time_elapsed       | 88           |\n",
      "|    total_timesteps    | 18500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -73          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3699         |\n",
      "|    policy_loss        | 0.072        |\n",
      "|    reward             | 0.0027927796 |\n",
      "|    std                | 3.29         |\n",
      "|    value_loss         | 3.18e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 210            |\n",
      "|    iterations         | 3800           |\n",
      "|    time_elapsed       | 90             |\n",
      "|    total_timesteps    | 19000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -74.2          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 3799           |\n",
      "|    policy_loss        | 0.173          |\n",
      "|    reward             | -0.00033722987 |\n",
      "|    std                | 3.43           |\n",
      "|    value_loss         | 5.92e-06       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 211          |\n",
      "|    iterations         | 3900         |\n",
      "|    time_elapsed       | 92           |\n",
      "|    total_timesteps    | 19500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -75.7        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3899         |\n",
      "|    policy_loss        | 0.259        |\n",
      "|    reward             | 0.0004856987 |\n",
      "|    std                | 3.62         |\n",
      "|    value_loss         | 2.33e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 212           |\n",
      "|    iterations         | 4000          |\n",
      "|    time_elapsed       | 94            |\n",
      "|    total_timesteps    | 20000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -77.2         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3999          |\n",
      "|    policy_loss        | 0.881         |\n",
      "|    reward             | 0.00062478526 |\n",
      "|    std                | 3.81          |\n",
      "|    value_loss         | 0.000149      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 213           |\n",
      "|    iterations         | 4100          |\n",
      "|    time_elapsed       | 96            |\n",
      "|    total_timesteps    | 20500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -78.5         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4099          |\n",
      "|    policy_loss        | -0.48         |\n",
      "|    reward             | 2.1669037e-05 |\n",
      "|    std                | 4             |\n",
      "|    value_loss         | 9.97e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 214          |\n",
      "|    iterations         | 4200         |\n",
      "|    time_elapsed       | 98           |\n",
      "|    total_timesteps    | 21000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -79.6        |\n",
      "|    explained_variance | -0.0116      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4199         |\n",
      "|    policy_loss        | 0.246        |\n",
      "|    reward             | 0.0027228806 |\n",
      "|    std                | 4.15         |\n",
      "|    value_loss         | 9.98e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 214            |\n",
      "|    iterations         | 4300           |\n",
      "|    time_elapsed       | 100            |\n",
      "|    total_timesteps    | 21500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -80.8          |\n",
      "|    explained_variance | -0.0695        |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 4299           |\n",
      "|    policy_loss        | -0.226         |\n",
      "|    reward             | -0.00074609055 |\n",
      "|    std                | 4.35           |\n",
      "|    value_loss         | 3.18e-05       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 215           |\n",
      "|    iterations         | 4400          |\n",
      "|    time_elapsed       | 101           |\n",
      "|    total_timesteps    | 22000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -82.3         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4399          |\n",
      "|    policy_loss        | 0.141         |\n",
      "|    reward             | -0.0014241707 |\n",
      "|    std                | 4.59          |\n",
      "|    value_loss         | 3.22e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 216           |\n",
      "|    iterations         | 4500          |\n",
      "|    time_elapsed       | 103           |\n",
      "|    total_timesteps    | 22500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -84           |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4499          |\n",
      "|    policy_loss        | -0.15         |\n",
      "|    reward             | 0.00029142457 |\n",
      "|    std                | 4.87          |\n",
      "|    value_loss         | 3.55e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 217           |\n",
      "|    iterations         | 4600          |\n",
      "|    time_elapsed       | 105           |\n",
      "|    total_timesteps    | 23000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -85.8         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4599          |\n",
      "|    policy_loss        | 0.331         |\n",
      "|    reward             | -0.0015287178 |\n",
      "|    std                | 5.18          |\n",
      "|    value_loss         | 1.61e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 218          |\n",
      "|    iterations         | 4700         |\n",
      "|    time_elapsed       | 107          |\n",
      "|    total_timesteps    | 23500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -87.4        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4699         |\n",
      "|    policy_loss        | 0.0264       |\n",
      "|    reward             | 0.0024897878 |\n",
      "|    std                | 5.51         |\n",
      "|    value_loss         | 1.07e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 219           |\n",
      "|    iterations         | 4800          |\n",
      "|    time_elapsed       | 109           |\n",
      "|    total_timesteps    | 24000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -89           |\n",
      "|    explained_variance | 0.0692        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4799          |\n",
      "|    policy_loss        | -0.37         |\n",
      "|    reward             | -4.273448e-05 |\n",
      "|    std                | 5.82          |\n",
      "|    value_loss         | 2.39e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 219          |\n",
      "|    iterations         | 4900         |\n",
      "|    time_elapsed       | 111          |\n",
      "|    total_timesteps    | 24500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -90.6        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4899         |\n",
      "|    policy_loss        | -0.0689      |\n",
      "|    reward             | -0.001009404 |\n",
      "|    std                | 6.15         |\n",
      "|    value_loss         | 7.11e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 220           |\n",
      "|    iterations         | 5000          |\n",
      "|    time_elapsed       | 113           |\n",
      "|    total_timesteps    | 25000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -92.1         |\n",
      "|    explained_variance | -8.36         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4999          |\n",
      "|    policy_loss        | 0.166         |\n",
      "|    reward             | -0.0017911027 |\n",
      "|    std                | 6.51          |\n",
      "|    value_loss         | 8.07e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 221          |\n",
      "|    iterations         | 5100         |\n",
      "|    time_elapsed       | 115          |\n",
      "|    total_timesteps    | 25500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -93.7        |\n",
      "|    explained_variance | 0.0704       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5099         |\n",
      "|    policy_loss        | 0.129        |\n",
      "|    reward             | 0.0012918188 |\n",
      "|    std                | 6.88         |\n",
      "|    value_loss         | 2.82e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 221           |\n",
      "|    iterations         | 5200          |\n",
      "|    time_elapsed       | 117           |\n",
      "|    total_timesteps    | 26000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -95.2         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5199          |\n",
      "|    policy_loss        | -0.156        |\n",
      "|    reward             | -0.0045016157 |\n",
      "|    std                | 7.27          |\n",
      "|    value_loss         | 3.78e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 222           |\n",
      "|    iterations         | 5300          |\n",
      "|    time_elapsed       | 119           |\n",
      "|    total_timesteps    | 26500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -96.8         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5299          |\n",
      "|    policy_loss        | -0.227        |\n",
      "|    reward             | 0.00033704087 |\n",
      "|    std                | 7.68          |\n",
      "|    value_loss         | 7.77e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 223          |\n",
      "|    iterations         | 5400         |\n",
      "|    time_elapsed       | 121          |\n",
      "|    total_timesteps    | 27000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -98.1        |\n",
      "|    explained_variance | -17.2        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5399         |\n",
      "|    policy_loss        | 0.862        |\n",
      "|    reward             | 0.0014371269 |\n",
      "|    std                | 8.06         |\n",
      "|    value_loss         | 0.000152     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 223            |\n",
      "|    iterations         | 5500           |\n",
      "|    time_elapsed       | 122            |\n",
      "|    total_timesteps    | 27500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -99.5          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5499           |\n",
      "|    policy_loss        | -0.736         |\n",
      "|    reward             | -6.5980916e-05 |\n",
      "|    std                | 8.47           |\n",
      "|    value_loss         | 7.37e-05       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 224            |\n",
      "|    iterations         | 5600           |\n",
      "|    time_elapsed       | 124            |\n",
      "|    total_timesteps    | 28000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -101           |\n",
      "|    explained_variance | -0.228         |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5599           |\n",
      "|    policy_loss        | 0.338          |\n",
      "|    reward             | -0.00067965087 |\n",
      "|    std                | 8.94           |\n",
      "|    value_loss         | 1.31e-05       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 224            |\n",
      "|    iterations         | 5700           |\n",
      "|    time_elapsed       | 126            |\n",
      "|    total_timesteps    | 28500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -103           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5699           |\n",
      "|    policy_loss        | -0.0667        |\n",
      "|    reward             | -0.00040223546 |\n",
      "|    std                | 9.48           |\n",
      "|    value_loss         | 1.26e-06       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 225           |\n",
      "|    iterations         | 5800          |\n",
      "|    time_elapsed       | 128           |\n",
      "|    total_timesteps    | 29000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -104          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5799          |\n",
      "|    policy_loss        | -0.197        |\n",
      "|    reward             | -0.0049888236 |\n",
      "|    std                | 10.1          |\n",
      "|    value_loss         | 4.04e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 225           |\n",
      "|    iterations         | 5900          |\n",
      "|    time_elapsed       | 130           |\n",
      "|    total_timesteps    | 29500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -106          |\n",
      "|    explained_variance | 0.604         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5899          |\n",
      "|    policy_loss        | 0.22          |\n",
      "|    reward             | -0.0032349075 |\n",
      "|    std                | 10.7          |\n",
      "|    value_loss         | 5e-06         |\n",
      "-----------------------------------------\n",
      "day: 2959, episode: 10\n",
      "begin_total_asset: 10000.00\n",
      "end_total_asset: 2876.45\n",
      "total_reward: -7123.55\n",
      "total_cost: 4475.16\n",
      "total_trades: 44987\n",
      "Sharpe: -0.137\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 225            |\n",
      "|    iterations         | 6000           |\n",
      "|    time_elapsed       | 132            |\n",
      "|    total_timesteps    | 30000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -107           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5999           |\n",
      "|    policy_loss        | 0.0849         |\n",
      "|    reward             | -0.00011175966 |\n",
      "|    std                | 11.2           |\n",
      "|    value_loss         | 2.5e-06        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 226           |\n",
      "|    iterations         | 6100          |\n",
      "|    time_elapsed       | 134           |\n",
      "|    total_timesteps    | 30500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -109          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6099          |\n",
      "|    policy_loss        | 0.497         |\n",
      "|    reward             | 0.00018229439 |\n",
      "|    std                | 11.8          |\n",
      "|    value_loss         | 2.87e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 226           |\n",
      "|    iterations         | 6200          |\n",
      "|    time_elapsed       | 136           |\n",
      "|    total_timesteps    | 31000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -110          |\n",
      "|    explained_variance | -0.743        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6199          |\n",
      "|    policy_loss        | -0.939        |\n",
      "|    reward             | 2.7318165e-05 |\n",
      "|    std                | 12.5          |\n",
      "|    value_loss         | 7.47e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 226           |\n",
      "|    iterations         | 6300          |\n",
      "|    time_elapsed       | 138           |\n",
      "|    total_timesteps    | 31500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -112          |\n",
      "|    explained_variance | -0.0953       |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6299          |\n",
      "|    policy_loss        | -0.363        |\n",
      "|    reward             | 0.00063127227 |\n",
      "|    std                | 13.2          |\n",
      "|    value_loss         | 1.11e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 227           |\n",
      "|    iterations         | 6400          |\n",
      "|    time_elapsed       | 140           |\n",
      "|    total_timesteps    | 32000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -114          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6399          |\n",
      "|    policy_loss        | -0.0764       |\n",
      "|    reward             | -0.0057178154 |\n",
      "|    std                | 14            |\n",
      "|    value_loss         | 4.33e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 227            |\n",
      "|    iterations         | 6500           |\n",
      "|    time_elapsed       | 142            |\n",
      "|    total_timesteps    | 32500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -115           |\n",
      "|    explained_variance | -7.65          |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 6499           |\n",
      "|    policy_loss        | 1.07           |\n",
      "|    reward             | 0.000107881286 |\n",
      "|    std                | 14.9           |\n",
      "|    value_loss         | 0.00012        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 227           |\n",
      "|    iterations         | 6600          |\n",
      "|    time_elapsed       | 144           |\n",
      "|    total_timesteps    | 33000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -117          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6599          |\n",
      "|    policy_loss        | -0.298        |\n",
      "|    reward             | 2.5591138e-05 |\n",
      "|    std                | 15.6          |\n",
      "|    value_loss         | 7.6e-06       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 228           |\n",
      "|    iterations         | 6700          |\n",
      "|    time_elapsed       | 146           |\n",
      "|    total_timesteps    | 33500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -118          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6699          |\n",
      "|    policy_loss        | 0.224         |\n",
      "|    reward             | -0.0005458671 |\n",
      "|    std                | 16.5          |\n",
      "|    value_loss         | 4.06e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 228          |\n",
      "|    iterations         | 6800         |\n",
      "|    time_elapsed       | 148          |\n",
      "|    total_timesteps    | 34000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -120         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6799         |\n",
      "|    policy_loss        | -0.124       |\n",
      "|    reward             | 0.0023932757 |\n",
      "|    std                | 17.5         |\n",
      "|    value_loss         | 3.66e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 229          |\n",
      "|    iterations         | 6900         |\n",
      "|    time_elapsed       | 150          |\n",
      "|    total_timesteps    | 34500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -122         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6899         |\n",
      "|    policy_loss        | -0.11        |\n",
      "|    reward             | 0.0014331124 |\n",
      "|    std                | 18.7         |\n",
      "|    value_loss         | 2.25e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 229          |\n",
      "|    iterations         | 7000         |\n",
      "|    time_elapsed       | 152          |\n",
      "|    total_timesteps    | 35000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -123         |\n",
      "|    explained_variance | -0.0092      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6999         |\n",
      "|    policy_loss        | 0.0631       |\n",
      "|    reward             | 0.0008955193 |\n",
      "|    std                | 19.9         |\n",
      "|    value_loss         | 2.24e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 229            |\n",
      "|    iterations         | 7100           |\n",
      "|    time_elapsed       | 154            |\n",
      "|    total_timesteps    | 35500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -125           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 7099           |\n",
      "|    policy_loss        | -0.143         |\n",
      "|    reward             | -0.00082124176 |\n",
      "|    std                | 21.2           |\n",
      "|    value_loss         | 3.41e-06       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 229          |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 156          |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -127         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | -0.0864      |\n",
      "|    reward             | 0.0026342738 |\n",
      "|    std                | 22.4         |\n",
      "|    value_loss         | 7.37e-07     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 229            |\n",
      "|    iterations         | 7300           |\n",
      "|    time_elapsed       | 158            |\n",
      "|    total_timesteps    | 36500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -128           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 7299           |\n",
      "|    policy_loss        | -0.367         |\n",
      "|    reward             | -0.00029435437 |\n",
      "|    std                | 23.7           |\n",
      "|    value_loss         | 8.76e-06       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 230           |\n",
      "|    iterations         | 7400          |\n",
      "|    time_elapsed       | 160           |\n",
      "|    total_timesteps    | 37000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -130          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7399          |\n",
      "|    policy_loss        | 0.0172        |\n",
      "|    reward             | 0.00029257376 |\n",
      "|    std                | 25.1          |\n",
      "|    value_loss         | 2.38e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 230           |\n",
      "|    iterations         | 7500          |\n",
      "|    time_elapsed       | 162           |\n",
      "|    total_timesteps    | 37500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -132          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7499          |\n",
      "|    policy_loss        | 0.192         |\n",
      "|    reward             | -0.0022539368 |\n",
      "|    std                | 26.7          |\n",
      "|    value_loss         | 3.68e-06      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 230         |\n",
      "|    iterations         | 7600        |\n",
      "|    time_elapsed       | 164         |\n",
      "|    total_timesteps    | 38000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -133        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7599        |\n",
      "|    policy_loss        | 0.635       |\n",
      "|    reward             | 0.001101288 |\n",
      "|    std                | 28.5        |\n",
      "|    value_loss         | 2.51e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 231         |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 166         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -135        |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | -0.971      |\n",
      "|    reward             | 0.005471725 |\n",
      "|    std                | 30.4        |\n",
      "|    value_loss         | 8.89e-05    |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 231            |\n",
      "|    iterations         | 7800           |\n",
      "|    time_elapsed       | 168            |\n",
      "|    total_timesteps    | 39000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -137           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 7799           |\n",
      "|    policy_loss        | 0.313          |\n",
      "|    reward             | -0.00055871875 |\n",
      "|    std                | 32.1           |\n",
      "|    value_loss         | 5.31e-06       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 231            |\n",
      "|    iterations         | 7900           |\n",
      "|    time_elapsed       | 170            |\n",
      "|    total_timesteps    | 39500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -138           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 7899           |\n",
      "|    policy_loss        | -0.0303        |\n",
      "|    reward             | -0.00058132934 |\n",
      "|    std                | 34             |\n",
      "|    value_loss         | 2.66e-06       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 232           |\n",
      "|    iterations         | 8000          |\n",
      "|    time_elapsed       | 172           |\n",
      "|    total_timesteps    | 40000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -140          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7999          |\n",
      "|    policy_loss        | 0.267         |\n",
      "|    reward             | -0.0008057212 |\n",
      "|    std                | 36.2          |\n",
      "|    value_loss         | 4.12e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 232           |\n",
      "|    iterations         | 8100          |\n",
      "|    time_elapsed       | 174           |\n",
      "|    total_timesteps    | 40500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -142          |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8099          |\n",
      "|    policy_loss        | 0.0954        |\n",
      "|    reward             | 0.00017135925 |\n",
      "|    std                | 38.7          |\n",
      "|    value_loss         | 9.55e-07      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 232        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -144       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 0.122      |\n",
      "|    reward             | 0.00013744 |\n",
      "|    std                | 41.4       |\n",
      "|    value_loss         | 8.86e-07   |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 232           |\n",
      "|    iterations         | 8300          |\n",
      "|    time_elapsed       | 178           |\n",
      "|    total_timesteps    | 41500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -146          |\n",
      "|    explained_variance | -0.508        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8299          |\n",
      "|    policy_loss        | -0.499        |\n",
      "|    reward             | 0.00090112205 |\n",
      "|    std                | 44.2          |\n",
      "|    value_loss         | 2.97e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 233           |\n",
      "|    iterations         | 8400          |\n",
      "|    time_elapsed       | 180           |\n",
      "|    total_timesteps    | 42000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -147          |\n",
      "|    explained_variance | 0.45          |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8399          |\n",
      "|    policy_loss        | -0.712        |\n",
      "|    reward             | -0.0004525568 |\n",
      "|    std                | 46.8          |\n",
      "|    value_loss         | 2.47e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 233         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 182         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -149        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -0.185      |\n",
      "|    reward             | 0.001893693 |\n",
      "|    std                | 49.6        |\n",
      "|    value_loss         | 1.97e-06    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 233          |\n",
      "|    iterations         | 8600         |\n",
      "|    time_elapsed       | 184          |\n",
      "|    total_timesteps    | 43000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -151         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8599         |\n",
      "|    policy_loss        | -0.0756      |\n",
      "|    reward             | -0.001518203 |\n",
      "|    std                | 52.8         |\n",
      "|    value_loss         | 8.21e-07     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 233           |\n",
      "|    iterations         | 8700          |\n",
      "|    time_elapsed       | 186           |\n",
      "|    total_timesteps    | 43500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -152          |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8699          |\n",
      "|    policy_loss        | -0.288        |\n",
      "|    reward             | -9.823933e-05 |\n",
      "|    std                | 56.2          |\n",
      "|    value_loss         | 3.96e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 233          |\n",
      "|    iterations         | 8800         |\n",
      "|    time_elapsed       | 188          |\n",
      "|    total_timesteps    | 44000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -154         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8799         |\n",
      "|    policy_loss        | -0.036       |\n",
      "|    reward             | 0.0029258567 |\n",
      "|    std                | 60           |\n",
      "|    value_loss         | 8.74e-07     |\n",
      "----------------------------------------\n",
      "day: 2959, episode: 15\n",
      "begin_total_asset: 10000.00\n",
      "end_total_asset: 1298.96\n",
      "total_reward: -8701.04\n",
      "total_cost: 4673.24\n",
      "total_trades: 44722\n",
      "Sharpe: -0.312\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 234           |\n",
      "|    iterations         | 8900          |\n",
      "|    time_elapsed       | 189           |\n",
      "|    total_timesteps    | 44500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -156          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8899          |\n",
      "|    policy_loss        | -1.58         |\n",
      "|    reward             | 0.00084980816 |\n",
      "|    std                | 63.9          |\n",
      "|    value_loss         | 0.00011       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 234           |\n",
      "|    iterations         | 9000          |\n",
      "|    time_elapsed       | 191           |\n",
      "|    total_timesteps    | 45000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -157          |\n",
      "|    explained_variance | 0.548         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8999          |\n",
      "|    policy_loss        | -0.825        |\n",
      "|    reward             | -0.0012552241 |\n",
      "|    std                | 67.1          |\n",
      "|    value_loss         | 3.47e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 234           |\n",
      "|    iterations         | 9100          |\n",
      "|    time_elapsed       | 193           |\n",
      "|    total_timesteps    | 45500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -159          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9099          |\n",
      "|    policy_loss        | 0.204         |\n",
      "|    reward             | -0.0011058028 |\n",
      "|    std                | 70.8          |\n",
      "|    value_loss         | 5.72e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 234          |\n",
      "|    iterations         | 9200         |\n",
      "|    time_elapsed       | 195          |\n",
      "|    total_timesteps    | 46000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -161         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9199         |\n",
      "|    policy_loss        | -0.453       |\n",
      "|    reward             | 0.0005368051 |\n",
      "|    std                | 75           |\n",
      "|    value_loss         | 9.84e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 235          |\n",
      "|    iterations         | 9300         |\n",
      "|    time_elapsed       | 197          |\n",
      "|    total_timesteps    | 46500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -162         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9299         |\n",
      "|    policy_loss        | 0.0706       |\n",
      "|    reward             | 0.0008738475 |\n",
      "|    std                | 79.8         |\n",
      "|    value_loss         | 1.13e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 235           |\n",
      "|    iterations         | 9400          |\n",
      "|    time_elapsed       | 199           |\n",
      "|    total_timesteps    | 47000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -164          |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9399          |\n",
      "|    policy_loss        | -0.212        |\n",
      "|    reward             | -0.0018528007 |\n",
      "|    std                | 84.9          |\n",
      "|    value_loss         | 3.55e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 235          |\n",
      "|    iterations         | 9500         |\n",
      "|    time_elapsed       | 201          |\n",
      "|    total_timesteps    | 47500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -166         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9499         |\n",
      "|    policy_loss        | -2.26        |\n",
      "|    reward             | -0.002959846 |\n",
      "|    std                | 90.2         |\n",
      "|    value_loss         | 0.00019      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 235         |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 203         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -167        |\n",
      "|    explained_variance | -2.95       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | 0.16        |\n",
      "|    reward             | 0.002939457 |\n",
      "|    std                | 95.4        |\n",
      "|    value_loss         | 8.73e-06    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 235          |\n",
      "|    iterations         | 9700         |\n",
      "|    time_elapsed       | 205          |\n",
      "|    total_timesteps    | 48500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -169         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9699         |\n",
      "|    policy_loss        | -0.463       |\n",
      "|    reward             | 0.0050854753 |\n",
      "|    std                | 101          |\n",
      "|    value_loss         | 8.12e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 236           |\n",
      "|    iterations         | 9800          |\n",
      "|    time_elapsed       | 207           |\n",
      "|    total_timesteps    | 49000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -171          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9799          |\n",
      "|    policy_loss        | 0.162         |\n",
      "|    reward             | -0.0004361119 |\n",
      "|    std                | 107           |\n",
      "|    value_loss         | 9.73e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 235           |\n",
      "|    iterations         | 9900          |\n",
      "|    time_elapsed       | 210           |\n",
      "|    total_timesteps    | 49500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -172          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9899          |\n",
      "|    policy_loss        | -0.168        |\n",
      "|    reward             | 0.00012596867 |\n",
      "|    std                | 115           |\n",
      "|    value_loss         | 1.22e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 235            |\n",
      "|    iterations         | 10000          |\n",
      "|    time_elapsed       | 212            |\n",
      "|    total_timesteps    | 50000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -174           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 9999           |\n",
      "|    policy_loss        | -0.0622        |\n",
      "|    reward             | -6.4260865e-05 |\n",
      "|    std                | 123            |\n",
      "|    value_loss         | 1.92e-07       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 235           |\n",
      "|    iterations         | 10100         |\n",
      "|    time_elapsed       | 214           |\n",
      "|    total_timesteps    | 50500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -176          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10099         |\n",
      "|    policy_loss        | 0.45          |\n",
      "|    reward             | 0.00013276294 |\n",
      "|    std                | 131           |\n",
      "|    value_loss         | 6.9e-06       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 234           |\n",
      "|    iterations         | 10200         |\n",
      "|    time_elapsed       | 217           |\n",
      "|    total_timesteps    | 51000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -178          |\n",
      "|    explained_variance | 0.72          |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10199         |\n",
      "|    policy_loss        | -0.163        |\n",
      "|    reward             | 0.00065717095 |\n",
      "|    std                | 138           |\n",
      "|    value_loss         | 1.03e-06      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 234         |\n",
      "|    iterations         | 10300       |\n",
      "|    time_elapsed       | 219         |\n",
      "|    total_timesteps    | 51500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -179        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10299       |\n",
      "|    policy_loss        | -0.037      |\n",
      "|    reward             | 0.001116257 |\n",
      "|    std                | 147         |\n",
      "|    value_loss         | 2.53e-07    |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                 |                 |\n",
      "|    fps                | 234             |\n",
      "|    iterations         | 10400           |\n",
      "|    time_elapsed       | 221             |\n",
      "|    total_timesteps    | 52000           |\n",
      "| train/                |                 |\n",
      "|    entropy_loss       | -181            |\n",
      "|    explained_variance | 5.96e-08        |\n",
      "|    learning_rate      | 0.0007          |\n",
      "|    n_updates          | 10399           |\n",
      "|    policy_loss        | 0.0058          |\n",
      "|    reward             | -0.000103320315 |\n",
      "|    std                | 157             |\n",
      "|    value_loss         | 4.36e-08        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 234            |\n",
      "|    iterations         | 10500          |\n",
      "|    time_elapsed       | 223            |\n",
      "|    total_timesteps    | 52500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -183           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 10499          |\n",
      "|    policy_loss        | 0.409          |\n",
      "|    reward             | -4.5520355e-05 |\n",
      "|    std                | 168            |\n",
      "|    value_loss         | 6.63e-06       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                 |                 |\n",
      "|    fps                | 234             |\n",
      "|    iterations         | 10600           |\n",
      "|    time_elapsed       | 225             |\n",
      "|    total_timesteps    | 53000           |\n",
      "| train/                |                 |\n",
      "|    entropy_loss       | -185            |\n",
      "|    explained_variance | 0               |\n",
      "|    learning_rate      | 0.0007          |\n",
      "|    n_updates          | 10599           |\n",
      "|    policy_loss        | -0.172          |\n",
      "|    reward             | -0.000115429684 |\n",
      "|    std                | 180             |\n",
      "|    value_loss         | 8.76e-07        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 234          |\n",
      "|    iterations         | 10700        |\n",
      "|    time_elapsed       | 227          |\n",
      "|    total_timesteps    | 53500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -187         |\n",
      "|    explained_variance | 0.211        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10699        |\n",
      "|    policy_loss        | -0.322       |\n",
      "|    reward             | 0.0051644924 |\n",
      "|    std                | 191          |\n",
      "|    value_loss         | 5.33e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 235            |\n",
      "|    iterations         | 10800          |\n",
      "|    time_elapsed       | 229            |\n",
      "|    total_timesteps    | 54000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -188           |\n",
      "|    explained_variance | 5.96e-08       |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 10799          |\n",
      "|    policy_loss        | -0.188         |\n",
      "|    reward             | -0.00057906023 |\n",
      "|    std                | 203            |\n",
      "|    value_loss         | 1.33e-06       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 235           |\n",
      "|    iterations         | 10900         |\n",
      "|    time_elapsed       | 231           |\n",
      "|    total_timesteps    | 54500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -190          |\n",
      "|    explained_variance | -0.267        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10899         |\n",
      "|    policy_loss        | -0.633        |\n",
      "|    reward             | 0.00023215545 |\n",
      "|    std                | 216           |\n",
      "|    value_loss         | 1.31e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 235         |\n",
      "|    iterations         | 11000       |\n",
      "|    time_elapsed       | 233         |\n",
      "|    total_timesteps    | 55000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -192        |\n",
      "|    explained_variance | 0.215       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10999       |\n",
      "|    policy_loss        | -0.0694     |\n",
      "|    reward             | 0.002067542 |\n",
      "|    std                | 229         |\n",
      "|    value_loss         | 3.13e-07    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 235          |\n",
      "|    iterations         | 11100        |\n",
      "|    time_elapsed       | 235          |\n",
      "|    total_timesteps    | 55500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -194         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 11099        |\n",
      "|    policy_loss        | 0.0865       |\n",
      "|    reward             | 0.0018060859 |\n",
      "|    std                | 245          |\n",
      "|    value_loss         | 5.74e-07     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 235            |\n",
      "|    iterations         | 11200          |\n",
      "|    time_elapsed       | 237            |\n",
      "|    total_timesteps    | 56000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -195           |\n",
      "|    explained_variance | 5.96e-08       |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 11199          |\n",
      "|    policy_loss        | 0.144          |\n",
      "|    reward             | -0.00056224363 |\n",
      "|    std                | 261            |\n",
      "|    value_loss         | 6.53e-07       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 235           |\n",
      "|    iterations         | 11300         |\n",
      "|    time_elapsed       | 239           |\n",
      "|    total_timesteps    | 56500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -197          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 11299         |\n",
      "|    policy_loss        | 0.183         |\n",
      "|    reward             | -0.0002629302 |\n",
      "|    std                | 277           |\n",
      "|    value_loss         | 1.78e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 235           |\n",
      "|    iterations         | 11400         |\n",
      "|    time_elapsed       | 241           |\n",
      "|    total_timesteps    | 57000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -199          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 11399         |\n",
      "|    policy_loss        | -1            |\n",
      "|    reward             | -0.0025464154 |\n",
      "|    std                | 293           |\n",
      "|    value_loss         | 2.56e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 235           |\n",
      "|    iterations         | 11500         |\n",
      "|    time_elapsed       | 243           |\n",
      "|    total_timesteps    | 57500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -200          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 11499         |\n",
      "|    policy_loss        | 0.166         |\n",
      "|    reward             | -0.0011993338 |\n",
      "|    std                | 311           |\n",
      "|    value_loss         | 5.41e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 236          |\n",
      "|    iterations         | 11600        |\n",
      "|    time_elapsed       | 245          |\n",
      "|    total_timesteps    | 58000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -202         |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 11599        |\n",
      "|    policy_loss        | 0.246        |\n",
      "|    reward             | 0.0011210714 |\n",
      "|    std                | 330          |\n",
      "|    value_loss         | 3.14e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 236           |\n",
      "|    iterations         | 11700         |\n",
      "|    time_elapsed       | 247           |\n",
      "|    total_timesteps    | 58500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -204          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 11699         |\n",
      "|    policy_loss        | -0.263        |\n",
      "|    reward             | -0.0029386333 |\n",
      "|    std                | 352           |\n",
      "|    value_loss         | 2.08e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 236           |\n",
      "|    iterations         | 11800         |\n",
      "|    time_elapsed       | 249           |\n",
      "|    total_timesteps    | 59000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -206          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 11799         |\n",
      "|    policy_loss        | -0.986        |\n",
      "|    reward             | -0.0017339432 |\n",
      "|    std                | 376           |\n",
      "|    value_loss         | 3.07e-05      |\n",
      "-----------------------------------------\n",
      "day: 2959, episode: 20\n",
      "begin_total_asset: 10000.00\n",
      "end_total_asset: 584.59\n",
      "total_reward: -9415.41\n",
      "total_cost: 4346.57\n",
      "total_trades: 44620\n",
      "Sharpe: -0.450\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 236        |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 251        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -207       |\n",
      "|    explained_variance | -0.555     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | 0.291      |\n",
      "|    reward             | 0.00477588 |\n",
      "|    std                | 399        |\n",
      "|    value_loss         | 1.48e-05   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 236         |\n",
      "|    iterations         | 12000       |\n",
      "|    time_elapsed       | 253         |\n",
      "|    total_timesteps    | 60000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -209        |\n",
      "|    explained_variance | -6.81       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11999       |\n",
      "|    policy_loss        | -1.12       |\n",
      "|    reward             | 0.001935845 |\n",
      "|    std                | 423         |\n",
      "|    value_loss         | 4.4e-05     |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 236            |\n",
      "|    iterations         | 12100          |\n",
      "|    time_elapsed       | 255            |\n",
      "|    total_timesteps    | 60500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -211           |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 12099          |\n",
      "|    policy_loss        | 0.182          |\n",
      "|    reward             | -0.00014237734 |\n",
      "|    std                | 450            |\n",
      "|    value_loss         | 7.79e-07       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 237           |\n",
      "|    iterations         | 12200         |\n",
      "|    time_elapsed       | 257           |\n",
      "|    total_timesteps    | 61000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -212          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 12199         |\n",
      "|    policy_loss        | -0.568        |\n",
      "|    reward             | 0.00020216481 |\n",
      "|    std                | 479           |\n",
      "|    value_loss         | 9.66e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 237          |\n",
      "|    iterations         | 12300        |\n",
      "|    time_elapsed       | 259          |\n",
      "|    total_timesteps    | 61500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -214         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12299        |\n",
      "|    policy_loss        | 0.0539       |\n",
      "|    reward             | 0.0007111818 |\n",
      "|    std                | 511          |\n",
      "|    value_loss         | 1.06e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 237          |\n",
      "|    iterations         | 12400        |\n",
      "|    time_elapsed       | 261          |\n",
      "|    total_timesteps    | 62000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -216         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12399        |\n",
      "|    policy_loss        | -0.154       |\n",
      "|    reward             | 0.0009931062 |\n",
      "|    std                | 546          |\n",
      "|    value_loss         | 1.14e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 237          |\n",
      "|    iterations         | 12500        |\n",
      "|    time_elapsed       | 263          |\n",
      "|    total_timesteps    | 62500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -218         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12499        |\n",
      "|    policy_loss        | 0.00474      |\n",
      "|    reward             | 8.173573e-05 |\n",
      "|    std                | 580          |\n",
      "|    value_loss         | 2.58e-08     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 237         |\n",
      "|    iterations         | 12600       |\n",
      "|    time_elapsed       | 265         |\n",
      "|    total_timesteps    | 63000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -219        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12599       |\n",
      "|    policy_loss        | -0.11       |\n",
      "|    reward             | 0.000442302 |\n",
      "|    std                | 616         |\n",
      "|    value_loss         | 2.99e-07    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 237          |\n",
      "|    iterations         | 12700        |\n",
      "|    time_elapsed       | 266          |\n",
      "|    total_timesteps    | 63500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -221         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12699        |\n",
      "|    policy_loss        | 0.0376       |\n",
      "|    reward             | 6.597121e-05 |\n",
      "|    std                | 658          |\n",
      "|    value_loss         | 3.3e-08      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 238            |\n",
      "|    iterations         | 12800          |\n",
      "|    time_elapsed       | 268            |\n",
      "|    total_timesteps    | 64000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -223           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 12799          |\n",
      "|    policy_loss        | 0.00224        |\n",
      "|    reward             | -6.7615845e-05 |\n",
      "|    std                | 705            |\n",
      "|    value_loss         | 2.77e-10       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 238            |\n",
      "|    iterations         | 12900          |\n",
      "|    time_elapsed       | 270            |\n",
      "|    total_timesteps    | 64500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -225           |\n",
      "|    explained_variance | 1.19e-07       |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 12899          |\n",
      "|    policy_loss        | -0.0223        |\n",
      "|    reward             | -1.1260986e-05 |\n",
      "|    std                | 755            |\n",
      "|    value_loss         | 1.08e-08       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 238          |\n",
      "|    iterations         | 13000        |\n",
      "|    time_elapsed       | 272          |\n",
      "|    total_timesteps    | 65000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -227         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12999        |\n",
      "|    policy_loss        | 0.0244       |\n",
      "|    reward             | 8.604622e-05 |\n",
      "|    std                | 809          |\n",
      "|    value_loss         | 1.5e-08      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 238           |\n",
      "|    iterations         | 13100         |\n",
      "|    time_elapsed       | 274           |\n",
      "|    total_timesteps    | 65500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -229          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13099         |\n",
      "|    policy_loss        | -0.531        |\n",
      "|    reward             | -0.0028286055 |\n",
      "|    std                | 863           |\n",
      "|    value_loss         | 5.66e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 238           |\n",
      "|    iterations         | 13200         |\n",
      "|    time_elapsed       | 277           |\n",
      "|    total_timesteps    | 66000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -230          |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13199         |\n",
      "|    policy_loss        | -0.0338       |\n",
      "|    reward             | -0.0025177298 |\n",
      "|    std                | 917           |\n",
      "|    value_loss         | 1.48e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 238          |\n",
      "|    iterations         | 13300        |\n",
      "|    time_elapsed       | 279          |\n",
      "|    total_timesteps    | 66500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -232         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13299        |\n",
      "|    policy_loss        | 0.0387       |\n",
      "|    reward             | 0.0022364415 |\n",
      "|    std                | 975          |\n",
      "|    value_loss         | 1.24e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 238           |\n",
      "|    iterations         | 13400         |\n",
      "|    time_elapsed       | 281           |\n",
      "|    total_timesteps    | 67000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -234          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13399         |\n",
      "|    policy_loss        | 0.555         |\n",
      "|    reward             | -0.0022995484 |\n",
      "|    std                | 1.04e+03      |\n",
      "|    value_loss         | 6.09e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 238          |\n",
      "|    iterations         | 13500        |\n",
      "|    time_elapsed       | 282          |\n",
      "|    total_timesteps    | 67500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -236         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13499        |\n",
      "|    policy_loss        | -0.292       |\n",
      "|    reward             | -0.001179863 |\n",
      "|    std                | 1.11e+03     |\n",
      "|    value_loss         | 2.3e-06      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 238          |\n",
      "|    iterations         | 13600        |\n",
      "|    time_elapsed       | 284          |\n",
      "|    total_timesteps    | 68000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -238         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13599        |\n",
      "|    policy_loss        | 0.727        |\n",
      "|    reward             | 0.0006290546 |\n",
      "|    std                | 1.18e+03     |\n",
      "|    value_loss         | 9.77e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 238           |\n",
      "|    iterations         | 13700         |\n",
      "|    time_elapsed       | 287           |\n",
      "|    total_timesteps    | 68500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -239          |\n",
      "|    explained_variance | 0.0115        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13699         |\n",
      "|    policy_loss        | 1.23          |\n",
      "|    reward             | -0.0012884475 |\n",
      "|    std                | 1.24e+03      |\n",
      "|    value_loss         | 3.77e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 238          |\n",
      "|    iterations         | 13800        |\n",
      "|    time_elapsed       | 288          |\n",
      "|    total_timesteps    | 69000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -241         |\n",
      "|    explained_variance | 0.653        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13799        |\n",
      "|    policy_loss        | -0.467       |\n",
      "|    reward             | -0.001446976 |\n",
      "|    std                | 1.31e+03     |\n",
      "|    value_loss         | 4.48e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 238            |\n",
      "|    iterations         | 13900          |\n",
      "|    time_elapsed       | 290            |\n",
      "|    total_timesteps    | 69500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -242           |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 13899          |\n",
      "|    policy_loss        | -0.147         |\n",
      "|    reward             | -0.00014440708 |\n",
      "|    std                | 1.39e+03       |\n",
      "|    value_loss         | 7.05e-07       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 238          |\n",
      "|    iterations         | 14000        |\n",
      "|    time_elapsed       | 293          |\n",
      "|    total_timesteps    | 70000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -244         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13999        |\n",
      "|    policy_loss        | 0.364        |\n",
      "|    reward             | -0.002795755 |\n",
      "|    std                | 1.48e+03     |\n",
      "|    value_loss         | 2.45e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 239          |\n",
      "|    iterations         | 14100        |\n",
      "|    time_elapsed       | 294          |\n",
      "|    total_timesteps    | 70500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -246         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14099        |\n",
      "|    policy_loss        | 0.678        |\n",
      "|    reward             | 0.0005575945 |\n",
      "|    std                | 1.58e+03     |\n",
      "|    value_loss         | 8.45e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 238           |\n",
      "|    iterations         | 14200         |\n",
      "|    time_elapsed       | 297           |\n",
      "|    total_timesteps    | 71000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -248          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 14199         |\n",
      "|    policy_loss        | -0.468        |\n",
      "|    reward             | -0.0024296443 |\n",
      "|    std                | 1.69e+03      |\n",
      "|    value_loss         | 3.9e-06       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 239            |\n",
      "|    iterations         | 14300          |\n",
      "|    time_elapsed       | 299            |\n",
      "|    total_timesteps    | 71500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -249           |\n",
      "|    explained_variance | -0.573         |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 14299          |\n",
      "|    policy_loss        | 0.665          |\n",
      "|    reward             | -0.00016835425 |\n",
      "|    std                | 1.79e+03       |\n",
      "|    value_loss         | 1.04e-05       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 239           |\n",
      "|    iterations         | 14400         |\n",
      "|    time_elapsed       | 300           |\n",
      "|    total_timesteps    | 72000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -251          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 14399         |\n",
      "|    policy_loss        | 0.622         |\n",
      "|    reward             | 0.00010311119 |\n",
      "|    std                | 1.9e+03       |\n",
      "|    value_loss         | 6.39e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 239          |\n",
      "|    iterations         | 14500        |\n",
      "|    time_elapsed       | 302          |\n",
      "|    total_timesteps    | 72500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -253         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14499        |\n",
      "|    policy_loss        | -0.035       |\n",
      "|    reward             | -0.006161648 |\n",
      "|    std                | 2.02e+03     |\n",
      "|    value_loss         | 2.08e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 239           |\n",
      "|    iterations         | 14600         |\n",
      "|    time_elapsed       | 304           |\n",
      "|    total_timesteps    | 73000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -254          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 14599         |\n",
      "|    policy_loss        | -0.127        |\n",
      "|    reward             | -0.0013822516 |\n",
      "|    std                | 2.14e+03      |\n",
      "|    value_loss         | 3.98e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 239           |\n",
      "|    iterations         | 14700         |\n",
      "|    time_elapsed       | 306           |\n",
      "|    total_timesteps    | 73500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -256          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 14699         |\n",
      "|    policy_loss        | 2.78          |\n",
      "|    reward             | -0.0014406155 |\n",
      "|    std                | 2.28e+03      |\n",
      "|    value_loss         | 0.000169      |\n",
      "-----------------------------------------\n",
      "day: 2959, episode: 25\n",
      "begin_total_asset: 10000.00\n",
      "end_total_asset: 1836.71\n",
      "total_reward: -8163.29\n",
      "total_cost: 6110.31\n",
      "total_trades: 44668\n",
      "Sharpe: -0.264\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 239          |\n",
      "|    iterations         | 14800        |\n",
      "|    time_elapsed       | 308          |\n",
      "|    total_timesteps    | 74000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -258         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14799        |\n",
      "|    policy_loss        | 0.156        |\n",
      "|    reward             | 0.0015269959 |\n",
      "|    std                | 2.42e+03     |\n",
      "|    value_loss         | 7.46e-07     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 239           |\n",
      "|    iterations         | 14900         |\n",
      "|    time_elapsed       | 310           |\n",
      "|    total_timesteps    | 74500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -259          |\n",
      "|    explained_variance | 0.574         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 14899         |\n",
      "|    policy_loss        | -0.889        |\n",
      "|    reward             | -0.0009972032 |\n",
      "|    std                | 2.55e+03      |\n",
      "|    value_loss         | 1.55e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 239          |\n",
      "|    iterations         | 15000        |\n",
      "|    time_elapsed       | 312          |\n",
      "|    total_timesteps    | 75000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -261         |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14999        |\n",
      "|    policy_loss        | -0.24        |\n",
      "|    reward             | 0.0013115914 |\n",
      "|    std                | 2.68e+03     |\n",
      "|    value_loss         | 1.97e-06     |\n",
      "----------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  -0.0779828655771008\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 512}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_3\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "b5d9cb94-51a9-4569-a9a8-7e18f1139f4e",
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.321723Z",
     "end_time": "2023-04-17T18:25:15.323056Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4JKB--8tj1g",
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.323168Z",
     "end_time": "2023-04-17T18:25:15.332036Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "8b89807b-ff71-4902-dd45-1f9111788cbb",
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.332670Z",
     "end_time": "2023-04-17T18:25:15.338362Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = df_account_value.append(temp,ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "d2dc62c2-e2a0-48fc-8183-0399d1b27f53",
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.339541Z",
     "end_time": "2023-04-17T18:25:15.546910Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "9a874df9-2c5f-423c-8fd2-8966aab63fc0",
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.380920Z",
     "end_time": "2023-04-17T18:25:15.547081Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "b419a565-8c15-47d8-f66c-00f81c3c526d",
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.402997Z",
     "end_time": "2023-04-17T18:25:15.547171Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "903ef035-f9f4-4678-d18a-1516254eaf3e",
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.408088Z",
     "end_time": "2023-04-17T18:25:15.630198Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "e61a64e0-58ed-4490-b19a-78bd4f76e666",
    "ExecuteTime": {
     "start_time": "2023-04-17T18:25:15.600932Z",
     "end_time": "2023-04-17T18:25:19.024991Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"==============Compare to DJIA===========\")\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "backtest_plot(df_account_value, \n",
    "              baseline_ticker = '^DJI', \n",
    "              baseline_start = df_account_value.loc[0,'date'],\n",
    "              baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "oBQx4bVQFi-a"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
