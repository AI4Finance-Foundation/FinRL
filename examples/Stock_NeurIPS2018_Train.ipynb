{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uijiWgkuh1jB",
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01",
        "bEv5KGC8h1jE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Install Packages"
      ],
      "metadata": {
        "id": "gT-zXutMgqOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install required packages\n",
        "!pip install swig\n",
        "!pip install wrds\n",
        "!pip install pyportfolioopt\n",
        "## install finrl library\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ],
      "metadata": {
        "id": "D0vEcPxSJ8hI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl.meta.data_processor import DataProcessor"
      ],
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from finrl import config\n",
        "from finrl import config_tickers\n",
        "import os\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    DATA_SAVE_DIR,\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        "    TRAIN_START_DATE,\n",
        "    TRAIN_END_DATE,\n",
        "    TEST_START_DATE,\n",
        "    TEST_END_DATE,\n",
        "    TRADE_START_DATE,\n",
        "    TRADE_END_DATE,\n",
        ")\n",
        "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
      ],
      "metadata": {
        "id": "wZ7Bl7i6I2AM"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ],
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_full = pd.read_csv('full_data.csv')"
      ],
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_full.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "1leIVVljwRGP",
        "outputId": "404200e4-0cf1-4244-d662-d95feda5e2ae"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date   tic       open       high        low      close       volume  \\\n",
              "0  2009-01-02  AAPL   3.067143   3.251429   3.041429   2.762747  746015200.0   \n",
              "1  2009-01-02  AMGN  58.590000  59.080002  57.750000  44.219173    6547900.0   \n",
              "2  2009-01-02   AXP  18.570000  19.520000  18.400000  15.365306   10955700.0   \n",
              "3  2009-01-02    BA  42.799999  45.560001  42.779999  33.941101    7010200.0   \n",
              "4  2009-01-02   CAT  44.910000  46.980000  44.709999  31.579334    7117200.0   \n",
              "\n",
              "   day  macd   boll_ub  boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
              "0  4.0   0.0  2.985942  2.65615   100.0  66.666667  100.0      2.762747   \n",
              "1  4.0   0.0  2.985942  2.65615   100.0  66.666667  100.0     44.219173   \n",
              "2  4.0   0.0  2.985942  2.65615   100.0  66.666667  100.0     15.365306   \n",
              "3  4.0   0.0  2.985942  2.65615   100.0  66.666667  100.0     33.941101   \n",
              "4  4.0   0.0  2.985942  2.65615   100.0  66.666667  100.0     31.579334   \n",
              "\n",
              "   close_60_sma        vix  turbulence  \n",
              "0      2.762747  39.189999         0.0  \n",
              "1     44.219173  39.189999         0.0  \n",
              "2     15.365306  39.189999         0.0  \n",
              "3     33.941101  39.189999         0.0  \n",
              "4     31.579334  39.189999         0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aebbffcd-78e5-41c4-bd87-9dd4b92d9c17\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>tic</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>vix</th>\n",
              "      <th>turbulence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>3.067143</td>\n",
              "      <td>3.251429</td>\n",
              "      <td>3.041429</td>\n",
              "      <td>2.762747</td>\n",
              "      <td>746015200.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.985942</td>\n",
              "      <td>2.65615</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.762747</td>\n",
              "      <td>2.762747</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>AMGN</td>\n",
              "      <td>58.590000</td>\n",
              "      <td>59.080002</td>\n",
              "      <td>57.750000</td>\n",
              "      <td>44.219173</td>\n",
              "      <td>6547900.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.985942</td>\n",
              "      <td>2.65615</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>44.219173</td>\n",
              "      <td>44.219173</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>AXP</td>\n",
              "      <td>18.570000</td>\n",
              "      <td>19.520000</td>\n",
              "      <td>18.400000</td>\n",
              "      <td>15.365306</td>\n",
              "      <td>10955700.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.985942</td>\n",
              "      <td>2.65615</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.365306</td>\n",
              "      <td>15.365306</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>BA</td>\n",
              "      <td>42.799999</td>\n",
              "      <td>45.560001</td>\n",
              "      <td>42.779999</td>\n",
              "      <td>33.941101</td>\n",
              "      <td>7010200.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.985942</td>\n",
              "      <td>2.65615</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>33.941101</td>\n",
              "      <td>33.941101</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>CAT</td>\n",
              "      <td>44.910000</td>\n",
              "      <td>46.980000</td>\n",
              "      <td>44.709999</td>\n",
              "      <td>31.579334</td>\n",
              "      <td>7117200.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.985942</td>\n",
              "      <td>2.65615</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>31.579334</td>\n",
              "      <td>31.579334</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aebbffcd-78e5-41c4-bd87-9dd4b92d9c17')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aebbffcd-78e5-41c4-bd87-9dd4b92d9c17 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aebbffcd-78e5-41c4-bd87-9dd4b92d9c17');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_START_DATE = '2009-01-01'\n",
        "TRAIN_END_DATE = '2020-07-01'\n",
        "TRADE_START_DATE = '2020-07-01'\n",
        "TRADE_END_DATE = '2021-10-29'"
      ],
      "metadata": {
        "id": "kok_6bA8yrdy"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVpvBNnLIajN",
        "outputId": "e16cb4e1-5d16-439a-ae65-7fd206213c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83897\n",
            "9715\n"
          ]
        }
      ],
      "source": [
        "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
        "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
        "print(len(train))\n",
        "print(len(trade))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(trade)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i2ejloo2Gza",
        "outputId": "2132b059-a540-40b3-bc70-c6ce8eea2a55"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trade.to_csv('trade_data.csv', index=False)"
      ],
      "metadata": {
        "id": "6PBJbcA2qntO"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INDICATORS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwk32SeKJGWZ",
        "outputId": "376abffd-952f-4036-e31f-6fdb2e29d92b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['macd',\n",
              " 'boll_ub',\n",
              " 'boll_lb',\n",
              " 'rsi_30',\n",
              " 'cci_30',\n",
              " 'dx_30',\n",
              " 'close_30_sma',\n",
              " 'close_60_sma']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "d01a6ac6-d7d1-48ce-b21a-1114f6206ea1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ],
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment for training"
      ],
      "metadata": {
        "id": "7We-q73jjaFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "2c653148-6a0d-4fed-b979-a1ac74a01245"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* The DRL algorithms are from **Stable Baselines 3**. Users are also encouraged to try **ElegantRL** and **Ray RLlib**.\n",
        "* FinRL includes fine-tuned standard DRL algorithms, such as DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
        "design their own DRL algorithms by adapting these DRL algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = False\n",
        "if_using_ppo = False\n",
        "if_using_td3 = False\n",
        "if_using_sac = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "### Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "GUCnkn-HIbmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0171d7-49bf-45da-8b35-2141a136f8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "0GVpkWGqH4-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f11a61-f775-4483-c5ae-25c195cc76d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 110          |\n",
            "|    iterations         | 100          |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 500          |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.1        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 99           |\n",
            "|    policy_loss        | -17.1        |\n",
            "|    reward             | -0.025902092 |\n",
            "|    std                | 0.999        |\n",
            "|    value_loss         | 0.688        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 114        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.0354     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 46.6       |\n",
            "|    reward             | -2.2253628 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 7.34       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -341      |\n",
            "|    reward             | 6.416695  |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 73.6      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 17          |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -0.252      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -2.94       |\n",
            "|    reward             | -0.30238307 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 1.51        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 21         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 540        |\n",
            "|    reward             | -11.766727 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 239        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 25         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.214     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 179        |\n",
            "|    reward             | 0.16841185 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 20.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 29         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.177     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 33         |\n",
            "|    reward             | -4.9284453 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.93       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0291     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 120        |\n",
            "|    reward             | -0.4669708 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 11.1       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 37          |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.098      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 121         |\n",
            "|    reward             | -0.25184277 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 11.1        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 42         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 69.3       |\n",
            "|    reward             | -6.8851914 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.2        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 46       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -418     |\n",
            "|    reward             | 4.421223 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 110      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 50         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0107    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -134       |\n",
            "|    reward             | 0.09691551 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 10.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 8.26      |\n",
            "|    reward             | -4.575844 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.88      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 58         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 83.4       |\n",
            "|    reward             | -0.5291632 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.96       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 63        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.0576   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 110       |\n",
            "|    reward             | 7.6928616 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 15.8      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 67          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 33.4        |\n",
            "|    reward             | -0.29514468 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.02        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 71        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.00146  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 5.15      |\n",
            "|    reward             | 1.3494592 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 65.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 75        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.00837  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -41.2     |\n",
            "|    reward             | 1.0282291 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.13      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 79         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.219     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 43.9       |\n",
            "|    reward             | 0.05136675 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.13       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 84          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0.255       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -46.7       |\n",
            "|    reward             | -0.34693396 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.95        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 88       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 84.8     |\n",
            "|    reward             | 3.545023 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 92         |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | -18.2      |\n",
            "|    reward             | -13.469638 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.96       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 119       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 96        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -2.26e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -2.71e+03 |\n",
            "|    reward             | -8.505754 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.52e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 119       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 42.9      |\n",
            "|    reward             | 1.1122239 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.56      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 119       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 104       |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -130      |\n",
            "|    reward             | 2.0885754 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 10.4      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 118          |\n",
            "|    iterations         | 2600         |\n",
            "|    time_elapsed       | 109          |\n",
            "|    total_timesteps    | 13000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.2        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2599         |\n",
            "|    policy_loss        | 24.5         |\n",
            "|    reward             | -0.086281165 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 1.34         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 113         |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | 20.2        |\n",
            "|    reward             | -0.12837109 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.594       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 117      |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 130      |\n",
            "|    reward             | 5.428774 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 122       |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -127      |\n",
            "|    reward             | 2.1065254 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 10.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 126       |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.0461   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 43.5      |\n",
            "|    reward             | 0.8540182 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.49      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 118          |\n",
            "|    iterations         | 3100         |\n",
            "|    time_elapsed       | 130          |\n",
            "|    total_timesteps    | 15500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.4        |\n",
            "|    explained_variance | -0.341       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3099         |\n",
            "|    policy_loss        | 40.9         |\n",
            "|    reward             | -0.013262476 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 1.66         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 119        |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 134        |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | -57.1      |\n",
            "|    reward             | -3.1539395 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 12.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 119        |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 138        |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | 2.55       |\n",
            "|    reward             | 0.49868086 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.641      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 143       |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 24.5      |\n",
            "|    reward             | 7.6937633 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 24.5      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 147         |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0.0114      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | 164         |\n",
            "|    reward             | -0.37935653 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 20.4        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 151       |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -173      |\n",
            "|    reward             | 0.3610375 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 24.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 3700       |\n",
            "|    time_elapsed       | 155        |\n",
            "|    total_timesteps    | 18500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -0.000642  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3699       |\n",
            "|    policy_loss        | 253        |\n",
            "|    reward             | 0.21891928 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 41.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 159       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 24.7      |\n",
            "|    reward             | 1.3517457 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.93      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 164       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -306      |\n",
            "|    reward             | 3.3998291 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 61.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 168       |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -184      |\n",
            "|    reward             | 3.305501  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 34.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 172        |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -0.000376  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | -21.7      |\n",
            "|    reward             | 0.39436668 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.76       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 176        |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -270       |\n",
            "|    reward             | -0.8461852 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 55.8       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 182      |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -98.1    |\n",
            "|    reward             | 3.653108 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 6.08     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 187       |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 60.3      |\n",
            "|    reward             | 2.1809804 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 5.84      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 191      |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 108      |\n",
            "|    reward             | 2.568917 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 8.29     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 195       |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 83.1      |\n",
            "|    reward             | 2.2391548 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 13.5      |\n",
            "-------------------------------------\n",
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6499427.46\n",
            "total_reward: 5499427.46\n",
            "total_cost: 40451.05\n",
            "total_trades: 45944\n",
            "Sharpe: 0.922\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 4700       |\n",
            "|    time_elapsed       | 199        |\n",
            "|    total_timesteps    | 23500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4699       |\n",
            "|    policy_loss        | -84        |\n",
            "|    reward             | 0.24948888 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 8.82       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 203        |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -164       |\n",
            "|    reward             | -1.1180211 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 20.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 207       |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -128      |\n",
            "|    reward             | 1.5313578 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 12        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 5000       |\n",
            "|    time_elapsed       | 211        |\n",
            "|    total_timesteps    | 25000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4999       |\n",
            "|    policy_loss        | -25.3      |\n",
            "|    reward             | -0.6657323 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.35       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 216        |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | 451        |\n",
            "|    reward             | -0.5858356 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 131        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 220       |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -227      |\n",
            "|    reward             | 16.598171 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 80.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 224        |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | -62.6      |\n",
            "|    reward             | 0.66746414 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.19       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 5400        |\n",
            "|    time_elapsed       | 228         |\n",
            "|    total_timesteps    | 27000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5399        |\n",
            "|    policy_loss        | -121        |\n",
            "|    reward             | -0.41932726 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 13          |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 232      |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | -0.00067 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 61       |\n",
            "|    reward             | 3.319186 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 12       |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 236        |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -73.3      |\n",
            "|    reward             | -0.6517232 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 7.96       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 240        |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | -313       |\n",
            "|    reward             | -5.1809063 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 62.5       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 245      |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | -0.213   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -40.3    |\n",
            "|    reward             | 1.32248  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 5900        |\n",
            "|    time_elapsed       | 249         |\n",
            "|    total_timesteps    | 29500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5899        |\n",
            "|    policy_loss        | 23.3        |\n",
            "|    reward             | -0.92037207 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.814       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 6000        |\n",
            "|    time_elapsed       | 253         |\n",
            "|    total_timesteps    | 30000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 1.79e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5999        |\n",
            "|    policy_loss        | -13.9       |\n",
            "|    reward             | -0.52867615 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.3         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 257        |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | -11.3      |\n",
            "|    reward             | -2.4047868 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.78       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 6200        |\n",
            "|    time_elapsed       | 261         |\n",
            "|    total_timesteps    | 31000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6199        |\n",
            "|    policy_loss        | -26.3       |\n",
            "|    reward             | -0.41215822 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.793       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 265       |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 267       |\n",
            "|    reward             | 7.7021155 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 65.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 269       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -0.00651  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 101       |\n",
            "|    reward             | 1.4491193 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 6.88      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 6500       |\n",
            "|    time_elapsed       | 273        |\n",
            "|    total_timesteps    | 32500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6499       |\n",
            "|    policy_loss        | 0.774      |\n",
            "|    reward             | -4.1835113 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 7.38       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 6600        |\n",
            "|    time_elapsed       | 278         |\n",
            "|    total_timesteps    | 33000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6599        |\n",
            "|    policy_loss        | -101        |\n",
            "|    reward             | 0.052990362 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 9.82        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 6700       |\n",
            "|    time_elapsed       | 282        |\n",
            "|    total_timesteps    | 33500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6699       |\n",
            "|    policy_loss        | -769       |\n",
            "|    reward             | -6.8543134 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 433        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 286       |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0.000872  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | -218      |\n",
            "|    reward             | 1.5278558 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 31.2      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 6900        |\n",
            "|    time_elapsed       | 290         |\n",
            "|    total_timesteps    | 34500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.8       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6899        |\n",
            "|    policy_loss        | -6.26       |\n",
            "|    reward             | -0.35871735 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 26.9        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 294        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 59.1       |\n",
            "|    reward             | 0.08136793 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 2.17       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 298       |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 52.6      |\n",
            "|    reward             | 1.0684192 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.48      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 303       |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | -259      |\n",
            "|    reward             | 1.4562882 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 51.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 307       |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | -0.00465  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | -269      |\n",
            "|    reward             | -1.013001 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 44.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 311        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 109        |\n",
            "|    reward             | -6.3664494 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 12.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 7500      |\n",
            "|    time_elapsed       | 315       |\n",
            "|    total_timesteps    | 37500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7499      |\n",
            "|    policy_loss        | 397       |\n",
            "|    reward             | -11.20227 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 88.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 319       |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | -126      |\n",
            "|    reward             | 2.4053032 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 11.5      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 7700        |\n",
            "|    time_elapsed       | 324         |\n",
            "|    total_timesteps    | 38500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7699        |\n",
            "|    policy_loss        | -58.3       |\n",
            "|    reward             | 0.032253746 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 2.14        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 7800        |\n",
            "|    time_elapsed       | 328         |\n",
            "|    total_timesteps    | 39000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7799        |\n",
            "|    policy_loss        | -59.7       |\n",
            "|    reward             | -0.24182229 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 2.82        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 332      |\n",
            "|    total_timesteps    | 39500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.9    |\n",
            "|    explained_variance | 0.00249  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 184      |\n",
            "|    reward             | 4.176115 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 37.4     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 336        |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | -57.5      |\n",
            "|    reward             | -3.8419132 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 2.74       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 340       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 336       |\n",
            "|    reward             | 11.435451 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 94.3      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 8200       |\n",
            "|    time_elapsed       | 344        |\n",
            "|    total_timesteps    | 41000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8199       |\n",
            "|    policy_loss        | -18.4      |\n",
            "|    reward             | 0.21417503 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.418      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 8300       |\n",
            "|    time_elapsed       | 348        |\n",
            "|    total_timesteps    | 41500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -0.199     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8299       |\n",
            "|    policy_loss        | -217       |\n",
            "|    reward             | -2.5451024 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 28.6       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 119          |\n",
            "|    iterations         | 8400         |\n",
            "|    time_elapsed       | 352          |\n",
            "|    total_timesteps    | 42000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.1        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 8399         |\n",
            "|    policy_loss        | -113         |\n",
            "|    reward             | -0.052508354 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 8.86         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 119         |\n",
            "|    iterations         | 8500        |\n",
            "|    time_elapsed       | 356         |\n",
            "|    total_timesteps    | 42500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8499        |\n",
            "|    policy_loss        | -96         |\n",
            "|    reward             | -0.58225566 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 6.48        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 119        |\n",
            "|    iterations         | 8600       |\n",
            "|    time_elapsed       | 361        |\n",
            "|    total_timesteps    | 43000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8599       |\n",
            "|    policy_loss        | 23.3       |\n",
            "|    reward             | -11.446985 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 14.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 119       |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 365       |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0.0285    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | -11.7     |\n",
            "|    reward             | 0.6681949 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.1       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 119        |\n",
            "|    iterations         | 8800       |\n",
            "|    time_elapsed       | 369        |\n",
            "|    total_timesteps    | 44000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8799       |\n",
            "|    policy_loss        | -22.6      |\n",
            "|    reward             | 0.45037633 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.461      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 119       |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 373       |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 5.2       |\n",
            "|    reward             | 1.4299366 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.702     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 119        |\n",
            "|    iterations         | 9000       |\n",
            "|    time_elapsed       | 377        |\n",
            "|    total_timesteps    | 45000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8999       |\n",
            "|    policy_loss        | 33.6       |\n",
            "|    reward             | -0.8488314 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 2.93       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 383       |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 63        |\n",
            "|    reward             | 1.7828076 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 3.6       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 387       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 9.53      |\n",
            "|    reward             | 3.0604067 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 3.73      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 392       |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | -0.00414  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | -135      |\n",
            "|    reward             | 0.4511822 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 11.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 9400       |\n",
            "|    time_elapsed       | 396        |\n",
            "|    total_timesteps    | 47000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0.00138    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9399       |\n",
            "|    policy_loss        | 119        |\n",
            "|    reward             | -1.1292077 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 9.87       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 9500       |\n",
            "|    time_elapsed       | 400        |\n",
            "|    total_timesteps    | 47500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9499       |\n",
            "|    policy_loss        | 68         |\n",
            "|    reward             | 0.09586185 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 4.19       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 118          |\n",
            "|    iterations         | 9600         |\n",
            "|    time_elapsed       | 404          |\n",
            "|    total_timesteps    | 48000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.3        |\n",
            "|    explained_variance | 0.00344      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9599         |\n",
            "|    policy_loss        | -473         |\n",
            "|    reward             | -0.047938753 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 133          |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 9700       |\n",
            "|    time_elapsed       | 408        |\n",
            "|    total_timesteps    | 48500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9699       |\n",
            "|    policy_loss        | 132        |\n",
            "|    reward             | -2.3405635 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 11.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 412       |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 44.5      |\n",
            "|    reward             | 1.9048831 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 18.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 9900       |\n",
            "|    time_elapsed       | 416        |\n",
            "|    total_timesteps    | 49500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9899       |\n",
            "|    policy_loss        | 17.4       |\n",
            "|    reward             | 0.10137671 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.313      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 420        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | 18.8       |\n",
            "|    reward             | -1.0958365 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.37       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 10100     |\n",
            "|    time_elapsed       | 425       |\n",
            "|    total_timesteps    | 50500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10099     |\n",
            "|    policy_loss        | 93.7      |\n",
            "|    reward             | 1.1327468 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 7.15      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 10200      |\n",
            "|    time_elapsed       | 429        |\n",
            "|    total_timesteps    | 51000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10199      |\n",
            "|    policy_loss        | -24.6      |\n",
            "|    reward             | 0.45432726 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.924      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 10300      |\n",
            "|    time_elapsed       | 433        |\n",
            "|    total_timesteps    | 51500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10299      |\n",
            "|    policy_loss        | -75.4      |\n",
            "|    reward             | 0.22216497 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 6.39       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 10400    |\n",
            "|    time_elapsed       | 437      |\n",
            "|    total_timesteps    | 52000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10399    |\n",
            "|    policy_loss        | 423      |\n",
            "|    reward             | 38.65231 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 118      |\n",
            "------------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4181549.56\n",
            "total_reward: 3181549.56\n",
            "total_cost: 10344.47\n",
            "total_trades: 37986\n",
            "Sharpe: 0.803\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 10500     |\n",
            "|    time_elapsed       | 441       |\n",
            "|    total_timesteps    | 52500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10499     |\n",
            "|    policy_loss        | 107       |\n",
            "|    reward             | 1.1010569 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 7.96      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 10600       |\n",
            "|    time_elapsed       | 445         |\n",
            "|    total_timesteps    | 53000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10599       |\n",
            "|    policy_loss        | 27.1        |\n",
            "|    reward             | -0.73630166 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.734       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 10700       |\n",
            "|    time_elapsed       | 449         |\n",
            "|    total_timesteps    | 53500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10699       |\n",
            "|    policy_loss        | 17.8        |\n",
            "|    reward             | -0.22588949 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 1.09        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 10800       |\n",
            "|    time_elapsed       | 453         |\n",
            "|    total_timesteps    | 54000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.6       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10799       |\n",
            "|    policy_loss        | -94.3       |\n",
            "|    reward             | -0.94560033 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 5.13        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 10900      |\n",
            "|    time_elapsed       | 458        |\n",
            "|    total_timesteps    | 54500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10899      |\n",
            "|    policy_loss        | -99        |\n",
            "|    reward             | -0.1108908 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 7.59       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 11000      |\n",
            "|    time_elapsed       | 462        |\n",
            "|    total_timesteps    | 55000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | -0.653     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10999      |\n",
            "|    policy_loss        | 38.5       |\n",
            "|    reward             | -1.1976044 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 3.04       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 11100       |\n",
            "|    time_elapsed       | 466         |\n",
            "|    total_timesteps    | 55500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 11099       |\n",
            "|    policy_loss        | 31.6        |\n",
            "|    reward             | -0.13661395 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.555       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 118          |\n",
            "|    iterations         | 11200        |\n",
            "|    time_elapsed       | 470          |\n",
            "|    total_timesteps    | 56000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.6        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 11199        |\n",
            "|    policy_loss        | -64          |\n",
            "|    reward             | -0.026883092 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 2.58         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 11300     |\n",
            "|    time_elapsed       | 474       |\n",
            "|    total_timesteps    | 56500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11299     |\n",
            "|    policy_loss        | 117       |\n",
            "|    reward             | 1.8654765 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 12.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 11400      |\n",
            "|    time_elapsed       | 479        |\n",
            "|    total_timesteps    | 57000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11399      |\n",
            "|    policy_loss        | 12.8       |\n",
            "|    reward             | -1.8629404 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.991      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 11500      |\n",
            "|    time_elapsed       | 483        |\n",
            "|    total_timesteps    | 57500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11499      |\n",
            "|    policy_loss        | 193        |\n",
            "|    reward             | -1.2851462 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 24.9       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 11600      |\n",
            "|    time_elapsed       | 487        |\n",
            "|    total_timesteps    | 58000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11599      |\n",
            "|    policy_loss        | 138        |\n",
            "|    reward             | 0.41024095 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 13.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 11700     |\n",
            "|    time_elapsed       | 491       |\n",
            "|    total_timesteps    | 58500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11699     |\n",
            "|    policy_loss        | -88       |\n",
            "|    reward             | 2.3023338 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 5.84      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 11800     |\n",
            "|    time_elapsed       | 495       |\n",
            "|    total_timesteps    | 59000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11799     |\n",
            "|    policy_loss        | 131       |\n",
            "|    reward             | 0.5045927 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 10.7      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 11900    |\n",
            "|    time_elapsed       | 500      |\n",
            "|    total_timesteps    | 59500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.8    |\n",
            "|    explained_variance | -0.0813  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11899    |\n",
            "|    policy_loss        | -7.35    |\n",
            "|    reward             | 3.921383 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 8.84     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 12000     |\n",
            "|    time_elapsed       | 504       |\n",
            "|    total_timesteps    | 60000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11999     |\n",
            "|    policy_loss        | 24.8      |\n",
            "|    reward             | 1.5839043 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 0.969     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 12100     |\n",
            "|    time_elapsed       | 508       |\n",
            "|    total_timesteps    | 60500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 0.0217    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12099     |\n",
            "|    policy_loss        | 17.6      |\n",
            "|    reward             | 5.2413754 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 30.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 12200      |\n",
            "|    time_elapsed       | 512        |\n",
            "|    total_timesteps    | 61000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.8      |\n",
            "|    explained_variance | -0.0525    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 12199      |\n",
            "|    policy_loss        | -74.3      |\n",
            "|    reward             | 0.68893266 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 2.84       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 12300      |\n",
            "|    time_elapsed       | 516        |\n",
            "|    total_timesteps    | 61500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 12299      |\n",
            "|    policy_loss        | -88.1      |\n",
            "|    reward             | -2.0740213 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 4.99       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 12400     |\n",
            "|    time_elapsed       | 521       |\n",
            "|    total_timesteps    | 62000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12399     |\n",
            "|    policy_loss        | 39.2      |\n",
            "|    reward             | 2.3194387 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 2.94      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 12500     |\n",
            "|    time_elapsed       | 525       |\n",
            "|    total_timesteps    | 62500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12499     |\n",
            "|    policy_loss        | -179      |\n",
            "|    reward             | 1.1314092 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 19.3      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 12600       |\n",
            "|    time_elapsed       | 529         |\n",
            "|    total_timesteps    | 63000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 12599       |\n",
            "|    policy_loss        | 249         |\n",
            "|    reward             | -0.83938116 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 31.9        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 12700     |\n",
            "|    time_elapsed       | 533       |\n",
            "|    total_timesteps    | 63500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | -0.00352  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12699     |\n",
            "|    policy_loss        | -2.22     |\n",
            "|    reward             | -6.035728 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 0.366     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 12800     |\n",
            "|    time_elapsed       | 538       |\n",
            "|    total_timesteps    | 64000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12799     |\n",
            "|    policy_loss        | -147      |\n",
            "|    reward             | -1.515549 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 19.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 12900     |\n",
            "|    time_elapsed       | 542       |\n",
            "|    total_timesteps    | 64500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12899     |\n",
            "|    policy_loss        | -188      |\n",
            "|    reward             | -0.503229 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 23.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 13000     |\n",
            "|    time_elapsed       | 546       |\n",
            "|    total_timesteps    | 65000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | -0.0324   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12999     |\n",
            "|    policy_loss        | -87.6     |\n",
            "|    reward             | 2.2470348 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 4.84      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 13100     |\n",
            "|    time_elapsed       | 550       |\n",
            "|    total_timesteps    | 65500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13099     |\n",
            "|    policy_loss        | -58.9     |\n",
            "|    reward             | 2.3868167 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 6.21      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 13200     |\n",
            "|    time_elapsed       | 554       |\n",
            "|    total_timesteps    | 66000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13199     |\n",
            "|    policy_loss        | -263      |\n",
            "|    reward             | 2.723112  |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 46.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 13300      |\n",
            "|    time_elapsed       | 559        |\n",
            "|    total_timesteps    | 66500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 2.38e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13299      |\n",
            "|    policy_loss        | 1.58e+03   |\n",
            "|    reward             | -9.0533495 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.57e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 13400      |\n",
            "|    time_elapsed       | 563        |\n",
            "|    total_timesteps    | 67000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13399      |\n",
            "|    policy_loss        | 30.6       |\n",
            "|    reward             | -0.5513047 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.24       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 13500      |\n",
            "|    time_elapsed       | 567        |\n",
            "|    total_timesteps    | 67500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43        |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13499      |\n",
            "|    policy_loss        | 134        |\n",
            "|    reward             | -0.6288722 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 10.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 13600      |\n",
            "|    time_elapsed       | 571        |\n",
            "|    total_timesteps    | 68000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43        |\n",
            "|    explained_variance | 0.0325     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13599      |\n",
            "|    policy_loss        | -457       |\n",
            "|    reward             | -2.6989737 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 129        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 13700      |\n",
            "|    time_elapsed       | 575        |\n",
            "|    total_timesteps    | 68500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13699      |\n",
            "|    policy_loss        | -49        |\n",
            "|    reward             | -2.2324624 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 2.63       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 13800      |\n",
            "|    time_elapsed       | 579        |\n",
            "|    total_timesteps    | 69000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 2.38e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13799      |\n",
            "|    policy_loss        | -303       |\n",
            "|    reward             | -11.217007 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 51.3       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 13900      |\n",
            "|    time_elapsed       | 585        |\n",
            "|    total_timesteps    | 69500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | -0.00958   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13899      |\n",
            "|    policy_loss        | 80         |\n",
            "|    reward             | 0.25760776 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 5.17       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 14000      |\n",
            "|    time_elapsed       | 589        |\n",
            "|    total_timesteps    | 70000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0.161      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 13999      |\n",
            "|    policy_loss        | 18.6       |\n",
            "|    reward             | 0.40102038 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.479      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 14100      |\n",
            "|    time_elapsed       | 594        |\n",
            "|    total_timesteps    | 70500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 14099      |\n",
            "|    policy_loss        | 72.4       |\n",
            "|    reward             | -1.5327096 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 4.04       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 14200       |\n",
            "|    time_elapsed       | 598         |\n",
            "|    total_timesteps    | 71000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 14199       |\n",
            "|    policy_loss        | 121         |\n",
            "|    reward             | -0.33936748 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 9.72        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 14300     |\n",
            "|    time_elapsed       | 602       |\n",
            "|    total_timesteps    | 71500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 14299     |\n",
            "|    policy_loss        | 10.4      |\n",
            "|    reward             | 1.1344056 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.076     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 14400      |\n",
            "|    time_elapsed       | 606        |\n",
            "|    total_timesteps    | 72000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 14399      |\n",
            "|    policy_loss        | 105        |\n",
            "|    reward             | -0.6752286 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 9.55       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 118          |\n",
            "|    iterations         | 14500        |\n",
            "|    time_elapsed       | 610          |\n",
            "|    total_timesteps    | 72500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -43.2        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 14499        |\n",
            "|    policy_loss        | -2.93        |\n",
            "|    reward             | -0.105026014 |\n",
            "|    std                | 1.08         |\n",
            "|    value_loss         | 1.34         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 14600     |\n",
            "|    time_elapsed       | 615       |\n",
            "|    total_timesteps    | 73000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 14599     |\n",
            "|    policy_loss        | 15.4      |\n",
            "|    reward             | 2.9473927 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.675     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 14700     |\n",
            "|    time_elapsed       | 619       |\n",
            "|    total_timesteps    | 73500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.3     |\n",
            "|    explained_variance | -0.64     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 14699     |\n",
            "|    policy_loss        | -89.8     |\n",
            "|    reward             | 1.0726844 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 5.1       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 14800      |\n",
            "|    time_elapsed       | 623        |\n",
            "|    total_timesteps    | 74000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 14799      |\n",
            "|    policy_loss        | -497       |\n",
            "|    reward             | -1.0968516 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 115        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 14900      |\n",
            "|    time_elapsed       | 627        |\n",
            "|    total_timesteps    | 74500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 14899      |\n",
            "|    policy_loss        | 104        |\n",
            "|    reward             | -0.8123098 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 5.71       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 15000      |\n",
            "|    time_elapsed       | 631        |\n",
            "|    total_timesteps    | 75000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 14999      |\n",
            "|    policy_loss        | 172        |\n",
            "|    reward             | -3.9862082 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 26.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 15100     |\n",
            "|    time_elapsed       | 635       |\n",
            "|    total_timesteps    | 75500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15099     |\n",
            "|    policy_loss        | -50.6     |\n",
            "|    reward             | 0.4777023 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 4.29      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 15200     |\n",
            "|    time_elapsed       | 640       |\n",
            "|    total_timesteps    | 76000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15199     |\n",
            "|    policy_loss        | 11.8      |\n",
            "|    reward             | 0.6397194 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 0.487     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 15300     |\n",
            "|    time_elapsed       | 644       |\n",
            "|    total_timesteps    | 76500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15299     |\n",
            "|    policy_loss        | -84.7     |\n",
            "|    reward             | 3.0211194 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 10.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 15400      |\n",
            "|    time_elapsed       | 648        |\n",
            "|    total_timesteps    | 77000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 15399      |\n",
            "|    policy_loss        | 120        |\n",
            "|    reward             | -4.3358035 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 13.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 15500     |\n",
            "|    time_elapsed       | 652       |\n",
            "|    total_timesteps    | 77500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15499     |\n",
            "|    policy_loss        | 154       |\n",
            "|    reward             | 3.8071017 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 16.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 15600     |\n",
            "|    time_elapsed       | 656       |\n",
            "|    total_timesteps    | 78000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15599     |\n",
            "|    policy_loss        | 85.8      |\n",
            "|    reward             | 1.7906585 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 6.73      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 118        |\n",
            "|    iterations         | 15700      |\n",
            "|    time_elapsed       | 660        |\n",
            "|    total_timesteps    | 78500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 15699      |\n",
            "|    policy_loss        | 78.7       |\n",
            "|    reward             | -1.6787672 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 5.63       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 15800     |\n",
            "|    time_elapsed       | 665       |\n",
            "|    total_timesteps    | 79000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15799     |\n",
            "|    policy_loss        | -5.96     |\n",
            "|    reward             | 1.7636905 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 1.82      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 118         |\n",
            "|    iterations         | 15900       |\n",
            "|    time_elapsed       | 669         |\n",
            "|    total_timesteps    | 79500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 15899       |\n",
            "|    policy_loss        | 36.6        |\n",
            "|    reward             | -0.74141675 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 0.855       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 16000     |\n",
            "|    time_elapsed       | 673       |\n",
            "|    total_timesteps    | 80000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15999     |\n",
            "|    policy_loss        | -186      |\n",
            "|    reward             | 2.3127518 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 38.7      |\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c, \n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=80000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_a2c.save(\"agent_a2c\")"
      ],
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "M2YadjfnLwgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a90cba8-5f4b-4c97-bb4d-caae8e552efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "tCDa78rqfO_a"
      },
      "outputs": [],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "y5D5PFUhMzSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da76b470-963b-43bd-fe73-681cf17d40ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 512, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "# PPO_PARAMS = {\n",
        "#     \"n_steps\": 2048,\n",
        "#     \"ent_coef\": 0.01,\n",
        "#     \"learning_rate\": 0.00045,\n",
        "#     \"batch_size\": 128,\n",
        "# }\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 512,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "JSAHhV4Xc-bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a19fd2-4e4a-4c07-cc6a-767a29d9d87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3, \n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "xwOhVjqRkCdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855c910b-c9df-451f-d389-cf3f8f0e3b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [],
      "source": [
        "trained_sac = agent.train_model(model=model_sac, \n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None"
      ]
    }
  ]
}
