{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mPT0ipYE28wL",
    "outputId": "4352663d-20eb-4080-a83e-bf6b97183bf4"
   },
   "outputs": [],
   "source": [
    "# ## install finrl library\n",
    "# !pip install wrds\n",
    "# !pip install swig\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/james/Dropbox/Investing/Personal_Algo_Trading_Practice/FinRL_JDB/examples\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# print (os.getcwd())\n",
    "print (os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../FinRL_JDB/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "cd002c5d-2490-4947-9bd3-2b0696cb0f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "743f675b-6126-44ea-bf39-7b3333d15044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (96942, 8)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = '2009-04-01'\n",
    "TRAIN_END_DATE = '2021-01-01'\n",
    "TEST_START_DATE = '2021-01-01'\n",
    "TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df to a csv file\n",
    "df.to_csv(DATA_SAVE_DIR + 'dow_30_joined_closes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GiRuFOTOtj1Y",
    "outputId": "632ce1e6-ad27-4f50-fec7-0ca27c8e3c96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         date       open       high        low      close     volume   tic  \\\n0  2009-04-01   3.717500   3.892857   3.710357   3.308903  589372000  AAPL   \n1  2009-04-01  48.779999  48.930000  47.099998  36.228405   10850100  AMGN   \n2  2009-04-01  13.340000  14.640000  13.080000  11.732112   27701800   AXP   \n3  2009-04-01  34.520000  35.599998  34.209999  26.850746    9288800    BA   \n4  2009-04-01  27.500000  29.520000  27.440001  19.726320   15308300   CAT   \n\n   day  \n0    2  \n1    2  \n2    2  \n3    2  \n4    2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-04-01</td>\n      <td>3.717500</td>\n      <td>3.892857</td>\n      <td>3.710357</td>\n      <td>3.308903</td>\n      <td>589372000</td>\n      <td>AAPL</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-04-01</td>\n      <td>48.779999</td>\n      <td>48.930000</td>\n      <td>47.099998</td>\n      <td>36.228405</td>\n      <td>10850100</td>\n      <td>AMGN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-04-01</td>\n      <td>13.340000</td>\n      <td>14.640000</td>\n      <td>13.080000</td>\n      <td>11.732112</td>\n      <td>27701800</td>\n      <td>AXP</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-04-01</td>\n      <td>34.520000</td>\n      <td>35.599998</td>\n      <td>34.209999</td>\n      <td>26.850746</td>\n      <td>9288800</td>\n      <td>BA</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-04-01</td>\n      <td>27.500000</td>\n      <td>29.520000</td>\n      <td>27.440001</td>\n      <td>19.726320</td>\n      <td>15308300</td>\n      <td>CAT</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DSw4ZEzVtj1Z",
    "outputId": "0015b377-84ec-4a6d-ac4e-e138c2c9bac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             date        open        high         low       close    volume  \\\n96937  2022-05-31  503.619995  504.109985  495.660004  491.949799   4003100   \n96938  2022-05-31  210.380005  214.350006  209.110001  211.322525   9586400   \n96939  2022-05-31   51.259998   51.560001   50.849998   49.042248  25016600   \n96940  2022-05-31   43.480000   44.270000   43.049999   42.811333   8192000   \n96941  2022-05-31  127.459999  129.899994  127.419998  127.591217  12304100   \n\n       tic  day  \n96937  UNH    1  \n96938    V    1  \n96939   VZ    1  \n96940  WBA    1  \n96941  WMT    1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96937</th>\n      <td>2022-05-31</td>\n      <td>503.619995</td>\n      <td>504.109985</td>\n      <td>495.660004</td>\n      <td>491.949799</td>\n      <td>4003100</td>\n      <td>UNH</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96938</th>\n      <td>2022-05-31</td>\n      <td>210.380005</td>\n      <td>214.350006</td>\n      <td>209.110001</td>\n      <td>211.322525</td>\n      <td>9586400</td>\n      <td>V</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96939</th>\n      <td>2022-05-31</td>\n      <td>51.259998</td>\n      <td>51.560001</td>\n      <td>50.849998</td>\n      <td>49.042248</td>\n      <td>25016600</td>\n      <td>VZ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96940</th>\n      <td>2022-05-31</td>\n      <td>43.480000</td>\n      <td>44.270000</td>\n      <td>43.049999</td>\n      <td>42.811333</td>\n      <td>8192000</td>\n      <td>WBA</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96941</th>\n      <td>2022-05-31</td>\n      <td>127.459999</td>\n      <td>129.899994</td>\n      <td>127.419998</td>\n      <td>127.591217</td>\n      <td>12304100</td>\n      <td>WMT</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV3HrZHLh1hy",
    "outputId": "42781af6-4cee-4277-8a00-cf46052f991c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(96942, 8)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4hYkeaPiICHS",
    "outputId": "59c51c93-f786-469b-c008-4e4416a041b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         date       open       high        low      close     volume   tic  \\\n0  2009-04-01   3.717500   3.892857   3.710357   3.308903  589372000  AAPL   \n1  2009-04-01  48.779999  48.930000  47.099998  36.228405   10850100  AMGN   \n2  2009-04-01  13.340000  14.640000  13.080000  11.732112   27701800   AXP   \n3  2009-04-01  34.520000  35.599998  34.209999  26.850746    9288800    BA   \n4  2009-04-01  27.500000  29.520000  27.440001  19.726320   15308300   CAT   \n\n   day  \n0    2  \n1    2  \n2    2  \n3    2  \n4    2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-04-01</td>\n      <td>3.717500</td>\n      <td>3.892857</td>\n      <td>3.710357</td>\n      <td>3.308903</td>\n      <td>589372000</td>\n      <td>AAPL</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-04-01</td>\n      <td>48.779999</td>\n      <td>48.930000</td>\n      <td>47.099998</td>\n      <td>36.228405</td>\n      <td>10850100</td>\n      <td>AMGN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-04-01</td>\n      <td>13.340000</td>\n      <td>14.640000</td>\n      <td>13.080000</td>\n      <td>11.732112</td>\n      <td>27701800</td>\n      <td>AXP</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-04-01</td>\n      <td>34.520000</td>\n      <td>35.599998</td>\n      <td>34.209999</td>\n      <td>26.850746</td>\n      <td>9288800</td>\n      <td>BA</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-04-01</td>\n      <td>27.500000</td>\n      <td>29.520000</td>\n      <td>27.440001</td>\n      <td>19.726320</td>\n      <td>15308300</td>\n      <td>CAT</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['date','tic']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2vryMsdNL9H",
    "outputId": "dff3babf-4aba-44dd-ad61-8845df60243e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "30"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcNyXa7RNPrF",
    "outputId": "fd13ad85-36fd-4a55-9084-dbf807bbeb02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "AAPL    3315\nAMGN    3315\nWMT     3315\nWBA     3315\nVZ      3315\nV       3315\nUNH     3315\nTRV     3315\nPG      3315\nNKE     3315\nMSFT    3315\nMRK     3315\nMMM     3315\nMCD     3315\nKO      3315\nJPM     3315\nJNJ     3315\nINTC    3315\nIBM     3315\nHON     3315\nHD      3315\nGS      3315\nDIS     3315\nCVX     3315\nCSCO    3315\nCRM     3315\nCAT     3315\nBA      3315\nAXP     3315\nDOW      807\nName: tic, dtype: int64"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "kM5bH9uroCeg"
   },
   "outputs": [],
   "source": [
    "#  INDICATORS = ['macd',\n",
    "#                'rsi_30',\n",
    "#                'cci_30',\n",
    "#                'dx_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "aa687295-c857-4366-d9af-96ea233c6463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "grvhGJJII3Xn",
    "outputId": "6dd919fa-032b-4180-adf4-1f732777cedc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             date        open        high         low       close    volume  \\\n86262  2021-01-25   48.419998   48.840000   48.240002   45.930290  16604200   \n4470   2009-11-09   58.380001   60.090000   58.240002   42.176170  12250500   \n24821  2012-08-21   43.709999   43.750000   42.830002   26.510324  20204000   \n30866  2013-06-24   72.900002   74.019997   72.410004   59.387035  10214400   \n90840  2021-09-09  131.787766  132.848953  131.510513  123.808319   3669682   \n\n       tic  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n86262   KO    0 -1.039004   52.214384   43.762783  42.486375 -108.128137   \n4470   CAT    0  1.095288   42.484327   36.963457  63.025428  116.375922   \n24821   VZ    1 -0.086684   28.084757   26.759295  46.634947 -198.567860   \n30866   HD    0 -0.360794   65.008155   59.209776  47.292348 -204.266935   \n90840  IBM    3 -0.563575  128.930399  122.662760  45.972120 -116.499763   \n\n           dx_30  close_30_sma  close_60_sma  turbulence  \n86262  39.096027     48.696966     48.628491   19.836791  \n4470   35.044754     38.470791     36.164122    0.000000  \n24821  25.930920     27.527103     26.985860   26.328787  \n30866  22.753387     62.268065     60.440481   61.604404  \n90840  16.213638    126.152558    126.472239   14.160630  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n      <th>macd</th>\n      <th>boll_ub</th>\n      <th>boll_lb</th>\n      <th>rsi_30</th>\n      <th>cci_30</th>\n      <th>dx_30</th>\n      <th>close_30_sma</th>\n      <th>close_60_sma</th>\n      <th>turbulence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>86262</th>\n      <td>2021-01-25</td>\n      <td>48.419998</td>\n      <td>48.840000</td>\n      <td>48.240002</td>\n      <td>45.930290</td>\n      <td>16604200</td>\n      <td>KO</td>\n      <td>0</td>\n      <td>-1.039004</td>\n      <td>52.214384</td>\n      <td>43.762783</td>\n      <td>42.486375</td>\n      <td>-108.128137</td>\n      <td>39.096027</td>\n      <td>48.696966</td>\n      <td>48.628491</td>\n      <td>19.836791</td>\n    </tr>\n    <tr>\n      <th>4470</th>\n      <td>2009-11-09</td>\n      <td>58.380001</td>\n      <td>60.090000</td>\n      <td>58.240002</td>\n      <td>42.176170</td>\n      <td>12250500</td>\n      <td>CAT</td>\n      <td>0</td>\n      <td>1.095288</td>\n      <td>42.484327</td>\n      <td>36.963457</td>\n      <td>63.025428</td>\n      <td>116.375922</td>\n      <td>35.044754</td>\n      <td>38.470791</td>\n      <td>36.164122</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>24821</th>\n      <td>2012-08-21</td>\n      <td>43.709999</td>\n      <td>43.750000</td>\n      <td>42.830002</td>\n      <td>26.510324</td>\n      <td>20204000</td>\n      <td>VZ</td>\n      <td>1</td>\n      <td>-0.086684</td>\n      <td>28.084757</td>\n      <td>26.759295</td>\n      <td>46.634947</td>\n      <td>-198.567860</td>\n      <td>25.930920</td>\n      <td>27.527103</td>\n      <td>26.985860</td>\n      <td>26.328787</td>\n    </tr>\n    <tr>\n      <th>30866</th>\n      <td>2013-06-24</td>\n      <td>72.900002</td>\n      <td>74.019997</td>\n      <td>72.410004</td>\n      <td>59.387035</td>\n      <td>10214400</td>\n      <td>HD</td>\n      <td>0</td>\n      <td>-0.360794</td>\n      <td>65.008155</td>\n      <td>59.209776</td>\n      <td>47.292348</td>\n      <td>-204.266935</td>\n      <td>22.753387</td>\n      <td>62.268065</td>\n      <td>60.440481</td>\n      <td>61.604404</td>\n    </tr>\n    <tr>\n      <th>90840</th>\n      <td>2021-09-09</td>\n      <td>131.787766</td>\n      <td>132.848953</td>\n      <td>131.510513</td>\n      <td>123.808319</td>\n      <td>3669682</td>\n      <td>IBM</td>\n      <td>3</td>\n      <td>-0.563575</td>\n      <td>128.930399</td>\n      <td>122.662760</td>\n      <td>45.972120</td>\n      <td>-116.499763</td>\n      <td>16.213638</td>\n      <td>126.152558</td>\n      <td>126.472239</td>\n      <td>14.160630</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "0194749d-62ec-420f-9b54-492873c266a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 50_000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 256\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 50_000, \n",
    "                 'ppo' : 50_000, \n",
    "                 'ddpg' : 50_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "b2a1cfbc-ced9-4d06-dd9a-4300845e1113",
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  203.40201777637154\n",
      "======Model training from:  2009-04-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_126_9\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 252        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -28.9      |\n",
      "|    reward             | 0.08385004 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.542      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 9.75      |\n",
      "|    reward             | 0.2381071 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.0639    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 268        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.168      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -52.9      |\n",
      "|    reward             | 0.21298411 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.51       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 271        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.0172     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 2.46       |\n",
      "|    reward             | 0.08503554 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.0459     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 272         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0.114       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -13.4       |\n",
      "|    reward             | -0.15352365 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.287       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 273         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | -11.1       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.642      |\n",
      "|    reward             | 0.012289424 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.0367      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 274          |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 12           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.8        |\n",
      "|    explained_variance | -5.09        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | -0.609       |\n",
      "|    reward             | -0.007419841 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.00544      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 271        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0.553      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 0.677      |\n",
      "|    reward             | 0.08371936 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.000449   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 271        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0.138      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -3.64      |\n",
      "|    reward             | 0.16921702 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.0159     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 271        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | -1.18      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 7.43       |\n",
      "|    reward             | 0.04619743 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.0369     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 268        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -4.61      |\n",
      "|    reward             | -0.1742362 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.0196     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 269          |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 22           |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.5        |\n",
      "|    explained_variance | -103         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -6.09        |\n",
      "|    reward             | 0.0025655488 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.0272       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | -1.37       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 1.44        |\n",
      "|    reward             | 0.045208417 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.00173     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 263          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 26           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -43          |\n",
      "|    explained_variance | -8.17        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | -1.19        |\n",
      "|    reward             | 0.0037249243 |\n",
      "|    std                | 1.07         |\n",
      "|    value_loss         | 0.00155      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | -0.142     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -0.268     |\n",
      "|    reward             | 0.08468936 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.00128    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 265         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.6       |\n",
      "|    explained_variance | -0.191      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -0.765      |\n",
      "|    reward             | -0.05319096 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.000819    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 266        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -0.885     |\n",
      "|    reward             | 0.01120981 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.00146    |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 265           |\n",
      "|    iterations         | 1800          |\n",
      "|    time_elapsed       | 33            |\n",
      "|    total_timesteps    | 9000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -44.1         |\n",
      "|    explained_variance | -254          |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1799          |\n",
      "|    policy_loss        | -6.43         |\n",
      "|    reward             | -0.0023366078 |\n",
      "|    std                | 1.11          |\n",
      "|    value_loss         | 0.051         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 265          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 35           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -44.5        |\n",
      "|    explained_variance | -4.08        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | -1.82        |\n",
      "|    reward             | 0.0149593735 |\n",
      "|    std                | 1.12         |\n",
      "|    value_loss         | 0.00273      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 264           |\n",
      "|    iterations         | 2000          |\n",
      "|    time_elapsed       | 37            |\n",
      "|    total_timesteps    | 10000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -45           |\n",
      "|    explained_variance | -72.8         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1999          |\n",
      "|    policy_loss        | 2.15          |\n",
      "|    reward             | 0.00013383865 |\n",
      "|    std                | 1.14          |\n",
      "|    value_loss         | 0.00543       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 2100        |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 10500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.7       |\n",
      "|    explained_variance | -41.6       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2099        |\n",
      "|    policy_loss        | 4.03        |\n",
      "|    reward             | 0.020164471 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 0.022       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 0.891       |\n",
      "|    reward             | 0.002911278 |\n",
      "|    std                | 1.2         |\n",
      "|    value_loss         | 0.000405    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 2300        |\n",
      "|    time_elapsed       | 43          |\n",
      "|    total_timesteps    | 11500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2299        |\n",
      "|    policy_loss        | 0.637       |\n",
      "|    reward             | 0.010758238 |\n",
      "|    std                | 1.24        |\n",
      "|    value_loss         | 0.00029     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 262           |\n",
      "|    iterations         | 2400          |\n",
      "|    time_elapsed       | 45            |\n",
      "|    total_timesteps    | 12000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -48           |\n",
      "|    explained_variance | -6.38         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2399          |\n",
      "|    policy_loss        | 2.15          |\n",
      "|    reward             | -0.0011224081 |\n",
      "|    std                | 1.27          |\n",
      "|    value_loss         | 0.00243       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 47          |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.5       |\n",
      "|    explained_variance | -36.3       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | -0.261      |\n",
      "|    reward             | 0.003355584 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 0.0012      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 263          |\n",
      "|    iterations         | 2600         |\n",
      "|    time_elapsed       | 49           |\n",
      "|    total_timesteps    | 13000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -49.1        |\n",
      "|    explained_variance | -11.3        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2599         |\n",
      "|    policy_loss        | -0.0503      |\n",
      "|    reward             | -0.009705312 |\n",
      "|    std                | 1.32         |\n",
      "|    value_loss         | 0.000105     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 260           |\n",
      "|    iterations         | 2700          |\n",
      "|    time_elapsed       | 51            |\n",
      "|    total_timesteps    | 13500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -49.9         |\n",
      "|    explained_variance | -6.47         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2699          |\n",
      "|    policy_loss        | -1.53         |\n",
      "|    reward             | -0.0003523636 |\n",
      "|    std                | 1.35          |\n",
      "|    value_loss         | 0.00102       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 258          |\n",
      "|    iterations         | 2800         |\n",
      "|    time_elapsed       | 54           |\n",
      "|    total_timesteps    | 14000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -50.8        |\n",
      "|    explained_variance | -0.00364     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2799         |\n",
      "|    policy_loss        | 0.482        |\n",
      "|    reward             | -0.001183385 |\n",
      "|    std                | 1.4          |\n",
      "|    value_loss         | 9.53e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 258           |\n",
      "|    iterations         | 2900          |\n",
      "|    time_elapsed       | 56            |\n",
      "|    total_timesteps    | 14500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -51.9         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2899          |\n",
      "|    policy_loss        | 0.256         |\n",
      "|    reward             | -0.0026809315 |\n",
      "|    std                | 1.45          |\n",
      "|    value_loss         | 5.96e-05      |\n",
      "-----------------------------------------\n",
      "day: 2959, episode: 5\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 6643.40\n",
      "total_reward: -43356.60\n",
      "total_cost: 4166.53\n",
      "total_trades: 45815\n",
      "Sharpe: 0.070\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 258           |\n",
      "|    iterations         | 3000          |\n",
      "|    time_elapsed       | 57            |\n",
      "|    total_timesteps    | 15000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -52.8         |\n",
      "|    explained_variance | -0.0996       |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2999          |\n",
      "|    policy_loss        | 0.201         |\n",
      "|    reward             | -0.0064705107 |\n",
      "|    std                | 1.5           |\n",
      "|    value_loss         | 4.24e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 258          |\n",
      "|    iterations         | 3100         |\n",
      "|    time_elapsed       | 59           |\n",
      "|    total_timesteps    | 15500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -53.7        |\n",
      "|    explained_variance | -1.79        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3099         |\n",
      "|    policy_loss        | -0.44        |\n",
      "|    reward             | 0.0009736575 |\n",
      "|    std                | 1.54         |\n",
      "|    value_loss         | 0.000118     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 257         |\n",
      "|    iterations         | 3200        |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 16000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -54.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3199        |\n",
      "|    policy_loss        | 0.857       |\n",
      "|    reward             | 0.003720156 |\n",
      "|    std                | 1.59        |\n",
      "|    value_loss         | 0.000288    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 255          |\n",
      "|    iterations         | 3300         |\n",
      "|    time_elapsed       | 64           |\n",
      "|    total_timesteps    | 16500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -55.4        |\n",
      "|    explained_variance | 0.132        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3299         |\n",
      "|    policy_loss        | -0.756       |\n",
      "|    reward             | -0.021391045 |\n",
      "|    std                | 1.63         |\n",
      "|    value_loss         | 0.000488     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 254          |\n",
      "|    iterations         | 3400         |\n",
      "|    time_elapsed       | 66           |\n",
      "|    total_timesteps    | 17000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -56          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3399         |\n",
      "|    policy_loss        | 0.475        |\n",
      "|    reward             | 0.0002396645 |\n",
      "|    std                | 1.67         |\n",
      "|    value_loss         | 0.000268     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 252         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 69          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -56.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 0.444       |\n",
      "|    reward             | 0.011361206 |\n",
      "|    std                | 1.7         |\n",
      "|    value_loss         | 0.000115    |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 250            |\n",
      "|    iterations         | 3600           |\n",
      "|    time_elapsed       | 71             |\n",
      "|    total_timesteps    | 18000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -57            |\n",
      "|    explained_variance | -0.955         |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 3599           |\n",
      "|    policy_loss        | 0.624          |\n",
      "|    reward             | -0.00075525534 |\n",
      "|    std                | 1.73           |\n",
      "|    value_loss         | 0.000171       |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 249         |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 74          |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -57.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 0.542       |\n",
      "|    reward             | 0.007601951 |\n",
      "|    std                | 1.77        |\n",
      "|    value_loss         | 9.97e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 247         |\n",
      "|    iterations         | 3800        |\n",
      "|    time_elapsed       | 76          |\n",
      "|    total_timesteps    | 19000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -58.4       |\n",
      "|    explained_variance | -2          |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3799        |\n",
      "|    policy_loss        | 0.721       |\n",
      "|    reward             | 0.009686122 |\n",
      "|    std                | 1.81        |\n",
      "|    value_loss         | 0.000196    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 246         |\n",
      "|    iterations         | 3900        |\n",
      "|    time_elapsed       | 79          |\n",
      "|    total_timesteps    | 19500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -59         |\n",
      "|    explained_variance | 0.169       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3899        |\n",
      "|    policy_loss        | 0.882       |\n",
      "|    reward             | 0.038978655 |\n",
      "|    std                | 1.85        |\n",
      "|    value_loss         | 0.00041     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 245         |\n",
      "|    iterations         | 4000        |\n",
      "|    time_elapsed       | 81          |\n",
      "|    total_timesteps    | 20000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -59.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3999        |\n",
      "|    policy_loss        | 5.38        |\n",
      "|    reward             | 0.039971948 |\n",
      "|    std                | 1.89        |\n",
      "|    value_loss         | 0.0106      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 244           |\n",
      "|    iterations         | 4100          |\n",
      "|    time_elapsed       | 83            |\n",
      "|    total_timesteps    | 20500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -60.1         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4099          |\n",
      "|    policy_loss        | -0.529        |\n",
      "|    reward             | -0.0074078753 |\n",
      "|    std                | 1.92          |\n",
      "|    value_loss         | 0.000423      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 243          |\n",
      "|    iterations         | 4200         |\n",
      "|    time_elapsed       | 86           |\n",
      "|    total_timesteps    | 21000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -60.5        |\n",
      "|    explained_variance | 0.136        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4199         |\n",
      "|    policy_loss        | 0.0645       |\n",
      "|    reward             | 0.0024565884 |\n",
      "|    std                | 1.95         |\n",
      "|    value_loss         | 1.76e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 242          |\n",
      "|    iterations         | 4300         |\n",
      "|    time_elapsed       | 88           |\n",
      "|    total_timesteps    | 21500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -61.2        |\n",
      "|    explained_variance | 0.0269       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4299         |\n",
      "|    policy_loss        | 0.568        |\n",
      "|    reward             | -0.012682746 |\n",
      "|    std                | 2            |\n",
      "|    value_loss         | 0.000111     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 241          |\n",
      "|    iterations         | 4400         |\n",
      "|    time_elapsed       | 91           |\n",
      "|    total_timesteps    | 22000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -62.2        |\n",
      "|    explained_variance | 2.38e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4399         |\n",
      "|    policy_loss        | -0.126       |\n",
      "|    reward             | -0.011759231 |\n",
      "|    std                | 2.07         |\n",
      "|    value_loss         | 2.12e-05     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 240            |\n",
      "|    iterations         | 4500           |\n",
      "|    time_elapsed       | 93             |\n",
      "|    total_timesteps    | 22500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -63.3          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 4499           |\n",
      "|    policy_loss        | 0.429          |\n",
      "|    reward             | -0.00092747214 |\n",
      "|    std                | 2.15           |\n",
      "|    value_loss         | 6.41e-05       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 239            |\n",
      "|    iterations         | 4600           |\n",
      "|    time_elapsed       | 95             |\n",
      "|    total_timesteps    | 23000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -64.2          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 4599           |\n",
      "|    policy_loss        | 1.03           |\n",
      "|    reward             | -0.00066697854 |\n",
      "|    std                | 2.22           |\n",
      "|    value_loss         | 0.000299       |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 237         |\n",
      "|    iterations         | 4700        |\n",
      "|    time_elapsed       | 98          |\n",
      "|    total_timesteps    | 23500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -65         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4699        |\n",
      "|    policy_loss        | 2.03        |\n",
      "|    reward             | 0.020789078 |\n",
      "|    std                | 2.28        |\n",
      "|    value_loss         | 0.00195     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 236           |\n",
      "|    iterations         | 4800          |\n",
      "|    time_elapsed       | 101           |\n",
      "|    total_timesteps    | 24000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -65.7         |\n",
      "|    explained_variance | 0.182         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4799          |\n",
      "|    policy_loss        | -0.429        |\n",
      "|    reward             | -0.0038845376 |\n",
      "|    std                | 2.33          |\n",
      "|    value_loss         | 9.64e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 234         |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 104         |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -66.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | -0.184      |\n",
      "|    reward             | 0.007308849 |\n",
      "|    std                | 2.4         |\n",
      "|    value_loss         | 2.46e-05    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 233           |\n",
      "|    iterations         | 5000          |\n",
      "|    time_elapsed       | 107           |\n",
      "|    total_timesteps    | 25000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -67.3         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4999          |\n",
      "|    policy_loss        | 1.54          |\n",
      "|    reward             | -0.0063377325 |\n",
      "|    std                | 2.47          |\n",
      "|    value_loss         | 0.000854      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 231         |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 110         |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -68.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | -0.781      |\n",
      "|    reward             | 0.016074818 |\n",
      "|    std                | 2.53        |\n",
      "|    value_loss         | 0.00019     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 230         |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -68.7       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | -0.22       |\n",
      "|    reward             | 0.018792644 |\n",
      "|    std                | 2.59        |\n",
      "|    value_loss         | 0.000236    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 229          |\n",
      "|    iterations         | 5300         |\n",
      "|    time_elapsed       | 115          |\n",
      "|    total_timesteps    | 26500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -69.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5299         |\n",
      "|    policy_loss        | -1.53        |\n",
      "|    reward             | -0.015598492 |\n",
      "|    std                | 2.63         |\n",
      "|    value_loss         | 0.000644     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 227          |\n",
      "|    iterations         | 5400         |\n",
      "|    time_elapsed       | 118          |\n",
      "|    total_timesteps    | 27000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -69.6        |\n",
      "|    explained_variance | -0.404       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5399         |\n",
      "|    policy_loss        | 0.129        |\n",
      "|    reward             | 0.0017427322 |\n",
      "|    std                | 2.67         |\n",
      "|    value_loss         | 1.19e-05     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 226            |\n",
      "|    iterations         | 5500           |\n",
      "|    time_elapsed       | 121            |\n",
      "|    total_timesteps    | 27500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -70.4          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5499           |\n",
      "|    policy_loss        | -0.403         |\n",
      "|    reward             | -0.00023556214 |\n",
      "|    std                | 2.75           |\n",
      "|    value_loss         | 4.74e-05       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 226            |\n",
      "|    iterations         | 5600           |\n",
      "|    time_elapsed       | 123            |\n",
      "|    total_timesteps    | 28000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -71.5          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5599           |\n",
      "|    policy_loss        | 0.478          |\n",
      "|    reward             | -3.7838145e-05 |\n",
      "|    std                | 2.85           |\n",
      "|    value_loss         | 5.41e-05       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 227           |\n",
      "|    iterations         | 5700          |\n",
      "|    time_elapsed       | 125           |\n",
      "|    total_timesteps    | 28500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -72.8         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5699          |\n",
      "|    policy_loss        | -0.395        |\n",
      "|    reward             | -9.765625e-08 |\n",
      "|    std                | 2.99          |\n",
      "|    value_loss         | 3.57e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 127         |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -74         |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | 0.575       |\n",
      "|    reward             | 0.005700323 |\n",
      "|    std                | 3.11        |\n",
      "|    value_loss         | 8.95e-05    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 226           |\n",
      "|    iterations         | 5900          |\n",
      "|    time_elapsed       | 130           |\n",
      "|    total_timesteps    | 29500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -75           |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5899          |\n",
      "|    policy_loss        | 0.283         |\n",
      "|    reward             | -0.0086948695 |\n",
      "|    std                | 3.22          |\n",
      "|    value_loss         | 3.26e-05      |\n",
      "-----------------------------------------\n",
      "day: 2959, episode: 10\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 10133.71\n",
      "total_reward: -39866.29\n",
      "total_cost: 1336.29\n",
      "total_trades: 41320\n",
      "Sharpe: 0.124\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 226            |\n",
      "|    iterations         | 6000           |\n",
      "|    time_elapsed       | 132            |\n",
      "|    total_timesteps    | 30000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -75.8          |\n",
      "|    explained_variance | 0.235          |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5999           |\n",
      "|    policy_loss        | 0.168          |\n",
      "|    reward             | -0.00074834994 |\n",
      "|    std                | 3.31           |\n",
      "|    value_loss         | 1.52e-05       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 226           |\n",
      "|    iterations         | 6100          |\n",
      "|    time_elapsed       | 134           |\n",
      "|    total_timesteps    | 30500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -76.7         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6099          |\n",
      "|    policy_loss        | 3.37          |\n",
      "|    reward             | -0.0026065253 |\n",
      "|    std                | 3.41          |\n",
      "|    value_loss         | 0.00195       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 225         |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 137         |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -77.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | -3.51       |\n",
      "|    reward             | 0.014129307 |\n",
      "|    std                | 3.51        |\n",
      "|    value_loss         | 0.00419     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 225          |\n",
      "|    iterations         | 6300         |\n",
      "|    time_elapsed       | 139          |\n",
      "|    total_timesteps    | 31500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -78.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6299         |\n",
      "|    policy_loss        | -4.63        |\n",
      "|    reward             | 0.0010328615 |\n",
      "|    std                | 3.59         |\n",
      "|    value_loss         | 0.00371      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 225          |\n",
      "|    iterations         | 6400         |\n",
      "|    time_elapsed       | 142          |\n",
      "|    total_timesteps    | 32000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -78.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6399         |\n",
      "|    policy_loss        | -1.96        |\n",
      "|    reward             | -0.034855194 |\n",
      "|    std                | 3.68         |\n",
      "|    value_loss         | 0.000783     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 225         |\n",
      "|    iterations         | 6500        |\n",
      "|    time_elapsed       | 144         |\n",
      "|    total_timesteps    | 32500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6499        |\n",
      "|    policy_loss        | 3.75        |\n",
      "|    reward             | 0.026733339 |\n",
      "|    std                | 3.75        |\n",
      "|    value_loss         | 0.00241     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 224          |\n",
      "|    iterations         | 6600         |\n",
      "|    time_elapsed       | 146          |\n",
      "|    total_timesteps    | 33000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -79.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6599         |\n",
      "|    policy_loss        | -0.553       |\n",
      "|    reward             | 0.0035429318 |\n",
      "|    std                | 3.82         |\n",
      "|    value_loss         | 5.31e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 224           |\n",
      "|    iterations         | 6700          |\n",
      "|    time_elapsed       | 149           |\n",
      "|    total_timesteps    | 33500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -80.7         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6699          |\n",
      "|    policy_loss        | -0.913        |\n",
      "|    reward             | -0.0017072067 |\n",
      "|    std                | 3.92          |\n",
      "|    value_loss         | 0.00041       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 224         |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 151         |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -81.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | -0.566      |\n",
      "|    reward             | 0.025078107 |\n",
      "|    std                | 4.04        |\n",
      "|    value_loss         | 0.000138    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 223         |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 154         |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -82.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | 0.124       |\n",
      "|    reward             | 0.006375802 |\n",
      "|    std                | 4.16        |\n",
      "|    value_loss         | 3.27e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 223          |\n",
      "|    iterations         | 7000         |\n",
      "|    time_elapsed       | 156          |\n",
      "|    total_timesteps    | 35000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -83.1        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6999         |\n",
      "|    policy_loss        | 4.12         |\n",
      "|    reward             | -0.002683654 |\n",
      "|    std                | 4.25         |\n",
      "|    value_loss         | 0.00349      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 224          |\n",
      "|    iterations         | 7100         |\n",
      "|    time_elapsed       | 158          |\n",
      "|    total_timesteps    | 35500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -83.6        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7099         |\n",
      "|    policy_loss        | -0.451       |\n",
      "|    reward             | 0.0063023185 |\n",
      "|    std                | 4.32         |\n",
      "|    value_loss         | 0.000378     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 224          |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 160          |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -84.1        |\n",
      "|    explained_variance | 1.85e-06     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | -0.841       |\n",
      "|    reward             | -0.008670982 |\n",
      "|    std                | 4.4          |\n",
      "|    value_loss         | 0.00023      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 224         |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 162         |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -84.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | -1.18       |\n",
      "|    reward             | 0.002880347 |\n",
      "|    std                | 4.5         |\n",
      "|    value_loss         | 0.000214    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 223          |\n",
      "|    iterations         | 7400         |\n",
      "|    time_elapsed       | 165          |\n",
      "|    total_timesteps    | 37000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -85.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7399         |\n",
      "|    policy_loss        | 1.1          |\n",
      "|    reward             | -0.005060942 |\n",
      "|    std                | 4.6          |\n",
      "|    value_loss         | 0.000406     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 223           |\n",
      "|    iterations         | 7500          |\n",
      "|    time_elapsed       | 167           |\n",
      "|    total_timesteps    | 37500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -85.9         |\n",
      "|    explained_variance | -0.183        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7499          |\n",
      "|    policy_loss        | 0.411         |\n",
      "|    reward             | 0.00036928253 |\n",
      "|    std                | 4.69          |\n",
      "|    value_loss         | 7.88e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 223           |\n",
      "|    iterations         | 7600          |\n",
      "|    time_elapsed       | 169           |\n",
      "|    total_timesteps    | 38000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -86.4         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7599          |\n",
      "|    policy_loss        | 7.56          |\n",
      "|    reward             | 3.4350967e-05 |\n",
      "|    std                | 4.76          |\n",
      "|    value_loss         | 0.00913       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 224         |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 171         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -86.6       |\n",
      "|    explained_variance | -0.388      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | -3.38       |\n",
      "|    reward             | 0.007665365 |\n",
      "|    std                | 4.81        |\n",
      "|    value_loss         | 0.00232     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 224           |\n",
      "|    iterations         | 7800          |\n",
      "|    time_elapsed       | 173           |\n",
      "|    total_timesteps    | 39000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -87.1         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7799          |\n",
      "|    policy_loss        | 0.726         |\n",
      "|    reward             | 0.00016376078 |\n",
      "|    std                | 4.88          |\n",
      "|    value_loss         | 7.84e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 225         |\n",
      "|    iterations         | 7900        |\n",
      "|    time_elapsed       | 175         |\n",
      "|    total_timesteps    | 39500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -87.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7899        |\n",
      "|    policy_loss        | -1.22       |\n",
      "|    reward             | 0.008345581 |\n",
      "|    std                | 4.97        |\n",
      "|    value_loss         | 0.000337    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 225          |\n",
      "|    iterations         | 8000         |\n",
      "|    time_elapsed       | 177          |\n",
      "|    total_timesteps    | 40000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -88.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7999         |\n",
      "|    policy_loss        | -0.0343      |\n",
      "|    reward             | -0.016547432 |\n",
      "|    std                | 5.06         |\n",
      "|    value_loss         | 3.34e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 226          |\n",
      "|    iterations         | 8100         |\n",
      "|    time_elapsed       | 179          |\n",
      "|    total_timesteps    | 40500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -88.6        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8099         |\n",
      "|    policy_loss        | -0.75        |\n",
      "|    reward             | -0.001247325 |\n",
      "|    std                | 5.15         |\n",
      "|    value_loss         | 0.000172     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 225         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 181         |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -89         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | 2.25        |\n",
      "|    reward             | 0.012989453 |\n",
      "|    std                | 5.22        |\n",
      "|    value_loss         | 0.000812    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 226           |\n",
      "|    iterations         | 8300          |\n",
      "|    time_elapsed       | 183           |\n",
      "|    total_timesteps    | 41500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -89.4         |\n",
      "|    explained_variance | 0.00869       |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8299          |\n",
      "|    policy_loss        | 0.161         |\n",
      "|    reward             | -0.0074232537 |\n",
      "|    std                | 5.29          |\n",
      "|    value_loss         | 0.000154      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 226          |\n",
      "|    iterations         | 8400         |\n",
      "|    time_elapsed       | 185          |\n",
      "|    total_timesteps    | 42000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -89.8        |\n",
      "|    explained_variance | -0.153       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8399         |\n",
      "|    policy_loss        | 0.967        |\n",
      "|    reward             | 0.0007623406 |\n",
      "|    std                | 5.37         |\n",
      "|    value_loss         | 0.000155     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 187         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -90.3       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -0.756      |\n",
      "|    reward             | 0.017825661 |\n",
      "|    std                | 5.46        |\n",
      "|    value_loss         | 0.000128    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 227          |\n",
      "|    iterations         | 8600         |\n",
      "|    time_elapsed       | 188          |\n",
      "|    total_timesteps    | 43000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -90.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8599         |\n",
      "|    policy_loss        | -2.51        |\n",
      "|    reward             | -0.011812323 |\n",
      "|    std                | 5.58         |\n",
      "|    value_loss         | 0.000808     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 8700        |\n",
      "|    time_elapsed       | 190         |\n",
      "|    total_timesteps    | 43500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -91.5       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8699        |\n",
      "|    policy_loss        | 0.412       |\n",
      "|    reward             | 0.005484443 |\n",
      "|    std                | 5.69        |\n",
      "|    value_loss         | 4.75e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 8800        |\n",
      "|    time_elapsed       | 193         |\n",
      "|    total_timesteps    | 44000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -92.1       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8799        |\n",
      "|    policy_loss        | -0.0181     |\n",
      "|    reward             | 0.023560641 |\n",
      "|    std                | 5.81        |\n",
      "|    value_loss         | 0.000101    |\n",
      "---------------------------------------\n",
      "day: 2959, episode: 15\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 25528.22\n",
      "total_reward: -24471.78\n",
      "total_cost: 13707.34\n",
      "total_trades: 44634\n",
      "Sharpe: 0.062\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 227          |\n",
      "|    iterations         | 8900         |\n",
      "|    time_elapsed       | 195          |\n",
      "|    total_timesteps    | 44500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -92.7        |\n",
      "|    explained_variance | 0.0246       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8899         |\n",
      "|    policy_loss        | 1.59         |\n",
      "|    reward             | 0.0039783926 |\n",
      "|    std                | 5.93         |\n",
      "|    value_loss         | 0.000704     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 227          |\n",
      "|    iterations         | 9000         |\n",
      "|    time_elapsed       | 197          |\n",
      "|    total_timesteps    | 45000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -93.3        |\n",
      "|    explained_variance | -0.241       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8999         |\n",
      "|    policy_loss        | -3.52        |\n",
      "|    reward             | -0.008198203 |\n",
      "|    std                | 6.04         |\n",
      "|    value_loss         | 0.00175      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 199         |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -94         |\n",
      "|    explained_variance | 0.381       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | -0.677      |\n",
      "|    reward             | -0.03332687 |\n",
      "|    std                | 6.19        |\n",
      "|    value_loss         | 7.04e-05    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 201        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -94.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 1.04       |\n",
      "|    reward             | 0.00831154 |\n",
      "|    std                | 6.34       |\n",
      "|    value_loss         | 0.000188   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 228         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 203         |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -95.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | -0.571      |\n",
      "|    reward             | 0.003718963 |\n",
      "|    std                | 6.5         |\n",
      "|    value_loss         | 4.75e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 206         |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -96.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | -1.9        |\n",
      "|    reward             | -0.02538269 |\n",
      "|    std                | 6.66        |\n",
      "|    value_loss         | 0.000847    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 227           |\n",
      "|    iterations         | 9500          |\n",
      "|    time_elapsed       | 208           |\n",
      "|    total_timesteps    | 47500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -96.6         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9499          |\n",
      "|    policy_loss        | 0.37          |\n",
      "|    reward             | -0.0039896998 |\n",
      "|    std                | 6.78          |\n",
      "|    value_loss         | 3.34e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 227           |\n",
      "|    iterations         | 9600          |\n",
      "|    time_elapsed       | 210           |\n",
      "|    total_timesteps    | 48000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -97.3         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9599          |\n",
      "|    policy_loss        | 0.165         |\n",
      "|    reward             | -0.0014938432 |\n",
      "|    std                | 6.95          |\n",
      "|    value_loss         | 8.65e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 228         |\n",
      "|    iterations         | 9700        |\n",
      "|    time_elapsed       | 212         |\n",
      "|    total_timesteps    | 48500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -98.1       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9699        |\n",
      "|    policy_loss        | -1.36       |\n",
      "|    reward             | 0.028768295 |\n",
      "|    std                | 7.13        |\n",
      "|    value_loss         | 0.000224    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 228           |\n",
      "|    iterations         | 9800          |\n",
      "|    time_elapsed       | 214           |\n",
      "|    total_timesteps    | 49000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -98.8         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9799          |\n",
      "|    policy_loss        | 0.677         |\n",
      "|    reward             | -0.0032200776 |\n",
      "|    std                | 7.33          |\n",
      "|    value_loss         | 5.97e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 228          |\n",
      "|    iterations         | 9900         |\n",
      "|    time_elapsed       | 216          |\n",
      "|    total_timesteps    | 49500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -99.6        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9899         |\n",
      "|    policy_loss        | -3.51        |\n",
      "|    reward             | -0.006403076 |\n",
      "|    std                | 7.51         |\n",
      "|    value_loss         | 0.00152      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 227           |\n",
      "|    iterations         | 10000         |\n",
      "|    time_elapsed       | 219           |\n",
      "|    total_timesteps    | 50000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -100          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9999          |\n",
      "|    policy_loss        | 0.0501        |\n",
      "|    reward             | -0.0040489538 |\n",
      "|    std                | 7.65          |\n",
      "|    value_loss         | 0.000137      |\n",
      "-----------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  0.012852326573706475\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 256}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_126_5\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 293           |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 6             |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0009014809 |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 294          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.015265938  |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -1.77        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.416       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0544      |\n",
      "|    reward               | 0.0022315546 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 297           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.018486608   |\n",
      "|    clip_fraction        | 0.189         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.2         |\n",
      "|    explained_variance   | -0.61         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.484        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0506       |\n",
      "|    reward               | -0.0023417857 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.048         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 297          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.018386923  |\n",
      "|    clip_fraction        | 0.21         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | -0.463       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.475       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0419      |\n",
      "|    reward               | -0.002064135 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0223       |\n",
      "------------------------------------------\n",
      "day: 2959, episode: 20\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 5012.96\n",
      "total_reward: -44987.04\n",
      "total_cost: 13228.60\n",
      "total_trades: 47760\n",
      "Sharpe: -0.180\n",
      "=================================\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 298            |\n",
      "|    iterations           | 5              |\n",
      "|    time_elapsed         | 34             |\n",
      "|    total_timesteps      | 10240          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.020842807    |\n",
      "|    clip_fraction        | 0.207          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -41.4          |\n",
      "|    explained_variance   | -2.15          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.491         |\n",
      "|    n_updates            | 40             |\n",
      "|    policy_gradient_loss | -0.0532        |\n",
      "|    reward               | -0.00039425134 |\n",
      "|    std                  | 1.01           |\n",
      "|    value_loss           | 0.0157         |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 298          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.022043396  |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.4        |\n",
      "|    explained_variance   | -0.603       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.495       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0507      |\n",
      "|    reward               | -0.005213262 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0177       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 298           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 47            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.023140835   |\n",
      "|    clip_fraction        | 0.23          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.4         |\n",
      "|    explained_variance   | -0.195        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.487        |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.0445       |\n",
      "|    reward               | -0.0073062633 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.0112        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 298           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 54            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.024014708   |\n",
      "|    clip_fraction        | 0.24          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.5         |\n",
      "|    explained_variance   | -2.87         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.492        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.0487       |\n",
      "|    reward               | -0.0011725975 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00686       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.027313607  |\n",
      "|    clip_fraction        | 0.243        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.5        |\n",
      "|    explained_variance   | -0.234       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.491       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0508      |\n",
      "|    reward               | 0.0060580377 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.011        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.025171341  |\n",
      "|    clip_fraction        | 0.264        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.6        |\n",
      "|    explained_variance   | 0.0578       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.494       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0471      |\n",
      "|    reward               | -0.008326945 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00809      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.023996897  |\n",
      "|    clip_fraction        | 0.266        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.7        |\n",
      "|    explained_variance   | -4.52        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.502       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0439      |\n",
      "|    reward               | 0.0009937317 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0037       |\n",
      "------------------------------------------\n",
      "day: 2959, episode: 25\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 4036.18\n",
      "total_reward: -45963.82\n",
      "total_cost: 15173.29\n",
      "total_trades: 47988\n",
      "Sharpe: -0.254\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.026705205  |\n",
      "|    clip_fraction        | 0.272        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.7        |\n",
      "|    explained_variance   | 0.0922       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.495       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0508      |\n",
      "|    reward               | -0.005054013 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00809      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02879791  |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -0.19       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.506      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    reward               | 0.011955836 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.00818     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 299           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 95            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.025627172   |\n",
      "|    clip_fraction        | 0.285         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.9         |\n",
      "|    explained_variance   | -1.19         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.477        |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.0375       |\n",
      "|    reward               | -0.0036078994 |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 0.0031        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.024419159  |\n",
      "|    clip_fraction        | 0.254        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42          |\n",
      "|    explained_variance   | 0.211        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.501       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0538      |\n",
      "|    reward               | 0.0018757858 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00618      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026280606 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.506      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    reward               | 0.006948719 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029194709 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.578       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.495      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    reward               | -0.01195459 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.028165344  |\n",
      "|    clip_fraction        | 0.272        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.2        |\n",
      "|    explained_variance   | -4.21        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.505       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.049       |\n",
      "|    reward               | 0.0012608689 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "day: 2959, episode: 30\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 3725.81\n",
      "total_reward: -46274.19\n",
      "total_cost: 12621.51\n",
      "total_trades: 47297\n",
      "Sharpe: -0.243\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031154469 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.511      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    reward               | 0.010874161 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.00446     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 300          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03091545   |\n",
      "|    clip_fraction        | 0.284        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.3        |\n",
      "|    explained_variance   | 0.582        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.491       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0454      |\n",
      "|    reward               | -0.009807115 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 300           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 143           |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.030077234   |\n",
      "|    clip_fraction        | 0.319         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.4         |\n",
      "|    explained_variance   | -1.53         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.495        |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.0459       |\n",
      "|    reward               | -0.0012416117 |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 0.00268       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 300           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 150           |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.03059451    |\n",
      "|    clip_fraction        | 0.28          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.4         |\n",
      "|    explained_variance   | 0.495         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.524        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.0514       |\n",
      "|    reward               | -0.0046113497 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 0.00397       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 299           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 157           |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.03856083    |\n",
      "|    clip_fraction        | 0.338         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.5         |\n",
      "|    explained_variance   | 0.542         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.492        |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.0416       |\n",
      "|    reward               | -0.0012833953 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 0.00366       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 299           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 164           |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.030216832   |\n",
      "|    clip_fraction        | 0.302         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -2.43         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.489        |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.0338       |\n",
      "|    reward               | -0.0052757896 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 0.00115       |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 299            |\n",
      "|    iterations           | 25             |\n",
      "|    time_elapsed         | 171            |\n",
      "|    total_timesteps      | 51200          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.031513974    |\n",
      "|    clip_fraction        | 0.303          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -42.6          |\n",
      "|    explained_variance   | 0.645          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.473         |\n",
      "|    n_updates            | 240            |\n",
      "|    policy_gradient_loss | -0.0478        |\n",
      "|    reward               | -0.00017758533 |\n",
      "|    std                  | 1.05           |\n",
      "|    value_loss           | 0.00301        |\n",
      "--------------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  0.283870828205795\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_5\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 137         |\n",
      "|    time_elapsed    | 85          |\n",
      "|    total_timesteps | 11840       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -7.53       |\n",
      "|    critic_loss     | 0.0379      |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 8880        |\n",
      "|    reward          | 0.028178252 |\n",
      "------------------------------------\n",
      "day: 2959, episode: 40\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 50164.05\n",
      "total_reward: 164.05\n",
      "total_cost: 49.94\n",
      "total_trades: 50389\n",
      "Sharpe: 0.355\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 8           |\n",
      "|    fps             | 94          |\n",
      "|    time_elapsed    | 251         |\n",
      "|    total_timesteps | 23680       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -3.79       |\n",
      "|    critic_loss     | 0.00675     |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 20720       |\n",
      "|    reward          | 0.028842803 |\n",
      "------------------------------------\n",
      "day: 2959, episode: 45\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 50164.05\n",
      "total_reward: 164.05\n",
      "total_cost: 49.94\n",
      "total_trades: 50301\n",
      "Sharpe: 0.355\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 12          |\n",
      "|    fps             | 77          |\n",
      "|    time_elapsed    | 461         |\n",
      "|    total_timesteps | 35520       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -2.09       |\n",
      "|    critic_loss     | 0.00169     |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 32560       |\n",
      "|    reward          | 0.028842803 |\n",
      "------------------------------------\n",
      "day: 2959, episode: 50\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 50164.05\n",
      "total_reward: 164.05\n",
      "total_cost: 49.94\n",
      "total_trades: 50301\n",
      "Sharpe: 0.355\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 16          |\n",
      "|    fps             | 78          |\n",
      "|    time_elapsed    | 603         |\n",
      "|    total_timesteps | 47360       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -1.18       |\n",
      "|    critic_loss     | 0.000898    |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 44400       |\n",
      "|    reward          | 0.028842803 |\n",
      "------------------------------------\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2009-04-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  203.40201777637154\n",
      "======Model training from:  2009-04-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_189_4\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 130           |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 3             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.2         |\n",
      "|    explained_variance | -2.39         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -12.2         |\n",
      "|    reward             | -0.0040586465 |\n",
      "|    std                | 1             |\n",
      "|    value_loss         | 0.119         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 137         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -36.6       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 4.65        |\n",
      "|    reward             | 0.074225545 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.0203      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 142        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0.678      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -2.73      |\n",
      "|    reward             | 0.11787241 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0069     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 151          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 13           |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -3.31        |\n",
      "|    reward             | -0.008698461 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.00726      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 159          |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 15           |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.9        |\n",
      "|    explained_variance | -2.91        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | 1.35         |\n",
      "|    reward             | -0.016047029 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 0.00289      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 164        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -18.8      |\n",
      "|    reward             | 0.06388337 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.263      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 169          |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 20           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.3        |\n",
      "|    explained_variance | -5.34        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | 0.573        |\n",
      "|    reward             | -0.010346141 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 0.00441      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 172         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0.000845    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -2.66       |\n",
      "|    reward             | 0.017358992 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.00429     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 174         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | -0.472      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -0.00647    |\n",
      "|    reward             | 0.114222035 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.000392    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 177         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.5       |\n",
      "|    explained_variance | -0.131      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 0.286       |\n",
      "|    reward             | 0.008989396 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 7.57e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 178         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.9       |\n",
      "|    explained_variance | -0.0506     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 0.512       |\n",
      "|    reward             | 0.000892118 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 0.000614    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 174        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.3      |\n",
      "|    explained_variance | 0.611      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 2.29       |\n",
      "|    reward             | 0.07531347 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.00314    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 172         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 37          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.8       |\n",
      "|    explained_variance | -2.49       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 0.311       |\n",
      "|    reward             | -0.01752067 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.000155    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 170           |\n",
      "|    iterations         | 1400          |\n",
      "|    time_elapsed       | 40            |\n",
      "|    total_timesteps    | 7000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -45.4         |\n",
      "|    explained_variance | 0.488         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1399          |\n",
      "|    policy_loss        | 0.23          |\n",
      "|    reward             | -0.0032627513 |\n",
      "|    std                | 1.16          |\n",
      "|    value_loss         | 9.58e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 170          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 43           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -46.1        |\n",
      "|    explained_variance | 0.487        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | -1.18        |\n",
      "|    reward             | 0.0016970207 |\n",
      "|    std                | 1.19         |\n",
      "|    value_loss         | 0.000947     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 167           |\n",
      "|    iterations         | 1600          |\n",
      "|    time_elapsed       | 47            |\n",
      "|    total_timesteps    | 8000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -46.7         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1599          |\n",
      "|    policy_loss        | -0.0978       |\n",
      "|    reward             | -0.0055141603 |\n",
      "|    std                | 1.21          |\n",
      "|    value_loss         | 3.08e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 167         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.2       |\n",
      "|    explained_variance | 0.128       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -3.4        |\n",
      "|    reward             | 0.010723946 |\n",
      "|    std                | 1.23        |\n",
      "|    value_loss         | 0.00632     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 167         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.7       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 0.491       |\n",
      "|    reward             | 0.015553352 |\n",
      "|    std                | 1.26        |\n",
      "|    value_loss         | 0.000183    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 166          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 57           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -48.3        |\n",
      "|    explained_variance | -8.4         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 0.0739       |\n",
      "|    reward             | -0.014304541 |\n",
      "|    std                | 1.28         |\n",
      "|    value_loss         | 0.000181     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 164         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.9       |\n",
      "|    explained_variance | -10.1       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 0.000555    |\n",
      "|    reward             | 0.016269477 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 6.18e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 163          |\n",
      "|    iterations         | 2100         |\n",
      "|    time_elapsed       | 64           |\n",
      "|    total_timesteps    | 10500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -49.6        |\n",
      "|    explained_variance | 9.54e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2099         |\n",
      "|    policy_loss        | 1.56         |\n",
      "|    reward             | -0.005893859 |\n",
      "|    std                | 1.34         |\n",
      "|    value_loss         | 0.00103      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 161         |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -50.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 0.131       |\n",
      "|    reward             | -0.02053954 |\n",
      "|    std                | 1.37        |\n",
      "|    value_loss         | 0.0001      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 161        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 1.68       |\n",
      "|    reward             | 0.03422702 |\n",
      "|    std                | 1.4        |\n",
      "|    value_loss         | 0.00163    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 162          |\n",
      "|    iterations         | 2400         |\n",
      "|    time_elapsed       | 73           |\n",
      "|    total_timesteps    | 12000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -51.4        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2399         |\n",
      "|    policy_loss        | 2.66         |\n",
      "|    reward             | -0.018511409 |\n",
      "|    std                | 1.43         |\n",
      "|    value_loss         | 0.0049       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 162          |\n",
      "|    iterations         | 2500         |\n",
      "|    time_elapsed       | 76           |\n",
      "|    total_timesteps    | 12500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -52          |\n",
      "|    explained_variance | 0.00105      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2499         |\n",
      "|    policy_loss        | 1.75         |\n",
      "|    reward             | -0.097200416 |\n",
      "|    std                | 1.46         |\n",
      "|    value_loss         | 0.00109      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 161           |\n",
      "|    iterations         | 2600          |\n",
      "|    time_elapsed       | 80            |\n",
      "|    total_timesteps    | 13000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -52.8         |\n",
      "|    explained_variance | -0.204        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2599          |\n",
      "|    policy_loss        | -0.644        |\n",
      "|    reward             | -0.0069629634 |\n",
      "|    std                | 1.5           |\n",
      "|    value_loss         | 0.000298      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 161         |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 83          |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -53.8       |\n",
      "|    explained_variance | 0.126       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | 0.715       |\n",
      "|    reward             | 0.003235195 |\n",
      "|    std                | 1.55        |\n",
      "|    value_loss         | 0.000255    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 163          |\n",
      "|    iterations         | 2800         |\n",
      "|    time_elapsed       | 85           |\n",
      "|    total_timesteps    | 14000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -54.7        |\n",
      "|    explained_variance | -1.58        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2799         |\n",
      "|    policy_loss        | -0.53        |\n",
      "|    reward             | 0.0039662938 |\n",
      "|    std                | 1.6          |\n",
      "|    value_loss         | 0.000143     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 2900         |\n",
      "|    time_elapsed       | 88           |\n",
      "|    total_timesteps    | 14500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -55.6        |\n",
      "|    explained_variance | 0.161        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2899         |\n",
      "|    policy_loss        | -0.0681      |\n",
      "|    reward             | -0.003791105 |\n",
      "|    std                | 1.65         |\n",
      "|    value_loss         | 0.000795     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 165          |\n",
      "|    iterations         | 3000         |\n",
      "|    time_elapsed       | 90           |\n",
      "|    total_timesteps    | 15000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -56.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2999         |\n",
      "|    policy_loss        | 1.01         |\n",
      "|    reward             | -0.038602784 |\n",
      "|    std                | 1.68         |\n",
      "|    value_loss         | 0.000583     |\n",
      "----------------------------------------\n",
      "day: 3022, episode: 5\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 20567.88\n",
      "total_reward: -29432.12\n",
      "total_cost: 5349.84\n",
      "total_trades: 41951\n",
      "Sharpe: 0.201\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 166          |\n",
      "|    iterations         | 3100         |\n",
      "|    time_elapsed       | 92           |\n",
      "|    total_timesteps    | 15500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -56.8        |\n",
      "|    explained_variance | 0.0298       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3099         |\n",
      "|    policy_loss        | -0.215       |\n",
      "|    reward             | 0.0017336815 |\n",
      "|    std                | 1.72         |\n",
      "|    value_loss         | 2.23e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 167         |\n",
      "|    iterations         | 3200        |\n",
      "|    time_elapsed       | 95          |\n",
      "|    total_timesteps    | 16000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -57.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3199        |\n",
      "|    policy_loss        | -0.577      |\n",
      "|    reward             | 0.008195509 |\n",
      "|    std                | 1.76        |\n",
      "|    value_loss         | 0.000115    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 168          |\n",
      "|    iterations         | 3300         |\n",
      "|    time_elapsed       | 97           |\n",
      "|    total_timesteps    | 16500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -58.4        |\n",
      "|    explained_variance | 0.0711       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3299         |\n",
      "|    policy_loss        | 0.63         |\n",
      "|    reward             | -0.021855056 |\n",
      "|    std                | 1.81         |\n",
      "|    value_loss         | 0.000132     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 169          |\n",
      "|    iterations         | 3400         |\n",
      "|    time_elapsed       | 100          |\n",
      "|    total_timesteps    | 17000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -59.1        |\n",
      "|    explained_variance | -2.16        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3399         |\n",
      "|    policy_loss        | -0.771       |\n",
      "|    reward             | -0.007308748 |\n",
      "|    std                | 1.86         |\n",
      "|    value_loss         | 0.000326     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 170         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 102         |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -59.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | -0.464      |\n",
      "|    reward             | 0.012702599 |\n",
      "|    std                | 1.9         |\n",
      "|    value_loss         | 0.000104    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 170          |\n",
      "|    iterations         | 3600         |\n",
      "|    time_elapsed       | 105          |\n",
      "|    total_timesteps    | 18000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -60.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3599         |\n",
      "|    policy_loss        | -6.93        |\n",
      "|    reward             | -0.034555625 |\n",
      "|    std                | 1.94         |\n",
      "|    value_loss         | 0.0202       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 169          |\n",
      "|    iterations         | 3700         |\n",
      "|    time_elapsed       | 109          |\n",
      "|    total_timesteps    | 18500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -60.9        |\n",
      "|    explained_variance | -0.637       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3699         |\n",
      "|    policy_loss        | -0.55        |\n",
      "|    reward             | -0.015389941 |\n",
      "|    std                | 1.98         |\n",
      "|    value_loss         | 0.00015      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 168          |\n",
      "|    iterations         | 3800         |\n",
      "|    time_elapsed       | 112          |\n",
      "|    total_timesteps    | 19000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -61.5        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3799         |\n",
      "|    policy_loss        | -0.316       |\n",
      "|    reward             | -0.016261533 |\n",
      "|    std                | 2.02         |\n",
      "|    value_loss         | 9.84e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 168          |\n",
      "|    iterations         | 3900         |\n",
      "|    time_elapsed       | 115          |\n",
      "|    total_timesteps    | 19500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -62.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3899         |\n",
      "|    policy_loss        | 1.38         |\n",
      "|    reward             | -0.012800949 |\n",
      "|    std                | 2.07         |\n",
      "|    value_loss         | 0.000627     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 167          |\n",
      "|    iterations         | 4000         |\n",
      "|    time_elapsed       | 119          |\n",
      "|    total_timesteps    | 20000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -62.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3999         |\n",
      "|    policy_loss        | -1.22        |\n",
      "|    reward             | -0.013279169 |\n",
      "|    std                | 2.11         |\n",
      "|    value_loss         | 0.000418     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 167           |\n",
      "|    iterations         | 4100          |\n",
      "|    time_elapsed       | 122           |\n",
      "|    total_timesteps    | 20500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -63.4         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4099          |\n",
      "|    policy_loss        | -0.759        |\n",
      "|    reward             | -0.0022084622 |\n",
      "|    std                | 2.16          |\n",
      "|    value_loss         | 0.000148      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 167          |\n",
      "|    iterations         | 4200         |\n",
      "|    time_elapsed       | 125          |\n",
      "|    total_timesteps    | 21000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -64          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4199         |\n",
      "|    policy_loss        | 3.48         |\n",
      "|    reward             | -0.013172226 |\n",
      "|    std                | 2.2          |\n",
      "|    value_loss         | 0.0029       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 166           |\n",
      "|    iterations         | 4300          |\n",
      "|    time_elapsed       | 128           |\n",
      "|    total_timesteps    | 21500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -64.6         |\n",
      "|    explained_variance | 0.0943        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4299          |\n",
      "|    policy_loss        | -0.87         |\n",
      "|    reward             | -0.0023377363 |\n",
      "|    std                | 2.25          |\n",
      "|    value_loss         | 0.000208      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 166           |\n",
      "|    iterations         | 4400          |\n",
      "|    time_elapsed       | 132           |\n",
      "|    total_timesteps    | 22000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -65.5         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4399          |\n",
      "|    policy_loss        | -1.27         |\n",
      "|    reward             | -0.0010246858 |\n",
      "|    std                | 2.32          |\n",
      "|    value_loss         | 0.000523      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 166          |\n",
      "|    iterations         | 4500         |\n",
      "|    time_elapsed       | 135          |\n",
      "|    total_timesteps    | 22500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -66.5        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4499         |\n",
      "|    policy_loss        | -0.637       |\n",
      "|    reward             | 0.0024230627 |\n",
      "|    std                | 2.4          |\n",
      "|    value_loss         | 0.000172     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 165          |\n",
      "|    iterations         | 4600         |\n",
      "|    time_elapsed       | 138          |\n",
      "|    total_timesteps    | 23000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -67.7        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4599         |\n",
      "|    policy_loss        | 0.0242       |\n",
      "|    reward             | -0.002409655 |\n",
      "|    std                | 2.51         |\n",
      "|    value_loss         | 3.81e-06     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 164         |\n",
      "|    iterations         | 4700        |\n",
      "|    time_elapsed       | 142         |\n",
      "|    total_timesteps    | 23500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -69.1       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4699        |\n",
      "|    policy_loss        | 0.347       |\n",
      "|    reward             | 0.001994657 |\n",
      "|    std                | 2.62        |\n",
      "|    value_loss         | 2.79e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 4800         |\n",
      "|    time_elapsed       | 146          |\n",
      "|    total_timesteps    | 24000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -70.5        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4799         |\n",
      "|    policy_loss        | 0.105        |\n",
      "|    reward             | 0.0044995565 |\n",
      "|    std                | 2.76         |\n",
      "|    value_loss         | 5.84e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 163           |\n",
      "|    iterations         | 4900          |\n",
      "|    time_elapsed       | 149           |\n",
      "|    total_timesteps    | 24500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -71.7         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4899          |\n",
      "|    policy_loss        | -1.56         |\n",
      "|    reward             | -0.0057553956 |\n",
      "|    std                | 2.87          |\n",
      "|    value_loss         | 0.000545      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 163         |\n",
      "|    iterations         | 5000        |\n",
      "|    time_elapsed       | 153         |\n",
      "|    total_timesteps    | 25000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -72.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4999        |\n",
      "|    policy_loss        | 0.313       |\n",
      "|    reward             | 0.002454438 |\n",
      "|    std                | 2.97        |\n",
      "|    value_loss         | 2.13e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 163          |\n",
      "|    iterations         | 5100         |\n",
      "|    time_elapsed       | 156          |\n",
      "|    total_timesteps    | 25500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -73.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5099         |\n",
      "|    policy_loss        | -0.7         |\n",
      "|    reward             | -0.005636198 |\n",
      "|    std                | 3.09         |\n",
      "|    value_loss         | 0.000117     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 163          |\n",
      "|    iterations         | 5200         |\n",
      "|    time_elapsed       | 158          |\n",
      "|    total_timesteps    | 26000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -75.2        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5199         |\n",
      "|    policy_loss        | -0.103       |\n",
      "|    reward             | 0.0014107277 |\n",
      "|    std                | 3.23         |\n",
      "|    value_loss         | 3.77e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 164           |\n",
      "|    iterations         | 5300          |\n",
      "|    time_elapsed       | 161           |\n",
      "|    total_timesteps    | 26500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -76.5         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5299          |\n",
      "|    policy_loss        | 0.758         |\n",
      "|    reward             | 0.00018978494 |\n",
      "|    std                | 3.39          |\n",
      "|    value_loss         | 0.000132      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 164         |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 163         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -77.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | 0.74        |\n",
      "|    reward             | 0.007468901 |\n",
      "|    std                | 3.54        |\n",
      "|    value_loss         | 0.000222    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 165          |\n",
      "|    iterations         | 5500         |\n",
      "|    time_elapsed       | 166          |\n",
      "|    total_timesteps    | 27500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -78.7        |\n",
      "|    explained_variance | -0.0877      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5499         |\n",
      "|    policy_loss        | -3.88        |\n",
      "|    reward             | 0.0015236662 |\n",
      "|    std                | 3.65         |\n",
      "|    value_loss         | 0.00556      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 165         |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 168         |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79.2       |\n",
      "|    explained_variance | 0.488       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | -3.87       |\n",
      "|    reward             | 0.034393128 |\n",
      "|    std                | 3.72        |\n",
      "|    value_loss         | 0.00262     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 166         |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 171         |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | 0.532       |\n",
      "|    reward             | 0.013548913 |\n",
      "|    std                | 3.77        |\n",
      "|    value_loss         | 0.000147    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 167         |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 173         |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79.9       |\n",
      "|    explained_variance | 0.118       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | -2.4        |\n",
      "|    reward             | 0.090686694 |\n",
      "|    std                | 3.81        |\n",
      "|    value_loss         | 0.00109     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 167        |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -80.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 18.7       |\n",
      "|    reward             | 0.08133357 |\n",
      "|    std                | 3.85       |\n",
      "|    value_loss         | 0.0785     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 167         |\n",
      "|    iterations         | 6000        |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 30000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -80.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5999        |\n",
      "|    policy_loss        | 17.3        |\n",
      "|    reward             | 0.043906957 |\n",
      "|    std                | 3.87        |\n",
      "|    value_loss         | 0.0549      |\n",
      "---------------------------------------\n",
      "day: 3022, episode: 10\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 144940.49\n",
      "total_reward: 94940.49\n",
      "total_cost: 23296.47\n",
      "total_trades: 46410\n",
      "Sharpe: 0.529\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 167            |\n",
      "|    iterations         | 6100           |\n",
      "|    time_elapsed       | 182            |\n",
      "|    total_timesteps    | 30500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -80.6          |\n",
      "|    explained_variance | -0.0193        |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 6099           |\n",
      "|    policy_loss        | 9.25           |\n",
      "|    reward             | -0.00062873354 |\n",
      "|    std                | 3.9            |\n",
      "|    value_loss         | 0.0132         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 166          |\n",
      "|    iterations         | 6200         |\n",
      "|    time_elapsed       | 185          |\n",
      "|    total_timesteps    | 31000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -80.9        |\n",
      "|    explained_variance | 0.597        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6199         |\n",
      "|    policy_loss        | -7.34        |\n",
      "|    reward             | -0.045174856 |\n",
      "|    std                | 3.94         |\n",
      "|    value_loss         | 0.00848      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 166         |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 189         |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -81.2       |\n",
      "|    explained_variance | 0.0596      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | -3.5        |\n",
      "|    reward             | 0.004695608 |\n",
      "|    std                | 3.98        |\n",
      "|    value_loss         | 0.00223     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 166        |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -81.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 1.26       |\n",
      "|    reward             | 0.04894969 |\n",
      "|    std                | 4.04       |\n",
      "|    value_loss         | 0.000317   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 165         |\n",
      "|    iterations         | 6500        |\n",
      "|    time_elapsed       | 196         |\n",
      "|    total_timesteps    | 32500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -82.1       |\n",
      "|    explained_variance | -0.083      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6499        |\n",
      "|    policy_loss        | -1.89       |\n",
      "|    reward             | 0.011144747 |\n",
      "|    std                | 4.11        |\n",
      "|    value_loss         | 0.00128     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 165         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 199         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -82.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | 17.9        |\n",
      "|    reward             | 0.017563578 |\n",
      "|    std                | 4.17        |\n",
      "|    value_loss         | 0.0645      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 165          |\n",
      "|    iterations         | 6700         |\n",
      "|    time_elapsed       | 202          |\n",
      "|    total_timesteps    | 33500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -82.8        |\n",
      "|    explained_variance | 0.0508       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6699         |\n",
      "|    policy_loss        | -0.586       |\n",
      "|    reward             | -0.013271439 |\n",
      "|    std                | 4.21         |\n",
      "|    value_loss         | 0.000107     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 165        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 205        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -83.2      |\n",
      "|    explained_variance | 0.266      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 3.31       |\n",
      "|    reward             | 0.03294416 |\n",
      "|    std                | 4.26       |\n",
      "|    value_loss         | 0.00174    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 164        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 209        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -83.6      |\n",
      "|    explained_variance | 0.51       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 0.0595     |\n",
      "|    reward             | 0.02025644 |\n",
      "|    std                | 4.33       |\n",
      "|    value_loss         | 0.000174   |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 164           |\n",
      "|    iterations         | 7000          |\n",
      "|    time_elapsed       | 212           |\n",
      "|    total_timesteps    | 35000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -84.1         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6999          |\n",
      "|    policy_loss        | -0.0953       |\n",
      "|    reward             | 0.00054169603 |\n",
      "|    std                | 4.4           |\n",
      "|    value_loss         | 0.000142      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 164           |\n",
      "|    iterations         | 7100          |\n",
      "|    time_elapsed       | 215           |\n",
      "|    total_timesteps    | 35500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -84.6         |\n",
      "|    explained_variance | 0.0105        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7099          |\n",
      "|    policy_loss        | -1.06         |\n",
      "|    reward             | -0.0077294707 |\n",
      "|    std                | 4.49          |\n",
      "|    value_loss         | 0.000353      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 164        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -85.2      |\n",
      "|    explained_variance | 0.174      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -10        |\n",
      "|    reward             | 0.17903735 |\n",
      "|    std                | 4.57       |\n",
      "|    value_loss         | 0.0146     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 7300         |\n",
      "|    time_elapsed       | 221          |\n",
      "|    total_timesteps    | 36500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -85.6        |\n",
      "|    explained_variance | -5.75        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7299         |\n",
      "|    policy_loss        | 0.403        |\n",
      "|    reward             | -0.002374757 |\n",
      "|    std                | 4.64         |\n",
      "|    value_loss         | 4.44e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 164         |\n",
      "|    iterations         | 7400        |\n",
      "|    time_elapsed       | 225         |\n",
      "|    total_timesteps    | 37000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -86.3       |\n",
      "|    explained_variance | -3.14       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7399        |\n",
      "|    policy_loss        | -0.35       |\n",
      "|    reward             | 0.003216507 |\n",
      "|    std                | 4.75        |\n",
      "|    value_loss         | 3.44e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 7500         |\n",
      "|    time_elapsed       | 227          |\n",
      "|    total_timesteps    | 37500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -87.2        |\n",
      "|    explained_variance | -1.16        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7499         |\n",
      "|    policy_loss        | -0.412       |\n",
      "|    reward             | 0.0048480546 |\n",
      "|    std                | 4.91         |\n",
      "|    value_loss         | 4.2e-05      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 7600         |\n",
      "|    time_elapsed       | 230          |\n",
      "|    total_timesteps    | 38000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -88.4        |\n",
      "|    explained_variance | 0.806        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7599         |\n",
      "|    policy_loss        | 0.822        |\n",
      "|    reward             | -0.009369518 |\n",
      "|    std                | 5.11         |\n",
      "|    value_loss         | 9.22e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 165           |\n",
      "|    iterations         | 7700          |\n",
      "|    time_elapsed       | 232           |\n",
      "|    total_timesteps    | 38500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -89.8         |\n",
      "|    explained_variance | -0.582        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7699          |\n",
      "|    policy_loss        | -1.2          |\n",
      "|    reward             | -0.0045260224 |\n",
      "|    std                | 5.36          |\n",
      "|    value_loss         | 0.000212      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 165         |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 235         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -91.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | 0.0972      |\n",
      "|    reward             | 0.001488377 |\n",
      "|    std                | 5.64        |\n",
      "|    value_loss         | 3.4e-06     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 166        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 237        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -92.6      |\n",
      "|    explained_variance | -0.156     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -0.396     |\n",
      "|    reward             | 0.01519644 |\n",
      "|    std                | 5.91       |\n",
      "|    value_loss         | 9.56e-05   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 166         |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 239         |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -93.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | 1.56        |\n",
      "|    reward             | 0.003311145 |\n",
      "|    std                | 6.14        |\n",
      "|    value_loss         | 0.000405    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 167          |\n",
      "|    iterations         | 8100         |\n",
      "|    time_elapsed       | 242          |\n",
      "|    total_timesteps    | 40500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -95          |\n",
      "|    explained_variance | -1.43        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8099         |\n",
      "|    policy_loss        | -0.222       |\n",
      "|    reward             | -0.001537436 |\n",
      "|    std                | 6.42         |\n",
      "|    value_loss         | 7.28e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 167           |\n",
      "|    iterations         | 8200          |\n",
      "|    time_elapsed       | 244           |\n",
      "|    total_timesteps    | 41000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -96.4         |\n",
      "|    explained_variance | 0.0738        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8199          |\n",
      "|    policy_loss        | -0.0597       |\n",
      "|    reward             | -0.0033864398 |\n",
      "|    std                | 6.73          |\n",
      "|    value_loss         | 5.67e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 167           |\n",
      "|    iterations         | 8300          |\n",
      "|    time_elapsed       | 247           |\n",
      "|    total_timesteps    | 41500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -97.9         |\n",
      "|    explained_variance | -0.106        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8299          |\n",
      "|    policy_loss        | 0.325         |\n",
      "|    reward             | 5.7037352e-05 |\n",
      "|    std                | 7.08          |\n",
      "|    value_loss         | 1.42e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 167           |\n",
      "|    iterations         | 8400          |\n",
      "|    time_elapsed       | 250           |\n",
      "|    total_timesteps    | 42000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -99.4         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8399          |\n",
      "|    policy_loss        | 0.0234        |\n",
      "|    reward             | 0.00015804543 |\n",
      "|    std                | 7.48          |\n",
      "|    value_loss         | 2.22e-07      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 167          |\n",
      "|    iterations         | 8500         |\n",
      "|    time_elapsed       | 253          |\n",
      "|    total_timesteps    | 42500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -101         |\n",
      "|    explained_variance | 0.254        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8499         |\n",
      "|    policy_loss        | 0.244        |\n",
      "|    reward             | 0.0012950478 |\n",
      "|    std                | 7.83         |\n",
      "|    value_loss         | 6.98e-05     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 167        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 257        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -102       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 3.96       |\n",
      "|    reward             | 0.01403883 |\n",
      "|    std                | 8.04       |\n",
      "|    value_loss         | 0.00155    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 166          |\n",
      "|    iterations         | 8700         |\n",
      "|    time_elapsed       | 260          |\n",
      "|    total_timesteps    | 43500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -102         |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8699         |\n",
      "|    policy_loss        | 0.0288       |\n",
      "|    reward             | -0.006638778 |\n",
      "|    std                | 8.25         |\n",
      "|    value_loss         | 0.000119     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 166          |\n",
      "|    iterations         | 8800         |\n",
      "|    time_elapsed       | 264          |\n",
      "|    total_timesteps    | 44000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -103         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8799         |\n",
      "|    policy_loss        | 3.59         |\n",
      "|    reward             | -0.013436489 |\n",
      "|    std                | 8.48         |\n",
      "|    value_loss         | 0.00148      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 166          |\n",
      "|    iterations         | 8900         |\n",
      "|    time_elapsed       | 267          |\n",
      "|    total_timesteps    | 44500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -104         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8899         |\n",
      "|    policy_loss        | -1.86        |\n",
      "|    reward             | -0.009248886 |\n",
      "|    std                | 8.71         |\n",
      "|    value_loss         | 0.000357     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 165           |\n",
      "|    iterations         | 9000          |\n",
      "|    time_elapsed       | 271           |\n",
      "|    total_timesteps    | 45000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -105          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8999          |\n",
      "|    policy_loss        | 0.386         |\n",
      "|    reward             | -0.0032337783 |\n",
      "|    std                | 8.95          |\n",
      "|    value_loss         | 6.52e-05      |\n",
      "-----------------------------------------\n",
      "day: 3022, episode: 15\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 11454.50\n",
      "total_reward: -38545.50\n",
      "total_cost: 27983.90\n",
      "total_trades: 49487\n",
      "Sharpe: -0.181\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 165          |\n",
      "|    iterations         | 9100         |\n",
      "|    time_elapsed       | 274          |\n",
      "|    total_timesteps    | 45500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -105         |\n",
      "|    explained_variance | -0.206       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9099         |\n",
      "|    policy_loss        | -0.352       |\n",
      "|    reward             | 0.0029876216 |\n",
      "|    std                | 9.15         |\n",
      "|    value_loss         | 4.57e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 165         |\n",
      "|    iterations         | 9200        |\n",
      "|    time_elapsed       | 277         |\n",
      "|    total_timesteps    | 46000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -106        |\n",
      "|    explained_variance | 0.872       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9199        |\n",
      "|    policy_loss        | 1.93        |\n",
      "|    reward             | 0.005347574 |\n",
      "|    std                | 9.4         |\n",
      "|    value_loss         | 0.000356    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 165           |\n",
      "|    iterations         | 9300          |\n",
      "|    time_elapsed       | 281           |\n",
      "|    total_timesteps    | 46500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -107          |\n",
      "|    explained_variance | -0.0681       |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9299          |\n",
      "|    policy_loss        | -0.00816      |\n",
      "|    reward             | -0.0019402265 |\n",
      "|    std                | 9.75          |\n",
      "|    value_loss         | 9.13e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 164           |\n",
      "|    iterations         | 9400          |\n",
      "|    time_elapsed       | 284           |\n",
      "|    total_timesteps    | 47000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -108          |\n",
      "|    explained_variance | -0.409        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9399          |\n",
      "|    policy_loss        | -0.187        |\n",
      "|    reward             | -0.0032439963 |\n",
      "|    std                | 10.2          |\n",
      "|    value_loss         | 0.000131      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 9500         |\n",
      "|    time_elapsed       | 288          |\n",
      "|    total_timesteps    | 47500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -110         |\n",
      "|    explained_variance | 0.282        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9499         |\n",
      "|    policy_loss        | 1.06         |\n",
      "|    reward             | 0.0038585255 |\n",
      "|    std                | 10.7         |\n",
      "|    value_loss         | 0.000195     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 9600         |\n",
      "|    time_elapsed       | 291          |\n",
      "|    total_timesteps    | 48000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -111         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9599         |\n",
      "|    policy_loss        | 0.904        |\n",
      "|    reward             | 0.0012810373 |\n",
      "|    std                | 11.2         |\n",
      "|    value_loss         | 7.8e-05      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 164         |\n",
      "|    iterations         | 9700        |\n",
      "|    time_elapsed       | 294         |\n",
      "|    total_timesteps    | 48500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -112        |\n",
      "|    explained_variance | -0.42       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9699        |\n",
      "|    policy_loss        | -2.91       |\n",
      "|    reward             | 0.004359619 |\n",
      "|    std                | 11.7        |\n",
      "|    value_loss         | 0.000804    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 9800         |\n",
      "|    time_elapsed       | 298          |\n",
      "|    total_timesteps    | 49000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -113         |\n",
      "|    explained_variance | 0.0713       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9799         |\n",
      "|    policy_loss        | -0.28        |\n",
      "|    reward             | -0.010769198 |\n",
      "|    std                | 12.1         |\n",
      "|    value_loss         | 0.000118     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 164           |\n",
      "|    iterations         | 9900          |\n",
      "|    time_elapsed       | 301           |\n",
      "|    total_timesteps    | 49500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -115          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9899          |\n",
      "|    policy_loss        | -0.939        |\n",
      "|    reward             | -0.0018761986 |\n",
      "|    std                | 12.6          |\n",
      "|    value_loss         | 0.000133      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 164            |\n",
      "|    iterations         | 10000          |\n",
      "|    time_elapsed       | 304            |\n",
      "|    total_timesteps    | 50000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -116           |\n",
      "|    explained_variance | -0.203         |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 9999           |\n",
      "|    policy_loss        | 0.975          |\n",
      "|    reward             | -0.00048315976 |\n",
      "|    std                | 13.1           |\n",
      "|    value_loss         | 8.78e-05       |\n",
      "------------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  0.3179602899279771\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 256}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_189_4\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 223         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 9           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.008695435 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.015616746  |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -4.93        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.475       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0523      |\n",
      "|    reward               | 0.0050142533 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0995       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016587557 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -1.16       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.481      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    reward               | 0.006457845 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.04        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.016777754  |\n",
      "|    clip_fraction        | 0.183        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | -0.638       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.477       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0407      |\n",
      "|    reward               | -0.008348287 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0173       |\n",
      "------------------------------------------\n",
      "day: 3022, episode: 20\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 7292.64\n",
      "total_reward: -42707.36\n",
      "total_cost: 17684.75\n",
      "total_trades: 49693\n",
      "Sharpe: -0.194\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.018840369  |\n",
      "|    clip_fraction        | 0.206        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.4        |\n",
      "|    explained_variance   | -6.4         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.49        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0514      |\n",
      "|    reward               | 0.0035604516 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0155       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.021702264  |\n",
      "|    clip_fraction        | 0.228        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.4        |\n",
      "|    explained_variance   | -0.675       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.491       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.048       |\n",
      "|    reward               | -0.010627215 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0164       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 210           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.019632824   |\n",
      "|    clip_fraction        | 0.209         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.5         |\n",
      "|    explained_variance   | -0.0151       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.477        |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.0397       |\n",
      "|    reward               | -0.0038953414 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00708       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021507416 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | -8.59       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.491      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    reward               | 0.005853975 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0091      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021118538 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | -0.171      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.496      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    reward               | 0.01276454  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.024423212  |\n",
      "|    clip_fraction        | 0.266        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.7        |\n",
      "|    explained_variance   | 0.29         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.444       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0393      |\n",
      "|    reward               | 3.990704e-06 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00591      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026709698 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -8.4        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.495      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    reward               | 0.013462625 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "day: 3022, episode: 25\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 2445.49\n",
      "total_reward: -47554.51\n",
      "total_cost: 11742.57\n",
      "total_trades: 48443\n",
      "Sharpe: -0.345\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.025190916  |\n",
      "|    clip_fraction        | 0.253        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.9        |\n",
      "|    explained_variance   | 0.0892       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.489       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0498      |\n",
      "|    reward               | 0.0058229356 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00726      |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 207            |\n",
      "|    iterations           | 13             |\n",
      "|    time_elapsed         | 128            |\n",
      "|    total_timesteps      | 26624          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.03000899     |\n",
      "|    clip_fraction        | 0.291          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -41.9          |\n",
      "|    explained_variance   | 0.457          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.452         |\n",
      "|    n_updates            | 120            |\n",
      "|    policy_gradient_loss | -0.0403        |\n",
      "|    reward               | -0.00038246156 |\n",
      "|    std                  | 1.03           |\n",
      "|    value_loss           | 0.004          |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.022917133  |\n",
      "|    clip_fraction        | 0.27         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42          |\n",
      "|    explained_variance   | -8.97        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.504       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0489      |\n",
      "|    reward               | -0.010212985 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029843131 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.496      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    reward               | 0.006681261 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 205           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 159           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.02626456    |\n",
      "|    clip_fraction        | 0.245         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.1         |\n",
      "|    explained_variance   | 0.583         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.46         |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.0408       |\n",
      "|    reward               | -0.0019529794 |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 0.00326       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02634529  |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | -5.44       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.486      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    reward               | 0.001868153 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.00197     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 205           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 179           |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.030371636   |\n",
      "|    clip_fraction        | 0.3           |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.3         |\n",
      "|    explained_variance   | 0.494         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.477        |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.0459       |\n",
      "|    reward               | -0.0011226096 |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 0.004         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 206           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 188           |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.029242098   |\n",
      "|    clip_fraction        | 0.279         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.3         |\n",
      "|    explained_variance   | 0.692         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.496        |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.0423       |\n",
      "|    reward               | 0.00024743564 |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 0.00267       |\n",
      "-------------------------------------------\n",
      "day: 3022, episode: 30\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 1978.52\n",
      "total_reward: -48021.48\n",
      "total_cost: 5537.74\n",
      "total_trades: 47792\n",
      "Sharpe: -0.314\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 206           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 198           |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.026173282   |\n",
      "|    clip_fraction        | 0.265         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.4         |\n",
      "|    explained_variance   | -12.2         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.476        |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.0409       |\n",
      "|    reward               | -0.0009734142 |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 0.00106       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 209          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03118802   |\n",
      "|    clip_fraction        | 0.304        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.4        |\n",
      "|    explained_variance   | 0.406        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.512       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0541      |\n",
      "|    reward               | -0.013468607 |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.0036       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 205           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 219           |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.032928575   |\n",
      "|    clip_fraction        | 0.316         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.5         |\n",
      "|    explained_variance   | 0.548         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.501        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.043        |\n",
      "|    reward               | -0.0017364168 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 0.00335       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 206           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 228           |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.033373643   |\n",
      "|    clip_fraction        | 0.321         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.5         |\n",
      "|    explained_variance   | -0.864        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.474        |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.0332       |\n",
      "|    reward               | -0.0028200806 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 0.00178       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 238          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.031805098  |\n",
      "|    clip_fraction        | 0.316        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.5        |\n",
      "|    explained_variance   | 0.589        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.509       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0457      |\n",
      "|    reward               | -0.005953168 |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.00294      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 248         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032391943 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.495      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    reward               | 0.003885131 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  -0.07988173985969069\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_4\n",
      "day: 3022, episode: 35\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 92960.63\n",
      "total_reward: 42960.63\n",
      "total_cost: 623.61\n",
      "total_trades: 48982\n",
      "Sharpe: 0.502\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 80          |\n",
      "|    time_elapsed    | 150         |\n",
      "|    total_timesteps | 12092       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -2.36       |\n",
      "|    critic_loss     | 0.0206      |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 9069        |\n",
      "|    reward          | 0.008286331 |\n",
      "------------------------------------\n",
      "day: 3022, episode: 40\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 50072.34\n",
      "total_reward: 72.34\n",
      "total_cost: 49.95\n",
      "total_trades: 42308\n",
      "Sharpe: 0.304\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 8           |\n",
      "|    fps             | 78          |\n",
      "|    time_elapsed    | 306         |\n",
      "|    total_timesteps | 24184       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -1.32       |\n",
      "|    critic_loss     | 0.00577     |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 21161       |\n",
      "|    reward          | 0.008286331 |\n",
      "------------------------------------\n",
      "day: 3022, episode: 45\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 50072.34\n",
      "total_reward: 72.34\n",
      "total_cost: 49.95\n",
      "total_trades: 42308\n",
      "Sharpe: 0.304\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 12          |\n",
      "|    fps             | 77          |\n",
      "|    time_elapsed    | 467         |\n",
      "|    total_timesteps | 36276       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -0.75       |\n",
      "|    critic_loss     | 0.00141     |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 33253       |\n",
      "|    reward          | 0.008286331 |\n",
      "------------------------------------\n",
      "day: 3022, episode: 50\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 50072.34\n",
      "total_reward: 72.34\n",
      "total_cost: 49.95\n",
      "total_trades: 42308\n",
      "Sharpe: 0.304\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 16          |\n",
      "|    fps             | 74          |\n",
      "|    time_elapsed    | 650         |\n",
      "|    total_timesteps | 48368       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -0.45       |\n",
      "|    critic_loss     | 0.00538     |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 45345       |\n",
      "|    reward          | 0.008286331 |\n",
      "------------------------------------\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2009-04-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  203.40201777637154\n",
      "======Model training from:  2009-04-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_252_4\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -2.09       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -8.43       |\n",
      "|    reward             | -0.03075812 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.0495      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.68      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -8.11      |\n",
      "|    reward             | 0.15008362 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.0404     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 110         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0.556       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -8.13       |\n",
      "|    reward             | 0.014295843 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.0355      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 113          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -6.03        |\n",
      "|    reward             | -0.022376942 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.031        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 114         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -6.65       |\n",
      "|    reward             | 0.016232008 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.0333      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 103        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | -0.2016778 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.102      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 95            |\n",
      "|    iterations         | 700           |\n",
      "|    time_elapsed       | 36            |\n",
      "|    total_timesteps    | 3500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -42.1         |\n",
      "|    explained_variance | -212          |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 699           |\n",
      "|    policy_loss        | -2.45         |\n",
      "|    reward             | -0.0005628746 |\n",
      "|    std                | 1.04          |\n",
      "|    value_loss         | 0.0081        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 91            |\n",
      "|    iterations         | 800           |\n",
      "|    time_elapsed       | 43            |\n",
      "|    total_timesteps    | 4000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -42.4         |\n",
      "|    explained_variance | -0.58         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 799           |\n",
      "|    policy_loss        | 7.17          |\n",
      "|    reward             | -0.0035269957 |\n",
      "|    std                | 1.04          |\n",
      "|    value_loss         | 0.0298        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | -2.85       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 2.32        |\n",
      "|    reward             | 0.008380456 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.00445     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 88           |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 56           |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -43.5        |\n",
      "|    explained_variance | -3.16        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | -1.39        |\n",
      "|    reward             | 0.0062254244 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 0.00113      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 90          |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.2       |\n",
      "|    explained_variance | -13         |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 0.75        |\n",
      "|    reward             | 0.012493926 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.00049     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 91           |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 65           |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -44.8        |\n",
      "|    explained_variance | -1.55        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -1.35        |\n",
      "|    reward             | -0.014821142 |\n",
      "|    std                | 1.14         |\n",
      "|    value_loss         | 0.00143      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 89           |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 72           |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -45.3        |\n",
      "|    explained_variance | -35.6        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | 0.807        |\n",
      "|    reward             | -0.005361226 |\n",
      "|    std                | 1.16         |\n",
      "|    value_loss         | 0.00181      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.7       |\n",
      "|    explained_variance | -1.68       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 2.81        |\n",
      "|    reward             | 0.009170498 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 0.00413     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 88          |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 84          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.2       |\n",
      "|    explained_variance | -29.3       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | -0.674      |\n",
      "|    reward             | -0.00139602 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 0.000311    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 89           |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 89           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -46.9        |\n",
      "|    explained_variance | -0.918       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | -1.34        |\n",
      "|    reward             | 0.0061928662 |\n",
      "|    std                | 1.22         |\n",
      "|    value_loss         | 0.00114      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 89           |\n",
      "|    iterations         | 1700         |\n",
      "|    time_elapsed       | 95           |\n",
      "|    total_timesteps    | 8500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -47.6        |\n",
      "|    explained_variance | -2.32        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1699         |\n",
      "|    policy_loss        | 0.596        |\n",
      "|    reward             | -0.001486388 |\n",
      "|    std                | 1.25         |\n",
      "|    value_loss         | 0.000249     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 89           |\n",
      "|    iterations         | 1800         |\n",
      "|    time_elapsed       | 101          |\n",
      "|    total_timesteps    | 9000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -48.5        |\n",
      "|    explained_variance | 1.79e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1799         |\n",
      "|    policy_loss        | 0.99         |\n",
      "|    reward             | 0.0036565894 |\n",
      "|    std                | 1.29         |\n",
      "|    value_loss         | 0.00167      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 105         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -49         |\n",
      "|    explained_variance | -5.25       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 1.1         |\n",
      "|    reward             | 0.019088106 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 0.00223     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 90           |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 110          |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -49.3        |\n",
      "|    explained_variance | 1.39e-05     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | -3.56        |\n",
      "|    reward             | 0.0044506337 |\n",
      "|    std                | 1.33         |\n",
      "|    value_loss         | 0.00506      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 90          |\n",
      "|    iterations         | 2100        |\n",
      "|    time_elapsed       | 115         |\n",
      "|    total_timesteps    | 10500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -49.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2099        |\n",
      "|    policy_loss        | 0.831       |\n",
      "|    reward             | 0.002090992 |\n",
      "|    std                | 1.34        |\n",
      "|    value_loss         | 0.000642    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | -1.6       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 3.13       |\n",
      "|    reward             | 0.03269141 |\n",
      "|    std                | 1.35       |\n",
      "|    value_loss         | 0.00534    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 90           |\n",
      "|    iterations         | 2300         |\n",
      "|    time_elapsed       | 126          |\n",
      "|    total_timesteps    | 11500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -50.2        |\n",
      "|    explained_variance | 0.092        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2299         |\n",
      "|    policy_loss        | 5.74         |\n",
      "|    reward             | -0.024676995 |\n",
      "|    std                | 1.37         |\n",
      "|    value_loss         | 0.0309       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 132        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 2.42       |\n",
      "|    reward             | -0.5439474 |\n",
      "|    std                | 1.38       |\n",
      "|    value_loss         | 0.0107     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 91           |\n",
      "|    iterations         | 2500         |\n",
      "|    time_elapsed       | 137          |\n",
      "|    total_timesteps    | 12500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -50.5        |\n",
      "|    explained_variance | -8.7         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2499         |\n",
      "|    policy_loss        | -4.27        |\n",
      "|    reward             | 0.0017305337 |\n",
      "|    std                | 1.38         |\n",
      "|    value_loss         | 0.0169       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 91          |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 141         |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -50.7       |\n",
      "|    explained_variance | 0.515       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 1.39        |\n",
      "|    reward             | -0.01951621 |\n",
      "|    std                | 1.39        |\n",
      "|    value_loss         | 0.00129     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 91           |\n",
      "|    iterations         | 2700         |\n",
      "|    time_elapsed       | 147          |\n",
      "|    total_timesteps    | 13500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -50.9        |\n",
      "|    explained_variance | 2.09e-06     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2699         |\n",
      "|    policy_loss        | 2.08         |\n",
      "|    reward             | -0.009369558 |\n",
      "|    std                | 1.4          |\n",
      "|    value_loss         | 0.00293      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51.2      |\n",
      "|    explained_variance | -1.02      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 1.19       |\n",
      "|    reward             | 0.16718662 |\n",
      "|    std                | 1.42       |\n",
      "|    value_loss         | 0.00252    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 91           |\n",
      "|    iterations         | 2900         |\n",
      "|    time_elapsed       | 157          |\n",
      "|    total_timesteps    | 14500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -51.4        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2899         |\n",
      "|    policy_loss        | 3.47         |\n",
      "|    reward             | -0.015615026 |\n",
      "|    std                | 1.43         |\n",
      "|    value_loss         | 0.00527      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 92          |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 162         |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -51.6       |\n",
      "|    explained_variance | 0.0832      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | 9.31        |\n",
      "|    reward             | -0.11598405 |\n",
      "|    std                | 1.43        |\n",
      "|    value_loss         | 0.0392      |\n",
      "---------------------------------------\n",
      "day: 3085, episode: 5\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 173859.28\n",
      "total_reward: 123859.28\n",
      "total_cost: 18287.99\n",
      "total_trades: 50300\n",
      "Sharpe: 0.619\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 92         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 167        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -4.04      |\n",
      "|    reward             | 0.03385759 |\n",
      "|    std                | 1.44       |\n",
      "|    value_loss         | 0.00741    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 171        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -52        |\n",
      "|    explained_variance | 0.113      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -3.23      |\n",
      "|    reward             | 0.01057111 |\n",
      "|    std                | 1.46       |\n",
      "|    value_loss         | 0.00411    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 93           |\n",
      "|    iterations         | 3300         |\n",
      "|    time_elapsed       | 176          |\n",
      "|    total_timesteps    | 16500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -52.4        |\n",
      "|    explained_variance | 0.481        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3299         |\n",
      "|    policy_loss        | -0.409       |\n",
      "|    reward             | -0.019664401 |\n",
      "|    std                | 1.48         |\n",
      "|    value_loss         | 0.000537     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 93            |\n",
      "|    iterations         | 3400          |\n",
      "|    time_elapsed       | 181           |\n",
      "|    total_timesteps    | 17000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -52.9         |\n",
      "|    explained_variance | 0.452         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3399          |\n",
      "|    policy_loss        | 0.0732        |\n",
      "|    reward             | -0.0004206174 |\n",
      "|    std                | 1.5           |\n",
      "|    value_loss         | 9.43e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 93           |\n",
      "|    iterations         | 3500         |\n",
      "|    time_elapsed       | 186          |\n",
      "|    total_timesteps    | 17500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -53.3        |\n",
      "|    explained_variance | 0.0998       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3499         |\n",
      "|    policy_loss        | 1.02         |\n",
      "|    reward             | -0.009359633 |\n",
      "|    std                | 1.52         |\n",
      "|    value_loss         | 0.000462     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 191        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -53.7      |\n",
      "|    explained_variance | 0.209      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 5.09       |\n",
      "|    reward             | 0.07123692 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 0.0113     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -54        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 4.12       |\n",
      "|    reward             | 0.08480362 |\n",
      "|    std                | 1.56       |\n",
      "|    value_loss         | 0.011      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 3800        |\n",
      "|    time_elapsed       | 201         |\n",
      "|    total_timesteps    | 19000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -54.3       |\n",
      "|    explained_variance | 0.135       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3799        |\n",
      "|    policy_loss        | -3.99       |\n",
      "|    reward             | 0.051741064 |\n",
      "|    std                | 1.58        |\n",
      "|    value_loss         | 0.00802     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 3900        |\n",
      "|    time_elapsed       | 205         |\n",
      "|    total_timesteps    | 19500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -54.7       |\n",
      "|    explained_variance | -0.816      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3899        |\n",
      "|    policy_loss        | -0.797      |\n",
      "|    reward             | -0.01634168 |\n",
      "|    std                | 1.6         |\n",
      "|    value_loss         | 0.000466    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 210        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -55.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | 0.02543369 |\n",
      "|    std                | 1.62       |\n",
      "|    value_loss         | 0.058      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 95            |\n",
      "|    iterations         | 4100          |\n",
      "|    time_elapsed       | 214           |\n",
      "|    total_timesteps    | 20500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -55.4         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4099          |\n",
      "|    policy_loss        | 2.56          |\n",
      "|    reward             | -0.0013508686 |\n",
      "|    std                | 1.63          |\n",
      "|    value_loss         | 0.00281       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 4200         |\n",
      "|    time_elapsed       | 219          |\n",
      "|    total_timesteps    | 21000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -55.6        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4199         |\n",
      "|    policy_loss        | 8.13         |\n",
      "|    reward             | -0.012914697 |\n",
      "|    std                | 1.65         |\n",
      "|    value_loss         | 0.0266       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 224        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -55.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -14.4      |\n",
      "|    reward             | 0.12305355 |\n",
      "|    std                | 1.66       |\n",
      "|    value_loss         | 0.133      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 4400        |\n",
      "|    time_elapsed       | 229         |\n",
      "|    total_timesteps    | 22000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -56.1       |\n",
      "|    explained_variance | -0.119      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4399        |\n",
      "|    policy_loss        | 1.46        |\n",
      "|    reward             | 0.018730987 |\n",
      "|    std                | 1.68        |\n",
      "|    value_loss         | 0.000715    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 96           |\n",
      "|    iterations         | 4500         |\n",
      "|    time_elapsed       | 233          |\n",
      "|    total_timesteps    | 22500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -56.5        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4499         |\n",
      "|    policy_loss        | -2.11        |\n",
      "|    reward             | -0.004669091 |\n",
      "|    std                | 1.7          |\n",
      "|    value_loss         | 0.00153      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 238         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -56.9       |\n",
      "|    explained_variance | 0.109       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | -6.63       |\n",
      "|    reward             | 0.057507105 |\n",
      "|    std                | 1.73        |\n",
      "|    value_loss         | 0.0154      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 96            |\n",
      "|    iterations         | 4700          |\n",
      "|    time_elapsed       | 242           |\n",
      "|    total_timesteps    | 23500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -57.3         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4699          |\n",
      "|    policy_loss        | 0.0383        |\n",
      "|    reward             | -0.0061491933 |\n",
      "|    std                | 1.75          |\n",
      "|    value_loss         | 0.000219      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 4800         |\n",
      "|    time_elapsed       | 246          |\n",
      "|    total_timesteps    | 24000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -57.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4799         |\n",
      "|    policy_loss        | 2.86         |\n",
      "|    reward             | -0.031009356 |\n",
      "|    std                | 1.78         |\n",
      "|    value_loss         | 0.00308      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 251        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -58        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | 2          |\n",
      "|    reward             | 0.03290545 |\n",
      "|    std                | 1.79       |\n",
      "|    value_loss         | 0.0014     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 5000         |\n",
      "|    time_elapsed       | 256          |\n",
      "|    total_timesteps    | 25000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -58.4        |\n",
      "|    explained_variance | 0.0454       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4999         |\n",
      "|    policy_loss        | 0.775        |\n",
      "|    reward             | 0.0009202772 |\n",
      "|    std                | 1.81         |\n",
      "|    value_loss         | 0.000369     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 261         |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -58.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | 1.1         |\n",
      "|    reward             | 0.015310198 |\n",
      "|    std                | 1.85        |\n",
      "|    value_loss         | 0.000381    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 5200         |\n",
      "|    time_elapsed       | 266          |\n",
      "|    total_timesteps    | 26000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -59.7        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5199         |\n",
      "|    policy_loss        | -0.13        |\n",
      "|    reward             | 0.0062517268 |\n",
      "|    std                | 1.9          |\n",
      "|    value_loss         | 5.8e-05      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 5300         |\n",
      "|    time_elapsed       | 271          |\n",
      "|    total_timesteps    | 26500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -60.4        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5299         |\n",
      "|    policy_loss        | -0.153       |\n",
      "|    reward             | -0.007767721 |\n",
      "|    std                | 1.94         |\n",
      "|    value_loss         | 3.46e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 5400         |\n",
      "|    time_elapsed       | 276          |\n",
      "|    total_timesteps    | 27000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -61.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5399         |\n",
      "|    policy_loss        | -0.866       |\n",
      "|    reward             | 0.0037374236 |\n",
      "|    std                | 2            |\n",
      "|    value_loss         | 0.000315     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 5500        |\n",
      "|    time_elapsed       | 281         |\n",
      "|    total_timesteps    | 27500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -61.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5499        |\n",
      "|    policy_loss        | 3.57        |\n",
      "|    reward             | 0.048440434 |\n",
      "|    std                | 2.05        |\n",
      "|    value_loss         | 0.00379     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 287         |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -62.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | 1.45        |\n",
      "|    reward             | 0.023225712 |\n",
      "|    std                | 2.09        |\n",
      "|    value_loss         | 0.000683    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 292         |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -63.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | -0.369      |\n",
      "|    reward             | 0.006335346 |\n",
      "|    std                | 2.14        |\n",
      "|    value_loss         | 3.6e-05     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 298         |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -63.8       |\n",
      "|    explained_variance | 0.0881      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | 1.89        |\n",
      "|    reward             | 0.005296567 |\n",
      "|    std                | 2.19        |\n",
      "|    value_loss         | 0.00115     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 5900         |\n",
      "|    time_elapsed       | 302          |\n",
      "|    total_timesteps    | 29500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -64.5        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5899         |\n",
      "|    policy_loss        | 0.184        |\n",
      "|    reward             | 0.0070329164 |\n",
      "|    std                | 2.24         |\n",
      "|    value_loss         | 0.000499     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 6000         |\n",
      "|    time_elapsed       | 307          |\n",
      "|    total_timesteps    | 30000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -65.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5999         |\n",
      "|    policy_loss        | 4.27         |\n",
      "|    reward             | -0.038684707 |\n",
      "|    std                | 2.29         |\n",
      "|    value_loss         | 0.00432      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 6100        |\n",
      "|    time_elapsed       | 312         |\n",
      "|    total_timesteps    | 30500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -65.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6099        |\n",
      "|    policy_loss        | -2.36       |\n",
      "|    reward             | 0.008452335 |\n",
      "|    std                | 2.33        |\n",
      "|    value_loss         | 0.0016      |\n",
      "---------------------------------------\n",
      "day: 3085, episode: 10\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 26606.92\n",
      "total_reward: -23393.08\n",
      "total_cost: 13316.22\n",
      "total_trades: 45231\n",
      "Sharpe: 0.111\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 97            |\n",
      "|    iterations         | 6200          |\n",
      "|    time_elapsed       | 317           |\n",
      "|    total_timesteps    | 31000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -66.1         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6199          |\n",
      "|    policy_loss        | 0.0879        |\n",
      "|    reward             | -0.0053836317 |\n",
      "|    std                | 2.36          |\n",
      "|    value_loss         | 4.03e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 97           |\n",
      "|    iterations         | 6300         |\n",
      "|    time_elapsed       | 323          |\n",
      "|    total_timesteps    | 31500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -66.6        |\n",
      "|    explained_variance | 0.564        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6299         |\n",
      "|    policy_loss        | 2.09         |\n",
      "|    reward             | -0.006410814 |\n",
      "|    std                | 2.41         |\n",
      "|    value_loss         | 0.00108      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 329         |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -67.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | -1.59       |\n",
      "|    reward             | 0.023721358 |\n",
      "|    std                | 2.47        |\n",
      "|    value_loss         | 0.00065     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 96           |\n",
      "|    iterations         | 6500         |\n",
      "|    time_elapsed       | 335          |\n",
      "|    total_timesteps    | 32500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -68          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6499         |\n",
      "|    policy_loss        | 1.94         |\n",
      "|    reward             | -0.008661695 |\n",
      "|    std                | 2.53         |\n",
      "|    value_loss         | 0.000843     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 341         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -68.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -2.27       |\n",
      "|    reward             | 0.004886668 |\n",
      "|    std                | 2.58        |\n",
      "|    value_loss         | 0.00166     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 96            |\n",
      "|    iterations         | 6700          |\n",
      "|    time_elapsed       | 347           |\n",
      "|    total_timesteps    | 33500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -68.9         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6699          |\n",
      "|    policy_loss        | -2.95         |\n",
      "|    reward             | -0.0023991822 |\n",
      "|    std                | 2.61          |\n",
      "|    value_loss         | 0.00263       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 353         |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -69.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | -0.601      |\n",
      "|    reward             | 0.012147407 |\n",
      "|    std                | 2.63        |\n",
      "|    value_loss         | 8.72e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 96           |\n",
      "|    iterations         | 6900         |\n",
      "|    time_elapsed       | 358          |\n",
      "|    total_timesteps    | 34500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -69.7        |\n",
      "|    explained_variance | 0.526        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6899         |\n",
      "|    policy_loss        | -0.599       |\n",
      "|    reward             | 0.0038468642 |\n",
      "|    std                | 2.68         |\n",
      "|    value_loss         | 0.000137     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 96            |\n",
      "|    iterations         | 7000          |\n",
      "|    time_elapsed       | 363           |\n",
      "|    total_timesteps    | 35000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -70.4         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6999          |\n",
      "|    policy_loss        | -0.477        |\n",
      "|    reward             | -0.0025611215 |\n",
      "|    std                | 2.75          |\n",
      "|    value_loss         | 6.99e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 7100         |\n",
      "|    time_elapsed       | 370          |\n",
      "|    total_timesteps    | 35500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -71.2        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7099         |\n",
      "|    policy_loss        | -0.729       |\n",
      "|    reward             | -0.012127895 |\n",
      "|    std                | 2.82         |\n",
      "|    value_loss         | 0.000222     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 376          |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -71.9        |\n",
      "|    explained_variance | 1.79e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | 1.25         |\n",
      "|    reward             | 0.0029971497 |\n",
      "|    std                | 2.9          |\n",
      "|    value_loss         | 0.000394     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 95            |\n",
      "|    iterations         | 7300          |\n",
      "|    time_elapsed       | 381           |\n",
      "|    total_timesteps    | 36500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -72.5         |\n",
      "|    explained_variance | 0.0454        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7299          |\n",
      "|    policy_loss        | -0.432        |\n",
      "|    reward             | -0.0004969105 |\n",
      "|    std                | 2.96          |\n",
      "|    value_loss         | 0.000654      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 7400         |\n",
      "|    time_elapsed       | 386          |\n",
      "|    total_timesteps    | 37000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -73          |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7399         |\n",
      "|    policy_loss        | -6.41        |\n",
      "|    reward             | -0.011555189 |\n",
      "|    std                | 3.01         |\n",
      "|    value_loss         | 0.00765      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 7500         |\n",
      "|    time_elapsed       | 393          |\n",
      "|    total_timesteps    | 37500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -73.5        |\n",
      "|    explained_variance | 0.113        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7499         |\n",
      "|    policy_loss        | -0.404       |\n",
      "|    reward             | 0.0016223728 |\n",
      "|    std                | 3.06         |\n",
      "|    value_loss         | 9.86e-05     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 397        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -74.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 0.178      |\n",
      "|    reward             | 0.01593337 |\n",
      "|    std                | 3.14       |\n",
      "|    value_loss         | 0.000461   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 403         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -75.1       |\n",
      "|    explained_variance | -0.063      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | -1.12       |\n",
      "|    reward             | -0.02380873 |\n",
      "|    std                | 3.24        |\n",
      "|    value_loss         | 0.000331    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 408         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -76         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | -0.0643     |\n",
      "|    reward             | 0.019136842 |\n",
      "|    std                | 3.34        |\n",
      "|    value_loss         | 1.15e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 7900         |\n",
      "|    time_elapsed       | 413          |\n",
      "|    total_timesteps    | 39500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -77          |\n",
      "|    explained_variance | 0.0976       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7899         |\n",
      "|    policy_loss        | -0.476       |\n",
      "|    reward             | -0.012870645 |\n",
      "|    std                | 3.45         |\n",
      "|    value_loss         | 4.65e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 8000         |\n",
      "|    time_elapsed       | 419          |\n",
      "|    total_timesteps    | 40000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -77.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7999         |\n",
      "|    policy_loss        | -4.76        |\n",
      "|    reward             | 0.0006886471 |\n",
      "|    std                | 3.55         |\n",
      "|    value_loss         | 0.00411      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 8100         |\n",
      "|    time_elapsed       | 423          |\n",
      "|    total_timesteps    | 40500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -78.5        |\n",
      "|    explained_variance | 0.000734     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8099         |\n",
      "|    policy_loss        | -1.4         |\n",
      "|    reward             | 0.0013722706 |\n",
      "|    std                | 3.64         |\n",
      "|    value_loss         | 0.000449     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 429         |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -0.728      |\n",
      "|    reward             | -0.06456305 |\n",
      "|    std                | 3.7         |\n",
      "|    value_loss         | 0.00031     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 8300        |\n",
      "|    time_elapsed       | 435         |\n",
      "|    total_timesteps    | 41500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8299        |\n",
      "|    policy_loss        | -0.562      |\n",
      "|    reward             | 0.083276324 |\n",
      "|    std                | 3.73        |\n",
      "|    value_loss         | 0.000165    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 8400         |\n",
      "|    time_elapsed       | 440          |\n",
      "|    total_timesteps    | 42000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -79.6        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8399         |\n",
      "|    policy_loss        | 9.44         |\n",
      "|    reward             | 0.0030327956 |\n",
      "|    std                | 3.77         |\n",
      "|    value_loss         | 0.0168       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 445         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -0.615      |\n",
      "|    reward             | -0.03308299 |\n",
      "|    std                | 3.8         |\n",
      "|    value_loss         | 0.000455    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 8600        |\n",
      "|    time_elapsed       | 450         |\n",
      "|    total_timesteps    | 43000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -79.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8599        |\n",
      "|    policy_loss        | -40.1       |\n",
      "|    reward             | -0.04193592 |\n",
      "|    std                | 3.82        |\n",
      "|    value_loss         | 0.298       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 8700        |\n",
      "|    time_elapsed       | 456         |\n",
      "|    total_timesteps    | 43500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -80.2       |\n",
      "|    explained_variance | -2.5        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8699        |\n",
      "|    policy_loss        | -2.32       |\n",
      "|    reward             | -0.01420198 |\n",
      "|    std                | 3.85        |\n",
      "|    value_loss         | 0.000905    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 8800         |\n",
      "|    time_elapsed       | 461          |\n",
      "|    total_timesteps    | 44000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -80.6        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8799         |\n",
      "|    policy_loss        | -0.187       |\n",
      "|    reward             | 0.0017606558 |\n",
      "|    std                | 3.91         |\n",
      "|    value_loss         | 3.56e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 8900        |\n",
      "|    time_elapsed       | 465         |\n",
      "|    total_timesteps    | 44500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -81.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8899        |\n",
      "|    policy_loss        | -0.494      |\n",
      "|    reward             | 0.014099917 |\n",
      "|    std                | 3.99        |\n",
      "|    value_loss         | 7.92e-05    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 95            |\n",
      "|    iterations         | 9000          |\n",
      "|    time_elapsed       | 471           |\n",
      "|    total_timesteps    | 45000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -82           |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8999          |\n",
      "|    policy_loss        | -0.033        |\n",
      "|    reward             | -0.0062269187 |\n",
      "|    std                | 4.1           |\n",
      "|    value_loss         | 4.19e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 9100         |\n",
      "|    time_elapsed       | 476          |\n",
      "|    total_timesteps    | 45500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -82.9        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9099         |\n",
      "|    policy_loss        | 0.737        |\n",
      "|    reward             | -0.008861457 |\n",
      "|    std                | 4.23         |\n",
      "|    value_loss         | 0.000207     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 9200        |\n",
      "|    time_elapsed       | 481         |\n",
      "|    total_timesteps    | 46000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -83.6       |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9199        |\n",
      "|    policy_loss        | 1.51        |\n",
      "|    reward             | 0.017340863 |\n",
      "|    std                | 4.34        |\n",
      "|    value_loss         | 0.000394    |\n",
      "---------------------------------------\n",
      "day: 3085, episode: 15\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 22430.34\n",
      "total_reward: -27569.66\n",
      "total_cost: 9919.01\n",
      "total_trades: 43203\n",
      "Sharpe: 0.286\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 487         |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -84.2       |\n",
      "|    explained_variance | 0.00522     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | -1.71       |\n",
      "|    reward             | 0.009160658 |\n",
      "|    std                | 4.43        |\n",
      "|    value_loss         | 0.000861    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 95         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 492        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -84.8      |\n",
      "|    explained_variance | 0.728      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 1.29       |\n",
      "|    reward             | 0.07283049 |\n",
      "|    std                | 4.51       |\n",
      "|    value_loss         | 0.000236   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 9500         |\n",
      "|    time_elapsed       | 497          |\n",
      "|    total_timesteps    | 47500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -85.2        |\n",
      "|    explained_variance | 0.0169       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9499         |\n",
      "|    policy_loss        | 1.17         |\n",
      "|    reward             | 0.0070764665 |\n",
      "|    std                | 4.58         |\n",
      "|    value_loss         | 0.000339     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 95          |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 503         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -85.5       |\n",
      "|    explained_variance | 0.0564      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | -9.05       |\n",
      "|    reward             | -0.04551615 |\n",
      "|    std                | 4.64        |\n",
      "|    value_loss         | 0.0155      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 95             |\n",
      "|    iterations         | 9700           |\n",
      "|    time_elapsed       | 508            |\n",
      "|    total_timesteps    | 48500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -85.9          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 9699           |\n",
      "|    policy_loss        | 1.91           |\n",
      "|    reward             | -0.00024576232 |\n",
      "|    std                | 4.7            |\n",
      "|    value_loss         | 0.000857       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 9800         |\n",
      "|    time_elapsed       | 514          |\n",
      "|    total_timesteps    | 49000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -86.3        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9799         |\n",
      "|    policy_loss        | 6.55         |\n",
      "|    reward             | -0.014803415 |\n",
      "|    std                | 4.75         |\n",
      "|    value_loss         | 0.00853      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 9900         |\n",
      "|    time_elapsed       | 520          |\n",
      "|    total_timesteps    | 49500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -86.6        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9899         |\n",
      "|    policy_loss        | -0.337       |\n",
      "|    reward             | 0.0084901815 |\n",
      "|    std                | 4.8          |\n",
      "|    value_loss         | 4.14e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 95           |\n",
      "|    iterations         | 10000        |\n",
      "|    time_elapsed       | 525          |\n",
      "|    total_timesteps    | 50000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -86.9        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9999         |\n",
      "|    policy_loss        | 0.935        |\n",
      "|    reward             | -0.012147713 |\n",
      "|    std                | 4.87         |\n",
      "|    value_loss         | 0.000174     |\n",
      "----------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -0.05166509698733083\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 256}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_252_4\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 168         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 12          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.009873062 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0167191    |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -2.81        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.463       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.056       |\n",
      "|    reward               | 0.0009050945 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.139        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 159           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.016683675   |\n",
      "|    clip_fraction        | 0.183         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.2         |\n",
      "|    explained_variance   | -1.65         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.47         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0467       |\n",
      "|    reward               | -0.0007126549 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0461        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016725559 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -6.53       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.462      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    reward               | 0.005974678 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0133      |\n",
      "-----------------------------------------\n",
      "day: 3085, episode: 20\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 23497.77\n",
      "total_reward: -26502.23\n",
      "total_cost: 40511.08\n",
      "total_trades: 53735\n",
      "Sharpe: 0.011\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.021630295  |\n",
      "|    clip_fraction        | 0.213        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | -1.55        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.458       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0578      |\n",
      "|    reward               | 0.0066147423 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0234       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.021783054  |\n",
      "|    clip_fraction        | 0.235        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | -0.468       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.48        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0443      |\n",
      "|    reward               | 0.0018360995 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0179       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022345763 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -3.65       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.485      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    reward               | 0.012126563 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00501     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022280678 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.564      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.457      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    reward               | 0.002896973 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0133      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02186874  |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.0407     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.487      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    reward               | 0.011374977 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 160           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 127           |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.03178544    |\n",
      "|    clip_fraction        | 0.308         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.5         |\n",
      "|    explained_variance   | -5.25         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.473        |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.0324       |\n",
      "|    reward               | -0.0039702966 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00289       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.024808351  |\n",
      "|    clip_fraction        | 0.253        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.6        |\n",
      "|    explained_variance   | -0.087       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.463       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0509      |\n",
      "|    reward               | -0.005209609 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00888      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 160           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 153           |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.028883277   |\n",
      "|    clip_fraction        | 0.285         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.6         |\n",
      "|    explained_variance   | 0.273         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.494        |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.0494       |\n",
      "|    reward               | -7.863503e-05 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00593       |\n",
      "-------------------------------------------\n",
      "day: 3085, episode: 25\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 7166.68\n",
      "total_reward: -42833.32\n",
      "total_cost: 20065.32\n",
      "total_trades: 50888\n",
      "Sharpe: -0.225\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 168          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.029052299  |\n",
      "|    clip_fraction        | 0.286        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.6        |\n",
      "|    explained_variance   | -1.56        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.48        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0342      |\n",
      "|    reward               | 0.0112389475 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00253      |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 157            |\n",
      "|    iterations           | 14             |\n",
      "|    time_elapsed         | 181            |\n",
      "|    total_timesteps      | 28672          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.027057681    |\n",
      "|    clip_fraction        | 0.281          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -41.6          |\n",
      "|    explained_variance   | 0.301          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.494         |\n",
      "|    n_updates            | 130            |\n",
      "|    policy_gradient_loss | -0.0484        |\n",
      "|    reward               | -0.00080501253 |\n",
      "|    std                  | 1.02           |\n",
      "|    value_loss           | 0.00559        |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 158           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 193           |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.03201914    |\n",
      "|    clip_fraction        | 0.287         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.7         |\n",
      "|    explained_variance   | 0.515         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.489        |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.0439       |\n",
      "|    reward               | 0.00017024935 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.00433       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 206          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03216313   |\n",
      "|    clip_fraction        | 0.304        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.8        |\n",
      "|    explained_variance   | -4.97        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.488       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0376      |\n",
      "|    reward               | 0.0003920257 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0011       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 219          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.030172603  |\n",
      "|    clip_fraction        | 0.292        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.8        |\n",
      "|    explained_variance   | 0.559        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.497       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0475      |\n",
      "|    reward               | 0.0034285951 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02902034  |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.51       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    reward               | 0.005894899 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.00279     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 246          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.035653718  |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.9        |\n",
      "|    explained_variance   | -0.96        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.48        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0322      |\n",
      "|    reward               | -0.006054883 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00135      |\n",
      "------------------------------------------\n",
      "day: 3085, episode: 30\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 7344.57\n",
      "total_reward: -42655.43\n",
      "total_cost: 23426.20\n",
      "total_trades: 51309\n",
      "Sharpe: -0.289\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 259          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.027694162  |\n",
      "|    clip_fraction        | 0.294        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42          |\n",
      "|    explained_variance   | 0.541        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.499       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.045       |\n",
      "|    reward               | -0.012000104 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00307      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 272          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.029417783  |\n",
      "|    clip_fraction        | 0.263        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42          |\n",
      "|    explained_variance   | 0.673        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.497       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0467      |\n",
      "|    reward               | -0.006815547 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00228      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 158           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 284           |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.03482625    |\n",
      "|    clip_fraction        | 0.341         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.1         |\n",
      "|    explained_variance   | -0.309        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.478        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.0258       |\n",
      "|    reward               | -0.0023528151 |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 0.00232       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 159           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 295           |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.030903526   |\n",
      "|    clip_fraction        | 0.295         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.1         |\n",
      "|    explained_variance   | 0.699         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.506        |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.0522       |\n",
      "|    reward               | -0.0013146728 |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 0.00245       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 308          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.033238534  |\n",
      "|    clip_fraction        | 0.292        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.2        |\n",
      "|    explained_variance   | 0.694        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.49        |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0398      |\n",
      "|    reward               | -0.009377017 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.00275      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03990893  |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | -0.668      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.485      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    reward               | 0.009173163 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  -0.07726033490335123\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_4\n",
      "day: 3085, episode: 35\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 147944.10\n",
      "total_reward: 97944.10\n",
      "total_cost: 953.07\n",
      "total_trades: 46885\n",
      "Sharpe: 0.530\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    episodes        | 4            |\n",
      "|    fps             | 120          |\n",
      "|    time_elapsed    | 102          |\n",
      "|    total_timesteps | 12344        |\n",
      "| train/             |              |\n",
      "|    actor_loss      | 0.644        |\n",
      "|    critic_loss     | 0.152        |\n",
      "|    learning_rate   | 0.0005       |\n",
      "|    n_updates       | 9258         |\n",
      "|    reward          | -0.019031918 |\n",
      "-------------------------------------\n",
      "day: 3085, episode: 40\n",
      "begin_total_asset: 50000.00\n",
      "end_total_asset: 50220.67\n",
      "total_reward: 220.67\n",
      "total_cost: 49.95\n",
      "total_trades: 46275\n",
      "Sharpe: 0.275\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    episodes        | 8            |\n",
      "|    fps             | 112          |\n",
      "|    time_elapsed    | 220          |\n",
      "|    total_timesteps | 24688        |\n",
      "| train/             |              |\n",
      "|    actor_loss      | 0.529        |\n",
      "|    critic_loss     | 0.00191      |\n",
      "|    learning_rate   | 0.0005       |\n",
      "|    n_updates       | 21602        |\n",
      "|    reward          | -0.019031918 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "b5d9cb94-51a9-4569-a9a8-7e18f1139f4e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4JKB--8tj1g",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "8b89807b-ff71-4902-dd45-1f9111788cbb",
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = df_account_value.append(temp,ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "d2dc62c2-e2a0-48fc-8183-0399d1b27f53",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "9a874df9-2c5f-423c-8fd2-8966aab63fc0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "b419a565-8c15-47d8-f66c-00f81c3c526d",
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "903ef035-f9f4-4678-d18a-1516254eaf3e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "e61a64e0-58ed-4490-b19a-78bd4f76e666",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"==============Compare to DJIA===========\")\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "backtest_plot(df_account_value, \n",
    "              baseline_ticker = '^DJI', \n",
    "              baseline_start = df_account_value.loc[0,'date'],\n",
    "              baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "algo39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
